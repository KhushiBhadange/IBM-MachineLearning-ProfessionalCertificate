{"cells":[{"cell_type":"markdown","id":"370d39ec-8785-420c-b563-4c80306bf109","metadata":{},"source":["# **Model-agnostic Explanations**\n"]},{"cell_type":"markdown","id":"26b785a6-47eb-497a-b132-2562417b4560","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","id":"097ed5f0-f336-49be-b89d-3287c3282111","metadata":{},"source":["In this lab, we will first train a random forest model to predict if employees are looking for a job change, then we want to interpret the trained model in order to understand how exactly it makes predictions. Since random forest model is normally very complex to understand, we will just treat it as a black-box model first. Then, you will have the practice opportunities to apply various model-agnostic explanation methods to explain the black-box model.\n"]},{"cell_type":"markdown","id":"cfafbbf4-702c-4808-8150-66479d32bbc3","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"00682978-d7b2-4ab3-82ca-b0c4930e40ac","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","id":"27f10014-7aa2-45ef-82a7-fd007b6b0220","metadata":{},"source":["* Calculate Permutation Feature Importance\n","* Use Partial Dependency Plot to illustrate relationships between feature and outcomes\n","* Build Global Surrogate Models\n","* Build Local Surrogate Models using `LIME`\n"]},{"cell_type":"markdown","id":"8ce607f7-b79b-433f-8c65-adb9bfa01863","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"1da0ba5f-2fb6-4cb2-a818-07a91cdd49fb","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"701a5a38-16b1-4c70-8d19-a8d167a87569","metadata":{},"source":["Let's first import required Python packages for this lab:\n"]},{"cell_type":"code","execution_count":null,"id":"a57cdac4-6a5a-4e81-846f-53d093ac4cd5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","\n","# install imbalanced-learn package\n","!pip install lime==0.2.0.1\n","\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\" "]},{"cell_type":"markdown","id":"51b83031-86dd-4cbd-be42-527d9f938756","metadata":{},"source":["And then import the required Python packages.\n"]},{"cell_type":"code","execution_count":null,"id":"54f21bf3-9e4e-4e05-9704-4a17240f9f00","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Import packages here\n","import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","import lime.lime_tabular\n","\n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz, plot_tree\n","from sklearn.inspection import permutation_importance, plot_partial_dependence"]},{"cell_type":"markdown","id":"aefed10d-e990-474e-a3d5-5d73ec46ed60","metadata":{},"source":["Then, let's load the dataset to be used in this lab.\n"]},{"cell_type":"code","execution_count":null,"id":"0fa5c59d-fd79-451a-bdf4-1c6f372f819e","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/datasets/hr_new_job_processed.csv\"\n","job_df=pd.read_csv(url)"]},{"cell_type":"code","execution_count":null,"id":"17462773-92af-4eeb-a8cd-083c6728d9d5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["job_df.describe()"]},{"cell_type":"markdown","id":"59cbaa9a-e33a-4f86-b7b2-151db16b44a0","metadata":{},"source":["The dataset contains the following features (predictors):\n","\n","- `city_ development index` : Developement index of the city, ranged from 0 to 1\n","- `training_hours`: Training hours completed, ranged from 0 to 336\n","- `company_size`: Size of the current company, ranged from 0 to 7 where 0 means less than 10 employees and 7 means more than 10,000 employees\n","- `education_level`: Education level of the candidate, ranged from 0 to 4 where 0 means Primary School and 4 means Phd\n","- `experience`: Total experience in years, ranged from 0 to 21\n","- `company_type` : _Categorical column_ with one-hot encodings. Type of current company:  'Pvt Ltd', 'Funded Startup', 'Early Stage Startup', 'Other', 'Public Sector', 'NGO'\n","\n","and the prediction outcome is:\n","- `target`: `0` – Not looking for a job change, `1` – Looking for a job change\n"]},{"cell_type":"markdown","id":"c2cb238b-3364-4ad8-9461-c657727bdf16","metadata":{},"source":["The predictive task is a straightforward binary classification task, more specifically, we want to use an employee's profile features to predict if he/she is looking for a job change or not.\n"]},{"cell_type":"markdown","id":"2c7872bb-b4d9-4374-be4b-0b095de28e47","metadata":{},"source":["## Build a Random Forest classifier as the Black-box model\n"]},{"cell_type":"markdown","id":"e97f780b-1a12-4246-9aee-a8e01e09c155","metadata":{},"source":["### Split the training and testing datasets\n"]},{"cell_type":"code","execution_count":null,"id":"562021ed-7cb9-416b-aec4-bf1100e7fcd3","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X = job_df.loc[:, job_df.columns != 'target']\n","y = job_df[['target']]"]},{"cell_type":"code","execution_count":null,"id":"585a4924-c1bd-4695-b937-3ba73862a045","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 12)"]},{"cell_type":"markdown","id":"07bfc60f-facb-4bce-b5f6-00c749e89357","metadata":{},"source":["Now let's train a `Random Forest` model with the following preset arguments. If you like, you may also use hyperparameter tuning methods to tune these parameters yourself.\n","- `random_state = 0` as a random seed to reproduce the result\n","- `max_depth = 25` means the max depth of a tree should be less than 25\n","- `max_features = 10` means the random forest includes max 10 features\n","- `n_estimators = 100` means total 100 trees will be built\n","- `bootstrap = True` means bootstrap samples will be used to build trees\n"]},{"cell_type":"code","execution_count":null,"id":"608c516a-96c1-4f04-a0fb-7366d0ddab13","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Define a black-box random forest model\n","black_box_model = RandomForestClassifier(random_state = 123, max_depth=25, \n","                             max_features=10, n_estimators=100, \n","                             bootstrap=True)\n","# Train the model\n","black_box_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"90e1e9d7-6fb1-4165-a58f-efcbfb8181ec","metadata":{},"source":["Next, let's make some predictions and evalute the model using `AUC` score:\n"]},{"cell_type":"code","execution_count":null,"id":"b9da5591-26aa-476d-bbe7-48a7e804b055","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#predicting test set\n","y_blackbox = black_box_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"65a2cd34-a5e9-4350-9c7d-8bf12863cdea","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["metrics.roc_auc_score(y_test, y_blackbox)"]},{"cell_type":"markdown","id":"49169e1a-80b9-4eb0-b5e6-3659bb73f746","metadata":{},"source":["Your AUC score should be around `0.81`, which indicates the model is doing a very good job in the test dataset.\n"]},{"cell_type":"markdown","id":"05b08d9d-55f7-4b8d-b6d1-7505f97fc256","metadata":{},"source":["Now we have a black-box random forest model trained, we want to use various model-agnostic methods to explain it. \n"]},{"cell_type":"markdown","id":"b74223f2-7866-417d-b415-9a7911746e76","metadata":{},"source":["Note that if you prefer other binary classification models such as XGBoosting, you could train one here by yourself as well, and it won't affect\n","the subsequent steps since our explanations are all model-agnostic.\n"]},{"cell_type":"markdown","id":"2e2f8d7f-6934-4bb2-b699-68eddf6575f3","metadata":{},"source":["## Permutation Feature Importance\n"]},{"cell_type":"markdown","id":"669d33f2-018b-4590-85e4-0cd28eeeb90f","metadata":{},"source":["One common way to explain a machine learning model is via finding its important features and **permutation feature importance** is a popular method to calculate feature importance.\n"]},{"cell_type":"markdown","id":"83afc269-a890-403b-a245-5e0e6ecc7f51","metadata":{},"source":["The basic idea of permutation feature importance is we shuffle interested feature values and make predictions using the shuffled values. \n","The feature importance will be measured by calculating the difference between the prediction errors before and after permutation.\n"]},{"cell_type":"markdown","id":"a4f4f03d-60a0-4c33-ba91-867937d23e04","metadata":{},"source":["In this lab, we will use `permutation_importance` function provided by `sklearn` to easily calculate importance for all features.\n"]},{"cell_type":"markdown","id":"8072eac4-579c-4ae8-b78d-a1dea8143218","metadata":{},"source":["You can call `permutation_importance` with the following key arguments:\n","- `estimator` the model to be estimated\n","- `X` training data X\n","- `y` target labels y\n","- `n_repeats`, Number of times to permute a feature, each permutation generates an importance value\n"]},{"cell_type":"code","execution_count":null,"id":"9081b6f2-0e53-41e2-b4fa-fdce7d977f61","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Use permutation_importance to calculate permutation feature importances\n","feature_importances = permutation_importance(estimator=black_box_model, X = X_train, y = y_train, n_repeats=5,\n","                                random_state=123, n_jobs=2)"]},{"cell_type":"markdown","id":"457c0c31-d14f-45ae-848e-a68890d11e9f","metadata":{},"source":["Let's take a look at the generated importance results:\n"]},{"cell_type":"code","execution_count":null,"id":"b9c16866-2ec9-4c31-9d5a-69eda97a635a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_importances.importances.shape"]},{"cell_type":"code","execution_count":null,"id":"98e3738c-bb17-49eb-bd32-19bec7c44ec9","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_importances.importances"]},{"cell_type":"markdown","id":"10e00916-8945-442e-aa0e-18b35c90efef","metadata":{},"source":["It is a `11 x 5` numpy array, 11 means we have 11 features, and 5 represents the total number of permutation times. \n","\n","For each permutation, we will have a list of importance score calculated for each feature. The value represents the portion of increased prediction errors, important features will have larger values.\n"]},{"cell_type":"markdown","id":"cf97e3d4-fade-4e91-87a0-671814ce25c1","metadata":{},"source":["However, the feature importance array above is very difficult to comprehend, let's sort and visualize it:\n"]},{"cell_type":"code","execution_count":null,"id":"a6e9584e-27e5-46f2-9f3a-0363e8573fe1","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def visualize_feature_importance(importance_array):\n","    # Sort the array based on mean value\n","    sorted_idx = importance_array.importances_mean.argsort()\n","    # Visualize the feature importances using boxplot\n","    fig, ax = plt.subplots()\n","    fig.set_figwidth(16)\n","    fig.set_figheight(10)\n","    fig.tight_layout()\n","    ax.boxplot(importance_array.importances[sorted_idx].T,\n","               vert=False, labels=X_train.columns[sorted_idx])\n","    ax.set_title(\"Permutation Importances (train set)\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"17caf45d-a32a-4d92-849e-5e6b06068e70","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["visualize_feature_importance(feature_importances)"]},{"cell_type":"markdown","id":"f44f4b1b-9cc0-4e75-a754-54f2bc5fe72a","metadata":{},"source":["Now you should see a box plot show ranked feature importances, and we can see the most important features are `city_development_index`, `company_size`, `training_hours`, `experiences`, `education_level`, and so on, and you should have a general understanding of how the black-box model determines if an employee is looking for a new job or not.\n"]},{"cell_type":"markdown","id":"18596d67-00c3-4ffb-b5ae-3b766fa97ce9","metadata":{},"source":["### Exercise: Use a different `n_repeats=10` to calculate and visualize feature importance values  \n"]},{"cell_type":"code","execution_count":null,"id":"46bb4d7f-e585-43c6-a306-09329c98d8bd","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your answer here\n","# Update n_repeats=10 and recalculate and visualize feature importance\n"]},{"cell_type":"markdown","id":"62bd3060-b409-4c2b-8851-54090161dc52","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","feature_importances = permutation_importance(estimator=black_box_model, X = X_train, y = y_train, n_repeats=10,\n","                                random_state=123, n_jobs=2)\n","\n","visualize_feature_importance(feature_importances)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"eb42f3c2-a6d4-4631-a5ff-63172bfd4939","metadata":{},"source":["## Partial Dependency Plot (PDP)\n"]},{"cell_type":"markdown","id":"912b9679-24dc-49db-b4af-d107672802ab","metadata":{},"source":["Partial Dependency Plot (PDP) is an effective way to illustrate the relationship between an interested feature and the model outcome. It essentially visualizes the marginal effects of a feature, that is, shows how the model outcome changes when a specific feature changes in its distribution.\n"]},{"cell_type":"markdown","id":"91be7ad6-78e5-4cd8-a91f-d0a42d9ff832","metadata":{},"source":["Since a machine learning model may include many features, and it is not feasible to create PDP for every single feature. Thus, we normally first find the most important features via ranking their feature importances. Then, we can only focus PDP on those important features.\n"]},{"cell_type":"markdown","id":"1b5d44c9-29ea-4964-9a96-11db3f18482b","metadata":{},"source":["From the previous step, we know some important features are `city_development_index`, `company_size`, `experience`, `education_level`, and we can easily create PDP for those features using `plot_partial_dependence` in `sklearn` package.\n"]},{"cell_type":"markdown","id":"66308e86-0a78-45c0-9c36-f375ce483f4b","metadata":{},"source":["Let's first try to create PDP for features `city_development_index`, `experience`:\n"]},{"cell_type":"code","execution_count":null,"id":"f9a6eed6-37bc-45e3-a414-edc7c179eeba","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Important features\n","important_features = ['city_development_index', 'experience']\n","\"arguments: \"\n","\" - estimator: the black box model\"\n","\" - X is the training data X\"\n","\" - features are the important features we are interested\"\n","plot_partial_dependence(estimator=black_box_model, \n","                        X=X_train, \n","                        features=important_features,\n","                        random_state=123)"]},{"cell_type":"markdown","id":"e61f7e61-98dd-48db-b9f1-a103dc17e144","metadata":{},"source":["Then you should see two PDPs are plotted for `city_development_index` and `experience`. They all have roughly negative linear relationship betweens the outcome, for example, if an employee is in a well-developed city and has a lot of experiences, then he/she is unlikely to change the current job.\n"]},{"cell_type":"markdown","id":"3f31cc26-7a12-4bf2-80e6-aa9293823be9","metadata":{},"source":["### Exercise: Create PDPs for other important features such as `company_size`, `education_level`, `training_hours`, and others\n"]},{"cell_type":"code","execution_count":null,"id":"58d10c1a-d1a3-400c-a1e1-cf2d93fa3dc0","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your solution here\n","# Create PDPs for other important features\n"]},{"cell_type":"markdown","id":"77636047-192b-48b5-806e-6bb1189f2a9f","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","important_features = ['company_size', 'education_level', 'training_hours']\n","    \n","plot_partial_dependence(estimator=black_box_model, \n","                        X=X_train, \n","                        features=important_features,\n","                        random_state=123)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"85496e5f-d02a-460f-9841-f36418fddb3a","metadata":{},"source":["## Global Surrogate Model\n"]},{"cell_type":"markdown","id":"e5dded9b-339c-47ec-932e-f362165394ca","metadata":{},"source":["Now you have explored how to explain the black-box model via analyzing its features. Next, we will learn how to explain it via approximate of their inputs and outputs with a global surrogate model.\n"]},{"cell_type":"markdown","id":"2675ee54-7a9c-4873-aace-878fae20e4c5","metadata":{},"source":["We will be training two self-interpretable models: 1) a logistic regression model and 2) a decision tree models using the inputs and outputs from the black-box model\n"]},{"cell_type":"markdown","id":"6fc5ed99-e803-478b-80b9-d1eb8b0ed500","metadata":{},"source":["You can follow these general steps to build a global surrogate model:\n","\n","- First, we select a dataset `X_test` as input\n","\n","- Then, we use the black-box model to make predictions `y_blackbox` using the `X_test`\n","\n","- With both training data and labels ready, we can use them to train a simple logistic regression model and a decision tree model\n","\n","- The surrogate model outputs its own predictions `y_surrogate`\n","\n","- Lastly, we can measure the difference between `y_surrogate` and `y_blackbox` using an accuracy score to determine how well the surrogate model approximating the black-box model\n"]},{"cell_type":"markdown","id":"01ccb873-13d0-4dd8-8d69-785c5eb24d1a","metadata":{},"source":["### Logistic regression surrogate model\n"]},{"cell_type":"markdown","id":"37e514ca-c427-4f7f-8bf0-df8822a5f18e","metadata":{},"source":["In order to compare the coefficients of the logistic regression model directly, we want to normalize the input X first.\n"]},{"cell_type":"code","execution_count":null,"id":"b7611e42-02a9-4cb3-bac8-18038581bacf","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# normalize X_test\n","min_max_scaler = StandardScaler()\n","X_test_minmax = min_max_scaler.fit_transform(X_test)"]},{"cell_type":"markdown","id":"3d2a976e-1f69-44d5-8ad4-e147bb781f14","metadata":{},"source":["Then, we can train a logistic regression model with an `L1` regularizer to simplify the model and increase interpretability. Note that `y_blackbox` is coming from the predictions of black-box model.\n"]},{"cell_type":"code","execution_count":null,"id":"03e99a3b-0c9d-46f9-9c0e-5999983b7ae6","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["lm_surrogate = LogisticRegression(max_iter=1000, \n","                                  random_state=123, penalty='l1', solver='liblinear')\n","lm_surrogate.fit(X_test_minmax, y_blackbox)"]},{"cell_type":"markdown","id":"b64c9d31-ef9d-4e39-9d0b-419609e53a6b","metadata":{},"source":["With the surrogate model trained, we can generate predictions using `X_test`, \n"]},{"cell_type":"code","execution_count":null,"id":"23afc17e-95d8-419f-b357-fad64e34c884","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_surrogate = lm_surrogate.predict(X_test_minmax)"]},{"cell_type":"markdown","id":"5752e0e9-33ed-495a-a84c-ac71375a2087","metadata":{},"source":["and calculate how accurate the surrogate model approximates the black-box model.\n"]},{"cell_type":"code","execution_count":null,"id":"2bef1492-43ed-4cae-9c90-6368fcbbdd0f","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["metrics.accuracy_score(y_blackbox, y_surrogate)"]},{"cell_type":"markdown","id":"92e8b00b-64ac-4dbd-b65b-021ecc6cc0a8","metadata":{},"source":["The score is around 0.75 which means the logistic regression surrogate model was able to reproduce about 75% of the original black-box model correctly.\n"]},{"cell_type":"markdown","id":"3b2a2307-43a3-42d2-8f7d-6cb30073eaa6","metadata":{},"source":["Next, we can start interpreting the much simpler logistic regression model `lm_surrogate` via analyzing its feature coefficients. We defined a function called `get_feature_coeffs` to extract and sort feature coefficients from `lm_surrogate` model:\n"]},{"cell_type":"code","execution_count":null,"id":"b53a69da-462d-4316-bef1-74974da01518","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Extract and sort feature coefficients\n","def get_feature_coefs(regression_model):\n","    coef_dict = {}\n","    # Filter coefficients less than 0.01\n","    for coef, feat in zip(regression_model.coef_[0, :], X_test.columns):\n","        if abs(coef) >= 0.01:\n","            coef_dict[feat] = coef\n","    # Sort coefficients\n","    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n","    return coef_dict"]},{"cell_type":"code","execution_count":null,"id":"a9ce0938-7d74-49c5-a424-eea634f36714","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["coef_dict = get_feature_coefs(lm_surrogate)\n","coef_dict"]},{"cell_type":"markdown","id":"29e07650-7306-4611-9cba-df4caf26abd7","metadata":{},"source":["We can get a coefficient dict object whose keys are features and values are coefficients, but such dict object is not easy to understand so let's just visualize it using a bar chart:\n"]},{"cell_type":"code","execution_count":null,"id":"32957fa9-2430-4306-b178-0535c2106403","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Generate bar colors based on if value is negative or positive\n","def get_bar_colors(values):\n","    color_vals = []\n","    for val in values:\n","        if val <= 0:\n","            color_vals.append('r')\n","        else:\n","            color_vals.append('g')\n","    return color_vals\n","\n","# Visualize coefficients\n","def visualize_coefs(coef_dict):\n","    features = list(coef_dict.keys())\n","    values = list(coef_dict.values())\n","    y_pos = np.arange(len(features))\n","    color_vals = get_bar_colors(values)\n","    plt.rcdefaults()\n","    fig, ax = plt.subplots()\n","    ax.barh(y_pos, values, align='center', color=color_vals)\n","    ax.set_yticks(y_pos)\n","    ax.set_yticklabels(features)\n","    # labels read top-to-bottom\n","    ax.invert_yaxis()  \n","    ax.set_xlabel('Feature Coefficients')\n","    ax.set_title('')\n","    plt.show()\n","    "]},{"cell_type":"markdown","id":"e564d11c-f77f-4e2f-a9ee-b7937ddf984e","metadata":{},"source":["Let's call `visualize_coefs` function to visualize the coefficients dict:\n"]},{"cell_type":"code","execution_count":null,"id":"a222d7e0-f27d-4468-bd81-54da4c59d974","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["visualize_coefs(coef_dict)"]},{"cell_type":"markdown","id":"9ad52f71-eda9-4d09-87d0-304fb0545aca","metadata":{},"source":["From the bar chart above, you can immediately find those important features with negative effects such as `city_development_index` and `experience`, and those have positive effects such as education_level or if the company is a `Pvt Ltd`.\n"]},{"cell_type":"markdown","id":"7835c924-bfaa-4880-9cf1-51347664488e","metadata":{},"source":["### Exercise: Build a global surrogate model using decision tree\n"]},{"cell_type":"code","execution_count":null,"id":"7b0cc9fc-ba44-4773-97a1-94d8fc6ad4f8","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your answer here\n","# Define a decision tree model"]},{"cell_type":"markdown","id":"bc9db84d-5391-4b9b-b31b-f2bf7f602f29","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","tree_surrogate = DecisionTreeClassifier(random_state=123, \n","                                         max_depth=5, \n","                                         max_features=10)\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"id":"7553fb54-ba4c-470c-bc90-fe4b0996ae10","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your answer here\n","# Train the decision tree model with X_test and y_blackbox, and make predictions on X_test"]},{"cell_type":"markdown","id":"107a5664-1778-4729-a396-8844aa62d787","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","tree_surrogate.fit(X_test, y_blackbox)\n","y_surrogate = tree_surrogate.predict(X_test)\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"id":"cea855ba-9567-4789-9206-a2a0c3dae37c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your answer here\n","# Measure the difference between"]},{"cell_type":"markdown","id":"0d3d4f13-dfc9-47a2-a437-aa98c978673d","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","metrics.accuracy_score(y_blackbox, y_surrogate)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"9e1522d3-defd-4570-9e51-66cf513ccae9","metadata":{},"source":["Now, you have trained the tree surrogate model, you could interprete it by export and print the tree:\n"]},{"cell_type":"code","execution_count":null,"id":"728de789-2439-465e-a583-fc1466c8b90f","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["tree_exp = export_text(tree_surrogate, feature_names=list(X_train.columns))"]},{"cell_type":"code","execution_count":null,"id":"4fefa19e-e875-4210-8b26-a326e9c07d51","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(tree_exp)"]},{"cell_type":"markdown","id":"d7c55072-1a0b-4cdb-8f94-1c3828d6c4c1","metadata":{},"source":["## Local interpretable model-agnostic explanations (LIME)\n"]},{"cell_type":"markdown","id":"40ecf7c0-be18-4710-8810-aa259d9c7ec9","metadata":{},"source":["Global surrogate models may have large prediction inconsistency between the complex black-box model and the simple surrogate models or there are many instance groups or clusters in the dataset which make the surrogate model more generalized to those different patterns and lose the interpretability on a specific data group. \n","\n","On the other hand, we are also interested in how black-box models make predictions on some representative instances. By understanding these very typical examples, we can sometimes obtain valuable insights without understanding the model’s behaviors on the entire dataset.\n"]},{"cell_type":"markdown","id":"05815728-23e8-43df-9d5a-8db8d9f6f1b7","metadata":{},"source":["Next, you will be building a local surrogate model using LIME method whose general steps are shown in the following flowchart:\n"]},{"cell_type":"markdown","id":"76580ffd-dbad-4541-8805-4216d7369480","metadata":{},"source":["![Local interpretable model-agnostic method](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/module6_model_interpretability/images/lime.png)\n"]},{"cell_type":"markdown","id":"6842c877-32e4-420d-a861-b7e5990c151a","metadata":{},"source":["We can use a open source [lime](https://github.com/marcotcr/lime) package to easily build a LIME explainer the our black-box model, let's get started.\n"]},{"cell_type":"markdown","id":"313e31e0-4e93-4d8d-8f23-8e7b10740336","metadata":{},"source":["First, we need to define a `LimeTabularExplainer` to explain those predictive models built on structured/tabular datasets, like the job-changing prediction dataset we are using.\n"]},{"cell_type":"markdown","id":"2b07d6bc-c211-49fb-959a-c2ec0db5ce5e","metadata":{},"source":["Note although LIME algorithm is a local surrogate model, it still also requires a training dataset containing your interested data instances. So that it can perform uniform sampling (feature permutations) around the interested data instances to generate the artificial dataset for the actual surrogate model training process.\n"]},{"cell_type":"code","execution_count":null,"id":"9f9c320c-dd41-4675-991a-23fc28908266","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["explainer = lime.lime_tabular.LimeTabularExplainer(\n","    # Set the training dataset to be X_test.values (2-D Numpy array)\n","    training_data=X_test.values,\n","    # Set the mode to be classification\n","    mode='classification',\n","    # Set class names to be `Not Changing` and `Changing`\n","    class_names = ['Not Changing', 'Changing'],\n","    # Set feature names\n","    feature_names=list(X_train.columns),\n","    random_state=123,\n","    verbose=True)"]},{"cell_type":"markdown","id":"dff6cb0c-0f24-42b7-ae5e-0f3c94325237","metadata":{},"source":["Now, let's try to select an interested employee from `X_test`, and we want to understand its prediction using the `LimeTabularExplainer`.\n"]},{"cell_type":"code","execution_count":null,"id":"48efc299-86ef-4ca7-8bb7-5aaf367c2f3b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["instance_index = 19\n","selected_instance = X_test.iloc[[instance_index]]\n","lime_test_instance = selected_instance.values.reshape(-1)\n","selected_instance"]},{"cell_type":"markdown","id":"776cb076-d098-4a64-ba93-7ac82718662e","metadata":{},"source":["Let's make a quick summary about this employee:\n","- His/her city is well-developed with a city development index > 0.9\n","- His/her training hour is 74 hours\n","- His/her company is a very big company, 7 means more than 10,000 employees\n","- His/her experience is more than 16 years\n","- His/her company is a Pvt Ltd (Private) company\n","- His/her has Master's degree(s)\n","\n","and our black-box model predicts its probability of changing a job is `0.03`, that is, very unlikely to leave his or her current job.\n"]},{"cell_type":"markdown","id":"b7703768-f442-4059-a517-65a7313b9500","metadata":{},"source":["Then, let's use `LimeTabularExplainer` to explain why the black-box model thinks this employee won't leave his/her current job.\n"]},{"cell_type":"code","execution_count":null,"id":"fd0bb6cf-14ea-485f-9f6e-08e128ab5247","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["exp = explainer.explain_instance(\n","                                 # Instance to explain\n","                                 lime_test_instance, \n","                                 # The prediction from black-box model\n","                                 black_box_model.predict_proba,\n","                                 # Use max 10 features\n","                                 num_features=10)\n","exp.as_pyplot_figure();"]},{"cell_type":"markdown","id":"43babedd-1c5d-4626-9ef1-3ba83065f4cc","metadata":{},"source":["`LimeTabularExplainer` outputs a bar chart similar to the coefficient or feature importance chart we plotted before. \n","From its output, we can easily interpret why the black-box thinks this employe won't change job, based on the following main factors:\n","- His/her company is a very big company\n","- His/her city is well-developed with city development\n","- His/her highest degree is Master or above\n","- His/her experience is more than 15 years\n","- His/her company is not NGO or Startup\n","\n","This interpretation is also aligned with our common sense, that is, if a well-educated employee has been working in a very big/good private company, located in a big city, for more than 15 years, then he/she is probably very satisfied with current job and does not want to change it.\n"]},{"cell_type":"markdown","id":"b8faffc3-30f3-4692-886c-534b5bf4e3cd","metadata":{},"source":["### Exercise: Find other data instances and use LimeTabularExplainer to explain their predictions of black-box model\n"]},{"cell_type":"code","execution_count":null,"id":"18f14600-3de0-4494-a9fc-8f046cd2c793","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Update instance_index, and rerun the explainer.explain_instance() method\n"]},{"cell_type":"markdown","id":"fb20be5d-10a6-4dd0-9257-ec1f2a49b84f","metadata":{},"source":["## Next Steps\n"]},{"cell_type":"markdown","id":"30b8b042-0616-4245-90fc-b514232bacc2","metadata":{},"source":["By now you have learned and applied various model-agnostic explanation algorithms such as Permutation Feature Importance, PDP, Global Surrogate Model, LIME, and others in this lab. There are many other such methods such as Feature Interactions, Individual Conditional Expectation, SHAP values, and so on, and we do not have enough time to explain them all in this course. \n","\n","We list the references to other popular model explanation methods which you may be interested:\n","- [Predictive learning via rule ensembles](https://arxiv.org/abs/0811.1679)\n","- [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)\n","- [Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation](https://arxiv.org/abs/1309.6392)\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
