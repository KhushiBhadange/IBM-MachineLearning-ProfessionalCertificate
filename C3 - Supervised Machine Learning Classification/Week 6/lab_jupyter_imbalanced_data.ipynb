{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Tackle Imbalanced Data Challenge**\n"]},{"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","metadata":{},"source":["In this lab, you will identify the imbalanced data problems in four binary classification scenarios, all with skewed class distributions: \n"]},{"cell_type":"markdown","metadata":{},"source":["| Task Name     | Class Ratio (Negative vs. Positive)  |\n","| ------------- |:-------------:|\n","| _Credit Card Fraud Detection_      | ~1000 : 1      | \n","| _Predicting Customer Churn_ | ~5 : 1      | \n","| _Tumor Type Estimation_ | ~2 : 1     | \n","| _Predicting Job Change_ | ~10 : 1      | \n"]},{"cell_type":"markdown","metadata":{},"source":["Next, you will try to tackle the imbalanced data challenges in the above tasks using class weighting and resampling methods:\n","- Effective class weighting strategies will assign minority class with more weights, so that it may have a larger impact on the model training process\n","- Resampling methods will generate synthetic datasets from the original datasets\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","metadata":{},"source":["* Identify typical patterns of imbalanced data challenges\n","* Apply `Class Re-weighting` method to adjust the impacts of different classes in model training processes\n","* Apply `Oversampling` and `Undersampling` to generate synthetic datasets and rebalance classes\n","* Evaluate your consolidated classifiers using robust metrics such as `F-score` and `AUC`\n"]},{"cell_type":"markdown","metadata":{},"source":["----\n"]},{"cell_type":"markdown","metadata":{},"source":["First, let us import the required packages for this lab:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","\n","# install imbalanced-learn package\n","!pip install imbalanced-learn==0.8.0\n","\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Import packages here\n","import pandas as pd\n","import numpy as np \n","import imblearn\n","from matplotlib.pyplot import figure\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.model_selection import train_test_split, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix, plot_confusion_matrix, precision_score, recall_score, roc_auc_score\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn import metrics\n","from sklearn.inspection import permutation_importance\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import GridSearchCV\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{},"source":["First, we want to provide some sample grid search methods if you are interested in tuning your model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["rs = 123\n","# Grid search hyperparameters for a logistic regression model\n","def grid_search_lr(X_train, y_train):\n","    params_grid = {\n","    'class_weight': [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}]\n","    }\n","    lr_model = LogisticRegression(random_state=rs, max_iter=1000)\n","    grid_search = GridSearchCV(estimator = lr_model, \n","                           param_grid = params_grid, \n","                           scoring='f1',\n","                           cv = 5, verbose = 1)\n","    grid_search.fit(X_train, y_train)\n","    best_params = grid_search.best_params_\n","    return best_params\n","\n","# Grid search hyperparameters for a random forest model\n","def grid_search_rf(X_train, y_train):\n","    params_grid = {\n","    'max_depth': [5, 10, 15, 20],\n","    'n_estimators': [25, 50, 100],\n","    'min_samples_split': [2, 5],\n","    'class_weight': [{0:0.1, 1:0.9}, {0:0.2, 1:0.8}, {0:0.3, 1:0.7}]\n","    }\n","    rf_model = RandomForestClassifier(random_state=rs)\n","    grid_search = GridSearchCV(estimator = rf_model, \n","                           param_grid = params_grid, \n","                           scoring='f1',\n","                           cv = 5, verbose = 1)\n","    grid_search.fit(X_train, y_train)\n","    best_params = grid_search.best_params_\n","    return best_params"]},{"cell_type":"markdown","metadata":{},"source":["and a method to split training and testing dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def split_data(df):\n","    X = df.loc[ : , df.columns != 'Class']\n","    y = df['Class'].astype('int')\n","    return train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","metadata":{},"source":["## Scenario: Credit Card Fraud Detection\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's start with a credit card fraud detection scenario.\n","The dataset contains transactions made by credit cards. As you can imagine, the majority of transactions are normal and only a very few real fraud transactions are in the dataset. Our goal is to train a classification model to recognize those fraudulent credit card transactions.\n"]},{"cell_type":"markdown","metadata":{},"source":["First, we load the dataset as a dataframe:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["credit_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/datasets/im_credit.csv\", index_col=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["credit_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Due to confidentiality issues, the original features and other background information about each transaction is hidden, and this dataset now contains only numerical features which are the result of a PCA transformation. \n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's see  how imbalanced this dataset is:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["credit_df['Class'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Visualize the count for each class\n","credit_df['Class'].value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, we only have about 0.001% fraud transactions in the dataset and you can not even see it on the previous bar chart! This is an extremely imbalanced dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we will quickly build a standard logistic regression model to see how it performs on such an imbalanced dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Split the training and testing dataset\n","X_train, X_test, y_train, y_test = split_data(credit_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Build a simple logistic regression model\n","model = LogisticRegression(random_state=rs, \n","                              max_iter = 1000)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","preds = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["And check its prediction accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["accuracy_score(y_test, preds)"]},{"cell_type":"markdown","metadata":{},"source":["OK, it achieves 99% prediction accuracy on the test dataset, looks like a great model! Before concluding we have found a simple and great model to detect credit card frauds, let's try other metrics first.\n"]},{"cell_type":"markdown","metadata":{},"source":["### More Robust Evaluation Metrics\n"]},{"cell_type":"markdown","metadata":{},"source":["Here are some effective robust binary classification evaluation metrics such as Precision, Recall, and Fscore, which can be defined as follows:\n"]},{"cell_type":"markdown","metadata":{},"source":["- `Precision`: the percentage of accurately predicted positive instances\n","\n","- `Recall`: the percentage of successfully recognized positive instances\n","\n","- `Fscore`: can also be called F-beta score, which is a weighted average of precision and recall to evaluate the model. The weights between recall and precision are controlled by the `beta`  parameter, the default value is 1 so the most common F-beta is F1 score\n"]},{"cell_type":"markdown","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/images/evaluation_metrics.png\" width=\"720\" alt=\"evaluation metrics\">\n","</center>\n"]},{"cell_type":"markdown","metadata":{},"source":["In addition, the plot of True positive rate and false positive ratio under different thresholds, known as Receiver operating characteristic `ROC` and its associated Area Under the Curve `AUC` are also reliable metrics.\n"]},{"cell_type":"markdown","metadata":{},"source":["First let's calculate Precision, Recall, and Fscore.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Calculate the precision, recall, f5 given the y_test and predictions\n","# Note that we use beta=5 which means we think the cost of positive class is 5 times of negative class\n","# You could try other beta values yourself\n","accuracy = accuracy_score(y_test, preds)\n","precision, recall, fbeta, support = precision_recall_fscore_support(y_test, preds, beta=5, pos_label=1, average='binary')\n","auc = roc_auc_score(y_test, preds)\n","print(f\"Accuracy is: {accuracy:.2f}\")\n","print(f\"Precision is: {precision:.2f}\")\n","print(f\"Recall is: {recall:.2f}\")\n","print(f\"Fscore is: {fbeta:.2f}\")\n","print(f\"AUC is: {auc:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["For many machine learning tasks on imbalanced datasets, like this credit card fraud detection, we normally care more about recall than precision. As a baseline, we want the model to be able to find all frauds and we would allow the model to make false-positive errors because the cost of false positives is usually not very high (maybe just costs a false notification email or phone call to confirm with customers). On the other hand, failing to recognize positive examples (such as fraud or a deadly disease) can be life-threatening \n","\n","As such, our priority is to improve the model's recall, then we will also want to keep precision as high as possible. \n"]},{"cell_type":"markdown","metadata":{},"source":["### Synthetic Minority Oversampling Technique (SMOTE)\n"]},{"cell_type":"markdown","metadata":{},"source":["SMOTE first creates many pairs or small clusters with two or more similar instances, the measure by instance distance such as Euclidean distance.\n","Then, within the boundary of each pair or cluster, SMOTE uniformly permutes features value, one feature at a time, to populate a collection of similar synthesized instances within each pair or cluster.\n","\n","As a result, SMOTE creates a class-balanced synthetic dataset without adding duplicated instances with minority labels. \n"]},{"cell_type":"markdown","metadata":{},"source":["The `imblearn` package provides us with many effective samplers including `SMOTE`, `RandomOverSampler`, and `RandomUnderSampler`. Let's use its `SMOTE` class to define a SMOTE sampler first\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from imblearn.over_sampling import RandomOverSampler, SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create a SMOTE sampler\n","smote_sampler = SMOTE(random_state = rs)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Resample training data using SMOTE\n","X_smo, y_smo = smote_sampler.fit_resample(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["After SMOTE resampling, we can see both positive and negative class has the same instances\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Visualize classes\n","y_smo.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","metadata":{},"source":["Let's retrain the logistic regression model with resampled training data using SMOTE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Re-train the model with resampled data\n","model.fit(X_smo, y_smo)\n","preds = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["and re-evaluate the model to see if there are any improvements with respect to Precision, Recall, and F1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Calculate the precision, recall, f5 given the y_test and predictions\n","# Note that we use beta=5 which means we think the cost of positive class is 5 times of negative class\n","# You could try other beta values yourself\n","precision, recall, f_beta, support = precision_recall_fscore_support(y_test, preds, beta=5, pos_label=1, average='binary')\n","auc = roc_auc_score(y_test, preds)\n","accuracy = accuracy_score(y_test, preds)\n","print(f\"Accuracy is: {accuracy:.2f}\")\n","print(f\"Precision is: {precision:.2f}\")\n","print(f\"Recall is: {recall:.2f}\")\n","print(f\"Fscore is: {f_beta:.2f}\")\n","print(f\"AUC is: {auc:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["First, we can see AUC is improved from 0.72 to 0.91, this is a good sign as SMOTE improves model classification ability.\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's take a look at the recall. The recall is now 0.88, mostly two times larger than the original recall value of 0.45. \n"]},{"cell_type":"markdown","metadata":{},"source":["However, our model's precision is only 0.03. One possible reason is our dataset was extremely skewed, and oversampling the minority class significantly disrupts the original data distribution so that the model trained on the SMOTE dataset generates large false positives on the test dataset sampled from the original dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["As such, for an extremely skewed dataset, we generally do not use oversampling as it significantly shifts the original data distribution. \n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's try class re-weighting to see if it can achieve better results than SMOTE in terms of Precision, Recall, and Fscore.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Class reweighting\n"]},{"cell_type":"markdown","metadata":{},"source":["For binary classification models, its loss function is normally calculated via a sum of the loss with respect to class 0 and the loss with respect to class 1. By default, their class weights are all 1s meaning we treat each class equally important.\n","\n","However, since the class distribution is skewed in imbalanced datasets and the loss function optimization process will be dominated by the majority class, we want to help the minority class by increasing its class weight in the loss function.\n"]},{"cell_type":"markdown","metadata":{},"source":["Class weights can be generally calculated via the following three strategies:\n"]},{"cell_type":"markdown","metadata":{},"source":["- Based on their instances portion in the dataset. For example, if positive instances only take 10% of the dataset, we assign its weight to be 0.9 and weight for the majority class to be 0.1\n","- Heuristics or domain knowledge. Misclassification normally has different costs per class, for example, the cost of failure to diagnose a disease is much higher than a false positive diagnose. If we already know such misclassification costs beforehand, we may use them to assign class weights\n","- Hyper-parameter tuning. Standard hyper-parameter tuning methods can be used to find optimized class weights. For example, grid searching from 0.1 to 0.9 for positive class weight to find out which hyperparameter combination generates the best model.\n"]},{"cell_type":"markdown","metadata":{},"source":["Python `sklearn` package provides a very convenient way to adjust class weights during model training via providing a class weight argument. Many standard classifiers in `sklearn` such as `LogisticRegression` and `RandomForestClassifier` support class reweighting natively.\n"]},{"cell_type":"markdown","metadata":{},"source":["Let see an example of class reweighting in `LogisticRegression`:\n"]},{"cell_type":"markdown","metadata":{},"source":["First we just need to create a class weights dict object, and since\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["class_weight = {}\n","\n","# Assign weight of class 0 to be 0.1\n","class_weight[0] = 0.1\n","\n","# Assign weight of class 1 to be 0.9\n","class_weight[1] = 0.9"]},{"cell_type":"markdown","metadata":{},"source":["Note that `0.1 vs. 0.9` is a pre-tuned weight combination, you could check the provided `grid_search_lr()` method at the beginning of this lab and use it to tune the optimized class weights yourself.\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's use the class weight dict in a `LogisticRegression` model by specifying its `class_weight` argument.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Define a logistic regression with weight\n","model = LogisticRegression(random_state=rs, \n","                              max_iter = 1000,\n","                              class_weight=class_weight)"]},{"cell_type":"markdown","metadata":{},"source":["During the training process, the positive instance will have much larger influence on reducing the total loss and weight updates. The trained classifier will be enhanced to recognize positive instances (increase recall). \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Train the model\n","model.fit(X_train, y_train)\n","# Make predictions on the test dataset\n","preds = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's see if our metrics are getting better.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Calculate the precision, recall, f5 given the y_test and predictions\n","# Note that we use beta=5 which means we think the cost of positive class is 5 times of negative class\n","# You could try other beta values yourself\n","precision, recall, f_beta, support = precision_recall_fscore_support(y_test, preds, beta=5, pos_label=1, average='binary')\n","auc = roc_auc_score(y_test, preds)\n","accuracy = accuracy_score(y_test, preds)\n","print(f\"Accuracy is: {accuracy:.2f}\")\n","print(f\"Precision is: {precision:.2f}\")\n","print(f\"Recall is: {recall:.2f}\")\n","print(f\"Fscore is: {f_beta:.2f}\")\n","print(f\"AUC is: {auc:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, we have much balanced `Precision` and `Recall` values. It indicates that, for an extremely imbalanced dataset like credit card fraud detection, often class reweighting is a better strategy than oversampling. Since undersampling will significantly shrink the training dataset, we normally do not use undersampling for an extremely imbalanced dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["To summarize the findings from the credit card fraud detection task:\n","- All models have very high accuracy and AUC values but this does not always mean they have good prediction performance, especially for classifying the fraud cases\n","- Classification on an extremly imbalanced dataset is usually a hard task, especially if we want to have both high recall and precision values\n","- `SMOTE` can achieve high recall but extremely low precision due to the caused disruptions of original data distribution\n","- Class reweighting can achieve a more balanced recall and precision value\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's consolidate your understandings about the imbalanced data challenge with a few more imbalanced classification scenarios.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Some utility model training and evaluation methods \n"]},{"cell_type":"markdown","metadata":{},"source":["Before we continue with more imbalanced classification scenarios, let's define some utility methods for you to simplify this lab and increase notebook readability. A method to define and train a logistic regression:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["rs = 123\n","# Build a logistic regression model\n","def build_lr(X_train, y_train, X_test, threshold=0.5, best_params=None):\n","    \n","    model = LogisticRegression(random_state=rs, \n","                              max_iter = 1000)\n","    # If best parameters are provided\n","    if best_params:\n","        model = LogisticRegression(penalty = 'l2',\n","                              random_state=rs, \n","                              max_iter = 1000,\n","                              class_weight=best_params['class_weight'])\n","    # Train the model\n","    model.fit(X_train, y_train)\n","    # If predicted probability is largr than threshold (default value is 0.5), generate a positive label\n","    predicted_proba = model.predict_proba(X_test)\n","    yp = (predicted_proba [:,1] >= threshold).astype('int')\n","    return yp, model"]},{"cell_type":"markdown","metadata":{},"source":["A method to define and train a random forest:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def build_rf(X_train, y_train, X_test, threshold=0.5, best_params=None):\n","    \n","    model = RandomForestClassifier(random_state = rs)\n","    # If best parameters are provided\n","    if best_params:\n","        model = RandomForestClassifier(random_state = rs,\n","                                   # If bootstrap sampling is used\n","                                   bootstrap = best_params['bootstrap'],\n","                                   # Max depth of each tree\n","                                   max_depth = best_params['max_depth'],\n","                                   # Class weight parameters\n","                                   class_weight=best_params['class_weight'],\n","                                   # Number of trees\n","                                   n_estimators=best_params['n_estimators'],\n","                                   # Minimal samples to split\n","                                   min_samples_split=best_params['min_samples_split'])\n","    # Train the model   \n","    model.fit(X_train, y_train)\n","    # If predicted probability is largr than threshold (default value is 0.5), generate a positive label\n","    predicted_proba = model.predict_proba(X_test)\n","    yp = (predicted_proba [:,1] >= threshold).astype('int')\n","    return yp, model"]},{"cell_type":"markdown","metadata":{},"source":["A method to evaluate a classifier's predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["rs = 123\n","def evaluate(yt, yp, eval_type=\"Original\"):\n","    results_pos = {}\n","    results_pos['type'] = eval_type\n","    # Accuracy\n","    results_pos['accuracy'] = accuracy_score(yt, yp)\n","    # Precision, recall, Fscore\n","    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, beta=5, pos_label=1, average='binary')\n","    results_pos['recall'] = recall\n","    # AUC\n","    results_pos['auc'] = roc_auc_score(yt, yp)\n","    # Precision\n","    results_pos['precision'] = precision\n","    # Fscore\n","    results_pos['fscore'] = f_beta\n","    return results_pos"]},{"cell_type":"markdown","metadata":{},"source":["A method to resample the original dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def resample(X_train, y_train):\n","    # SMOTE sampler (Oversampling)\n","    smote_sampler = SMOTE(random_state = 123)\n","    # Undersampling\n","    under_sampler = RandomUnderSampler(random_state=123)\n","    # Resampled datasets\n","    X_smo, y_smo = smote_sampler.fit_resample(X_train, y_train)\n","    X_under, y_under = under_sampler.fit_resample(X_train, y_train)\n","    return X_smo, y_smo, X_under, y_under"]},{"cell_type":"markdown","metadata":{},"source":["And a method to visualize metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def visualize_eval_metrics(results):\n","    df = pd.DataFrame(data=results)\n","    #table = pd.pivot_table(df, values='type', index=['accuracy', 'precision', 'recall', 'f1', 'auc'],\n","    #                columns=['type'])\n","    #df = df.set_index('type').transpose()\n","    print(df)\n","    x = np.arange(5)\n","    original = df.iloc[0, 1:].values\n","    class_weight = df.iloc[1, 1:].values\n","    smote = df.iloc[2, 1:].values\n","    under = df.iloc[3, 1:].values\n","    width = 0.2\n","    figure(figsize=(12, 10), dpi=80)\n","    plt.bar(x-0.2, original, width, color='#95a5a6')\n","    plt.bar(x, class_weight, width, color='#d35400')\n","    plt.bar(x+0.2, smote, width, color='#2980b9')\n","    plt.bar(x+0.4, under, width, color='#3498db')\n","    plt.xticks(x, ['Accuracy', 'Recall', 'AUC', 'Precision', 'Fscore'])\n","    plt.xlabel(\"Evaluation Metrics\")\n","    plt.ylabel(\"Score\")\n","    plt.legend([\"Original\", \"Class Weight\", \"SMOTE\", \"Undersampling\"])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Scenario: Predicting Customer Churn\n"]},{"cell_type":"markdown","metadata":{},"source":["In the second scenario, we will be predicting customer churns (leaving the business) of a telecom company.\n"]},{"cell_type":"markdown","metadata":{},"source":["First, let's read the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["churn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/datasets/im_churn.csv\", index_col=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["churn_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["This dataset is processed and contains features about a customer's telcom service types, tenure, charges, and payments. Based on such features, we would like to predict if a customer is leaving the business or not (churn).\n"]},{"cell_type":"markdown","metadata":{},"source":["Then, we need to split the data into training and testing datasets,\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_train, X_test, y_train, y_test = split_data(churn_df)"]},{"cell_type":"markdown","metadata":{},"source":["and take a look at its class distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_train.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from the bar chart above, the non-churn customers are almost 4 times more than the churn customers, which makes it an imbalanced dataset as well. Since this dataset has 42 features, it would be better to use a more complex classification model and we choose to use `RandomForestClassifier` from `sklearn` package.\n"]},{"cell_type":"markdown","metadata":{},"source":["`RandomForestClassifier` has many hyperparameters that need to be tuned, we have pre-tuned the main parameters for you. You may also use the provided `grid_search_rf()` method to play with hyperparameter tuning yourself.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Tuned random forest parameters\n","best_params_no_weight = {'bootstrap': True,\n","                         'class_weight': None, \n","                         'max_depth': 10, \n","                         'min_samples_split': 5, \n","                         'n_estimators': 50}"]},{"cell_type":"markdown","metadata":{},"source":["Next, since we want to compare the performance of different models, we create a list to contain all model performance metrics.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["results = []"]},{"cell_type":"markdown","metadata":{},"source":["### Train a regular random forest classifier\n"]},{"cell_type":"markdown","metadata":{},"source":["Then, we will train a regular random forest classifier without any add-ons (class weights or resampling).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["preds, model = build_rf(X_train, y_train, X_test, best_params=best_params_no_weight)\n","result = evaluate(y_test, preds, \"Original\")\n","print(result)\n","results.append(result)"]},{"cell_type":"markdown","metadata":{},"source":["As you may expect, we have a very high prediction `accuracy` but even using a random forest classifier, our `recall` is very low with only 0.28. \n","Next, let's try the class reweighting method to see if we can improve the performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Add class re-weighting\n"]},{"cell_type":"markdown","metadata":{},"source":["We will add class weights to the random forest classifier with pre-tuned weight 0.8 to churn class and weight 0.2 to non-churn class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["class_weight = {}\n","# 0.2 to Non-churn class\n","class_weight[0] = 0.2\n","# 0.8 to Churn class\n","class_weight[1] = 0.8"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Pre-tuned hyper parameters\n","best_params_weight = {'bootstrap': True,\n","                         'class_weight': class_weight, \n","                         'max_depth': 10, \n","                         'min_samples_split': 5, \n","                         'n_estimators': 50}"]},{"cell_type":"markdown","metadata":{},"source":["Build a random forest model with the class weight 0.8 vs. 0.2.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# class weight\n","preds_cw, weight_model = build_rf(X_train, y_train, X_test, best_params=best_params_weight)"]},{"cell_type":"markdown","metadata":{},"source":["Then evaluate the refined model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["result = evaluate(y_test, preds_cw, \"Class Weight\")\n","print(result)\n","results.append(result)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from the evaluation results above, `Recall` and `Fscore` are significantly improved by adding class weights. Such improvements indicate that class reweighting is effective for the imbalanced customer churn dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Resampling: SMOTE and Undersampling\n"]},{"cell_type":"markdown","metadata":{},"source":["Then, we want to use resampling to see if it is also able to improve the model performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["First, we create resampled training datasets:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# X_smo is resampled from X_train using SMOTE\n","# y_smo is resampled from y_train using SMOTE\n","# X_under is resampled from X_train using Undersampling\n","# y_under is resampled from y_train using Undersampling\n","X_smo, y_smo, X_under, y_under = resample(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Then we can retrain a random forest classifier using SMOTE resampled input `X_smo` and output `y_smo` and evaluate its performance,\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["preds_smo, smo_model = build_rf(X_smo, y_smo, X_test, best_params=best_params_no_weight)\n","result = evaluate(y_test, preds_smo, \"SMOTE\")\n","print(result)\n","results.append(result)"]},{"cell_type":"markdown","metadata":{},"source":["and we can retrain a random forest classifier using the undersampled input `X_under` and output `y_under` and evaluate its performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["preds_under, under_model = build_rf(X_under, y_under, X_test, best_params=best_params_no_weight)\n","result = evaluate(y_test, preds_under, \"Undersampling\")\n","print(result)\n","results.append(result)"]},{"cell_type":"markdown","metadata":{},"source":["### Compare the performance among different random forest models\n"]},{"cell_type":"markdown","metadata":{},"source":["By now, we have built four different random forest models: the model trained with the original dataset, the model with class weights, the model trained with SMOTE dataset, and the model trained with undersampling. \n","\n","To better analyze and compare their performance, we can visualize the metrics using a grouped bar chart.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["visualize_eval_metrics(results)"]},{"cell_type":"markdown","metadata":{},"source":["Let's first see what each color means. The grey bars represent the original model, the orange bars represent the model with class weight, the dark blue bars represent model trained with SMOTE, and the light blue bars represent the model trained with undersampling.\n","\n","As we can inferÂ the following insights from the bar charts:\n"]},{"cell_type":"markdown","metadata":{},"source":["- All models have high accuracy\n","- Recall is improved with class weights and resampling methods, and undersampling produces the highest recall\n","- AUC is also improved with class weights and resampling methods, and undersampling produces the highest AUC again\n","- Precisions are decreased with class weights and resampling methods as they all introduced many false positives (which are expected as we are trying to increase the impact of the positive class)\n","- Fscore is improved with class weights and resampling method, and undersampling has the highest Fscore\n"]},{"cell_type":"markdown","metadata":{},"source":["By analyzing the bar chart above, we can see that undersampling seems to be the best method to help alleviate the imbalanced challenge in the customer churn dataset. Although all class weights, SMOTE, and undersampling decreased the precision (increased false positives) but sometimes it is not a bad idea to assume some of your customers are about to leave (even if they are not) as motivation to improve your services.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Scenario: Tumor Type Estimation\n"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let's see an interesting imbalanced dataset that has very high Precision, Recall, and F-score initially without using class reweighting and resampling.\n"]},{"cell_type":"markdown","metadata":{},"source":["This dataset contains tumor samples and we need to estimate/predict if a tumor sample is cancerous or not.\n","\n","First, like other scenarios, let's load and split the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["tumor_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/datasets/im_cancer.csv\", index_col=False)\n","X_train, X_test, y_train, y_test = split_data(tumor_df)"]},{"cell_type":"markdown","metadata":{},"source":["and check its class distribution:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_train.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, this dataset is not as imbalanced as previous datasets. So that it is possible we dont need to use class reweighting and resampling in order to have good classification performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["But we still create synthetic datasets with SMOTE and undersampling and class weights to compare their performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_smo, y_smo, X_under, y_under = resample(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Use tuned random forest parameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["best_params_weight = {'bootstrap': True,\n","                         'class_weight': {0: 0.2, 1: 0.8}, \n","                         'max_depth': 10, \n","                         'min_samples_split': 5, \n","                         'n_estimators': 50}"]},{"cell_type":"markdown","metadata":{},"source":["Train and evaluate different random forest models on the tumor type dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# no class-weights\n","results=[]\n","preds, model = build_rf(X_train, y_train, X_test)\n","results.append(evaluate(y_test, preds))\n","# class weight\n","preds, model = build_rf(X_train, y_train, X_test, best_params=best_params_weight)\n","results.append(evaluate(y_test, preds))\n","# Resampling\n","preds, model = build_rf(X_smo, y_smo, X_test)\n","results.append(evaluate(y_test, preds))\n","preds, model = build_rf(X_under, y_under, X_test)\n","results.append(evaluate(y_test, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["visualize_eval_metrics(results)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, all grey bars (the original model) for Accuracy, Recall, AUC, Precision, and F-score have very high values and adding class reweighting and resampling does not help improve performance that much. This is possible that the tumor patterns in this dataset are obvious and can be easily picked up by the standard random forest model or the dataset is not very imbalanced and can be considered as an ordinary binary classification task.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise: Predicting Job Change\n"]},{"cell_type":"markdown","metadata":{},"source":["Now it is time to apply what you have learned in this lab. We have an imbalanced HR dataset and we want to use it to build a classification model to predict if an employee is looking for a new job or not.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Load the dataset\n","hr_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML201EN-SkillsNetwork/labs/module_4/datasets/im_hr.csv\", index_col=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Check its class distribution\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["### Split the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["### Build a logistic regression model with the original dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["### Add class weights to the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["### Improve the model with SMOTE resampled dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["### Improve the model with Undersampling resampled dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# type your code here"]},{"cell_type":"markdown","metadata":{},"source":["<details><summary>Click here for a solution</summary>\n","\n","```python\n","y_train.value_counts().plot.bar(color=['green', 'red'])\n","best_params = {'class_weight': {0: 0.1, 1: 0.9}}\n","results = []\n","# no class-weights\n","preds, model = build_lr(X_train, y_train, X_test)\n","result = evaluate(y_test, preds)\n","results.append(result)\n","# class weight\n","preds, weight_model = build_lr(X_train, y_train, X_test, best_params=best_params)\n","result = evaluate(y_test, preds, eval_type=\"Class Weight\")\n","results.append(result)\n","# Resampling\n","preds, smote_model = build_lr(X_smo, y_smo, X_test)\n","result = evaluate(y_test, preds, eval_type=\"SMOTE\")\n","results.append(result)\n","preds_under, under_model = build_lr(X_under, y_under, X_test)\n","result = evaluate(y_test, preds_under, eval_type=\"Undersampling\")\n","#metrics.plot_roc_curve(smote_model, X_test, y_test) \n","results.append(result)\n","visualize_eval_metrics(results)\n","```\n","\n","</details>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
