{"cells":[{"cell_type":"markdown","id":"2363d2b5-7a45-4729-aefa-f0aedd8a8f02","metadata":{},"source":["# **Support Vector Machine**\n"]},{"cell_type":"markdown","id":"0af98779-3f59-4445-ad14-1954ec529137","metadata":{},"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"2edb927d-593f-4e35-999c-7970aca964cb","metadata":{},"source":["In this lab, you will learn and obtain hands-on practices on Support Vector Machine model.\n"]},{"cell_type":"markdown","id":"b65f52e5-c5fb-44a4-b106-302d48febba9","metadata":{},"source":["We will be using a real-world diabetes food items suggestion dataset, which contains detailed nutrition information about a food item. The objective is to classify what food a diabetic patient should choose More Often or Less Often for a specific food item given its nutrients.\n"]},{"cell_type":"markdown","id":"34f3baa0-c454-43e2-8c0c-9a702312d6fe","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"b12f38a7-6ab6-400b-b5be-94c9136ec7e2","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","id":"80c348ba-a0d2-4d39-93b1-5223602f6160","metadata":{},"source":["*   Train and evaluate SVM classifiers\n","*   Tune important SVM hyperparameters such as regularization and kernel types\n","*   Plot hyperplanes and margins from trained SVM models\n"]},{"cell_type":"markdown","id":"eeb377e7-a84a-4a72-bc02-eddaf045b148","metadata":{},"source":["## SVM Overview\n"]},{"cell_type":"markdown","id":"a25c0015-9123-4a0d-98d2-be82b12b73cd","metadata":{},"source":["SVM tries to find hyperplanes that have the maximum margin. The hyperplanes are determined by support vectors (data points have the smallest distance to the hyperplanes). Meanwhile, in order to reduce model variance, the SVM model aims to find the maximum possible margins so that unseen data will be more likely to be classified correctly.\n"]},{"cell_type":"markdown","id":"35d7935e-1a7e-4421-b488-3562270d5ac4","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/module3_svm/images/svm.png\" width=\"720\" alt=\"evaluation metrics\">\n","</center>\n"]},{"cell_type":"markdown","id":"73902216-1e01-484a-9451-0efd67afbc81","metadata":{},"source":["SVM addresses non-linear separable via kernel trick. Kernels are a special type of function that takes two vectors and returns a real number, like a dot-product operation. As such, kernels are not any real mapping functions from low dimensional spaces to high dimensional spaces.\n"]},{"cell_type":"markdown","id":"77a994aa-7906-4332-b363-2b674b06d152","metadata":{},"source":["For example, suppose we have two vectors $x = (x_{1}, x_{2})$ and $y = (y_{1}, y_{2})$\n"]},{"cell_type":"markdown","id":"2811dc4f-7974-497d-a256-ece6a94df4fc","metadata":{},"source":["Now we have a simple polynomial kernel like the following:\n"]},{"cell_type":"markdown","id":"9d33f584-ad29-4bf9-a388-1ee31fc2ad95","metadata":{},"source":["$$k(x, y) = (x^Ty)^2$$\n"]},{"cell_type":"markdown","id":"f4b04036-06bc-4b2a-9260-315d61d313d8","metadata":{},"source":["If we apply the kernel on vector `x` and `y`, we will get:\n"]},{"cell_type":"markdown","id":"7190430a-0b9d-40af-b095-53051cd3daa3","metadata":{},"source":["$$k(x, y) = (x^Ty)^2 = (x_{1}y_{1} + x_{2}y_{2})^2 = x_{1}^2y_{1}^2 + x_{2}^2y_{2}^2 + 2x_{1}x_{2}y_{1}y_{2}$$\n"]},{"cell_type":"markdown","id":"6d896fa8-6db0-4a03-8d55-287673d77936","metadata":{},"source":["It can be seen as a dot-product between two higher-dimensional vectors (`3-dimensional`):\n"]},{"cell_type":"markdown","id":"a7543913-7a69-415a-aed8-be11e0023d82","metadata":{},"source":["$$\\hat{x} = (x_{1}^2, x_{2}^2, \\sqrt{2}x_{1}x_{2}) $$\n"]},{"cell_type":"markdown","id":"29454f59-3be2-409f-aa5a-fcc705ec898c","metadata":{},"source":["$$\\hat{y} = (y_{1}^2, x_{2}^2, \\sqrt{2}y_{1}y_{2})$$\n"]},{"cell_type":"markdown","id":"5c6790e8-c069-4ad0-ba1d-1338ecdde8aa","metadata":{},"source":["As such, computing the `k(x, y)` is equivalent to computing a dot-product of the higher dimensional vectors, without doing the actual feature space transforms. Consequently, SVM with non-linear kernels can transform existing features into high dimensional features that can be linearly separated in higher dimensional spaces.\n"]},{"cell_type":"markdown","id":"07a78b84-a970-4bc7-9e21-4b53147f55d9","metadata":{},"source":["***\n"]},{"cell_type":"markdown","id":"01fee4c2-1a88-4477-8e6a-bda37d9693e6","metadata":{},"source":["## Setup lab environment\n"]},{"cell_type":"code","execution_count":null,"id":"84e511e0-7112-4cc0-b17e-e6714753ae01","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","\n","# install imbalanced-learn package\n","!pip install imbalanced-learn==0.8.0\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\" "]},{"cell_type":"code","execution_count":null,"id":"586f9147-bd4e-416b-980e-1996d4fb656a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Import required packages\n","import pandas as pd\n","import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import MinMaxScaler\n","# Evaluation metrics related methods\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from imblearn.under_sampling import RandomUnderSampler\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"id":"6657b06b-901e-499f-a1c1-e8f916af4905","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Setup a random seed to be 123\n","rs = 123"]},{"cell_type":"markdown","id":"1f5d9443-fdd8-49e2-b4ac-5deed59b0f06","metadata":{},"source":["## Load and explore dataset\n"]},{"cell_type":"markdown","id":"2ef8981b-9126-4f9c-aa0f-26dcc21e973c","metadata":{},"source":["Let's first load the dataset as a `Pandas` dataframe and conduct some basic explorations\n"]},{"cell_type":"code","execution_count":null,"id":"748c82e7-f481-4bf5-852e-dac6f6543d09","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Load the dataset\n","dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/food_items_binary.csv\"\n","food_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"1dfe8992-bb61-45ba-80df-06ededbc97b3","metadata":{},"source":["and let's quickly looks at its first 5 rows\n"]},{"cell_type":"code","execution_count":null,"id":"755a2e0b-bf51-4a13-ab3c-790ee30b812a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["food_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"98fcb4bc-4d16-4a32-9ff5-93c81a163c4a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Get the row entries with col 0 to -1 (16)\n","feature_cols = list(food_df.iloc[:, :-1].columns)\n","feature_cols"]},{"cell_type":"code","execution_count":null,"id":"d119b27d-4c68-41e3-9bc8-fd4f4a64fa43","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X = food_df.iloc[:, :-1]\n","y = food_df.iloc[:, -1:]"]},{"cell_type":"code","execution_count":null,"id":"3b90465e-5134-4f5c-bdd9-2da9560bb506","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","id":"aea69bae-8f4a-4656-8b26-49e58648b2d2","metadata":{},"source":["as we can see from the outputs above, this food item dataset contains 17 types of nutrients about a food item such as Calories, Total Fat, Protein, Sugar, and so on, as numeric variables.\n"]},{"cell_type":"markdown","id":"eaa87348-35e1-4480-b31f-60437af0879d","metadata":{},"source":["Next, let's check the target variable, such as the `class` column to see what are label values and their distribution.\n"]},{"cell_type":"code","execution_count":null,"id":"1fb46e94-f259-47cf-ac55-df970febcab0","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# # Get the row entries with the last col 'class'\n","y.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"id":"a9a5f59c-5fab-43b9-bf0b-27529e09c9d1","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y.value_counts().plot.bar(color=['red', 'green'])"]},{"cell_type":"markdown","id":"ad153a7d-9474-47e9-baba-84a5e79704aa","metadata":{},"source":["As we can see from the bar chart above, this dataset has two classes `Less Often` and `More Often`. The two labels are imbalanced with most food items should be chosen less often for diabetic patients.\n"]},{"cell_type":"markdown","id":"8cf399e6-318b-45e8-8713-88dc369c6c4b","metadata":{},"source":["## Build a SVM model with default parameters\n"]},{"cell_type":"markdown","id":"bf6f9da1-8c53-4762-a363-b2cbe9ff3cd0","metadata":{},"source":["First, let's split the training and testing dataset. Training dataset will be used to train and tune models, and testing dataset will be used to evaluate the models. Note that you may also split a validation dataset from the training dataset for model tuning only.\n"]},{"cell_type":"code","execution_count":null,"id":"5226e0cf-33b1-45d4-83c2-bc508ab6927b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# First, let's split the training and testing dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"d5703350-a06e-4cd4-8afc-854e61ec569b","metadata":{},"source":["Okay, now we have the training and testing datasets ready, let's start the model training task.\n"]},{"cell_type":"markdown","id":"a47b6155-07f1-404c-81c7-6a1e3789df41","metadata":{},"source":["We first define a `sklearn.svm import SVC` model with all default arguments.\n"]},{"cell_type":"code","execution_count":null,"id":"4dca459f-3880-499a-82b8-3f392083062a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = SVC()"]},{"cell_type":"markdown","id":"f29fa6e9-dd35-431f-9eaa-f56ab36c3b14","metadata":{},"source":["Then train the model with training dataset:\n"]},{"cell_type":"code","execution_count":null,"id":"b8657797-64c8-40f2-8eab-139118c51bc3","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"d412ad05-7087-452f-b478-c2123f6dd41d","metadata":{},"source":["and make predictions\n"]},{"cell_type":"code","execution_count":null,"id":"560d9db5-64db-4741-8c35-25465ab2898e","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["preds = model.predict(X_test)"]},{"cell_type":"markdown","id":"662992cf-17b5-4d70-ac62-b9ea196714e2","metadata":{},"source":["Here we defined a utility method to evaluate the model performance.\n"]},{"cell_type":"code","execution_count":null,"id":"ab0caedd-54fe-4cfc-8bc1-4bc6b5a5bfa2","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n","    results_pos = {}\n","    results_pos['accuracy'] = accuracy_score(yt, yp)\n","    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n","    results_pos['recall'] = recall\n","    results_pos['precision'] = precision\n","    results_pos['f1score'] = f_beta\n","    return results_pos"]},{"cell_type":"code","execution_count":null,"id":"fccf1a30-1c25-4cc6-91fc-39f769e33fd7","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"9a60a9d2-dc3c-42a3-a392-ad9bf0f79e8c","metadata":{},"source":["As we can see from the evaluation results above, the default SVM model achieves relatively good performance on this binary classification task. The overall accuracy is around `0.95` and the f1score is around `0.82`.\n"]},{"cell_type":"markdown","id":"e8cfce09-5c28-4bda-a980-1a2553128f70","metadata":{},"source":["Now, you have easily built a SVM model with relatively good performance. Can we achieve better classification performance by customizing the model?\n"]},{"cell_type":"markdown","id":"30d9142c-312a-4739-8a70-f1f4a68bb5ca","metadata":{},"source":["## Train SVM with different regularization parameters and kernels\n"]},{"cell_type":"markdown","id":"c4725a45-0403-4793-ad78-aa3bdfb4b0ac","metadata":{},"source":["The `SVC` model provided by sklearn has two important arguments to be tuned: regularization parameter `C` and `kernel`.\n"]},{"cell_type":"markdown","id":"a74ddaef-85bc-4c85-b750-2182ccecaac1","metadata":{},"source":["The `C` argument is a regularization parameter that controls the trade-off between achieving a low training error and keeping the decision boundary as simple as possible. \n","\n","*   For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly, which may cause the model to overfit.\n","*   Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points, but potentially better generalization to unseen data.\n"]},{"cell_type":"markdown","id":"2da8ca64-9a35-4487-852c-c040d28f8a5e","metadata":{},"source":["The `kernel` argument specifies the kernel to be used for transforming features to higher-dimensional spaces, some commonly used non-linear kernels are:\n","\n","*   `rbf`: Gaussian Radial Basis Function (RBF)\n","*   `poly`: Polynomial Kernel\n","*   `sigmoid`: Sigmoid Kernel\n"]},{"cell_type":"markdown","id":"254a8743-fca1-42aa-91e8-9642e18abbf4","metadata":{},"source":["Let's first try `C = 10` and ` kernel = 'rbf'  `\n"]},{"cell_type":"code","execution_count":null,"id":"84f4c425-5402-4e1a-81e5-7a90da5a7a18","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = SVC(C=10, kernel='rbf')\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"7df8ef95-2cb9-4e4f-a2a2-216819046d36","metadata":{},"source":["You should see that we have better performance than the default SVM model trained in the previous step. Now, it's your turn to try different parameters yourself.\n"]},{"cell_type":"markdown","id":"1c130d10-0a9b-4606-8736-566cbe22f533","metadata":{},"source":["### Coding Exercise: Try different `C` values and `kernels` to see which combination produces SVM models with better classification performance.\n"]},{"cell_type":"code","execution_count":null,"id":"9fc1f078-6761-4357-bc18-964dbed38d82","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Type your code here"]},{"cell_type":"markdown","id":"b72992ba-9ded-4af6-a17a-a8b4f2cf925c","metadata":{},"source":["## Tune regularization parameter C and Kernels via GridSearch\n"]},{"cell_type":"markdown","id":"26cb50e3-bb28-443b-8d4a-7af9402a9ddb","metadata":{},"source":["Exhaustively trying different hyperparameters by hands is infeasible. Thus, `sklearn` provides users with many automatic hyperparameter tuning methods. A popular one is grid search cross-validation `GridSearchCV`\n"]},{"cell_type":"markdown","id":"008b8953-8867-406d-8e8f-ed4398ad5220","metadata":{},"source":["Next, let's quickly try `GridSearchCV` to find the optimized `C` and `kernel` combination:\n"]},{"cell_type":"markdown","id":"6f7b665c-34b6-4bb0-938f-3e68dd43f5e9","metadata":{},"source":["We first define some candidate parameter values we want to search in a `dict` object, like the following setting:\n"]},{"cell_type":"code","execution_count":null,"id":"a16fc961-aff4-4ade-8aad-cc1d4035e118","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["params_grid = {\n","    'C': [1, 10, 100],\n","    'kernel': ['poly', 'rbf', 'sigmoid']\n","}"]},{"cell_type":"markdown","id":"030a29ce-e839-4393-9e89-07e73d56234a","metadata":{},"source":["Then, we define a  SVM model\n"]},{"cell_type":"code","execution_count":null,"id":"9296da9e-944d-4f59-80b2-0a329cadee7d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = SVC()"]},{"cell_type":"markdown","id":"753984f8-3ced-48b8-9a05-4c5b155f647c","metadata":{},"source":["and use create a `GridSearchCV` method to grid search `params_grid` and find the optimized combination with best `f1` score. The searching process may take several minutes to complete.\n"]},{"cell_type":"code","execution_count":null,"id":"7a69e5ed-adce-42ff-a238-3d9e2a8c0b43","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Define a GridSearchCV to search the best parameters\n","grid_search = GridSearchCV(estimator = model, \n","                           param_grid = params_grid, \n","                           scoring='f1',\n","                           cv = 5, verbose = 1)\n","# Search the best parameters with training data\n","grid_search.fit(X_train, y_train.values.ravel())\n","best_params = grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"id":"d7488196-3134-4aed-bc2b-4df2f99e4f1a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["best_params"]},{"cell_type":"markdown","id":"23fb58df-e92a-4078-b1ed-1492b8f2cd82","metadata":{},"source":["Okay, we can see `C=100` and `kernel=`rbf\\`\\` seems to produce the highest f1score. Let's quickly try this combination to see the model performance.\n"]},{"cell_type":"code","execution_count":null,"id":"8a6da4c2-4a05-4c47-be71-a4df32ca51f5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = SVC(C=100, kernel='rbf')\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"26240a07-159e-400c-827f-ed35aba6e1b2","metadata":{},"source":["The best f1score now becomes `0.88` after hyperparameter tuning.\n"]},{"cell_type":"markdown","id":"44d435f0-1f8a-45fc-8c49-9bbf665dacf1","metadata":{},"source":["## Plot SVM hyperplane and margin\n"]},{"cell_type":"markdown","id":"a20114f2-9565-43be-9474-9ad69278a0ba","metadata":{},"source":["Okay, you have learned how to define, train, evaluate, and fine-tune a SVM model with `sklearn`. However, so far we only obtained plain evaluation metrics and they are not intuitive to help us understand and interpret an SVM model. It would be great to visualize the see actual hyperplains and margins learned in an SVM model.\n"]},{"cell_type":"markdown","id":"fb09f428-1701-49f6-b677-d9d74d161f15","metadata":{},"source":["Since it is challenging to visualize a hyperplane higher than 3 dimensions. To illustrate the idea, we will focus on a 2-dimensional feature space.\n"]},{"cell_type":"markdown","id":"add4fca9-ccb0-4b98-a5ba-94960e191d6c","metadata":{},"source":["We first simplify the dataset with only two features `Calories` and `Dietary Fiber`, and include only 1000 instances:\n"]},{"cell_type":"code","execution_count":null,"id":"d54e7d54-3457-405b-a107-1b47c01356a1","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["simplified_food_df = food_df[['Calories', 'Dietary Fiber', 'class']]"]},{"cell_type":"code","execution_count":null,"id":"884fc9ff-a80e-4979-b181-6247301a791b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X = simplified_food_df.iloc[:1000, :-1].values\n","y = simplified_food_df.iloc[:1000, -1:].values"]},{"cell_type":"markdown","id":"40ece18e-01c9-4ecd-84ec-158420286dc9","metadata":{},"source":["and we undersample the majority class `Class = 0` to balance the class distribution so we will produce a clearer visualization.\n"]},{"cell_type":"code","execution_count":null,"id":"4a319f7f-a44b-400c-9c08-ac87bae7bdbf","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["under_sampler = RandomUnderSampler(random_state=rs)\n","X_under, y_under = under_sampler.fit_resample(X, y)"]},{"cell_type":"code","execution_count":null,"id":"cbced6bc-989c-45a7-a6c4-374d99db35ca","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(f\"Dataset resampled shape, X: {X_under.shape}, y: {y_under.shape}\")"]},{"cell_type":"markdown","id":"a2c2b032-9c11-41c6-97f9-48667643bbdf","metadata":{},"source":["To better show the hyperplane and margins, we normalize the features with a `MinMaxScaler`.\n"]},{"cell_type":"code","execution_count":null,"id":"b09bac5f-d2c6-462c-a4c4-462013b579ce","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["scaler = MinMaxScaler()\n","X_under = scaler.fit_transform(X_under)"]},{"cell_type":"markdown","id":"53212c2c-19d7-4aff-a759-47fd40a2daf7","metadata":{},"source":["Okay, let's first train a linear SVM model with `kernel=linear` so that we can get a linear hyperplane and margins.\n"]},{"cell_type":"code","execution_count":null,"id":"18157b41-4bdc-4089-b898-18bba649ad9a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["linear_svm = SVC(C=1000, kernel='linear')\n","linear_svm.fit(X_under, y_under)"]},{"cell_type":"markdown","id":"fcb7c8f4-0c6c-4f50-a9db-f5d5a145223d","metadata":{},"source":["Here we also provided an utility method to plot the decision boundary (hyperplane), support vectors, and margins. You may write your own visualization method if you are interested.\n"]},{"cell_type":"code","execution_count":null,"id":"f2b7fd10-27d3-4703-858d-b82b56b211b4","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def plot_decision_boundry(X, y, model):\n","    plt.figure(figsize=(16, 12))\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","\n","    # plot the decision function\n","    ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # create grid to evaluate model\n","    xx = np.linspace(xlim[0], xlim[1], 30)\n","    yy = np.linspace(ylim[0], ylim[1], 30)\n","    YY, XX = np.meshgrid(yy, xx)\n","    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","    Z = model.decision_function(xy).reshape(XX.shape)\n","\n","    # plot decision boundary and margins\n","    ax.contour(\n","        XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"]\n","    )\n","\n","    # plot support vectors\n","    ax.scatter(\n","        model.support_vectors_[:, 0],\n","        model.support_vectors_[:, 1],\n","        s=100,\n","        linewidth=1,\n","        facecolors=\"none\",\n","        edgecolors=\"k\",\n","    )\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"035051b3-bec5-497f-a11f-cd7c21ae1d1b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plot_decision_boundry(X_under, y_under, linear_svm)"]},{"cell_type":"markdown","id":"aba1112e-405c-4bf4-a9f2-c185a96782c9","metadata":{},"source":["Okay, we can see a clear linear hyperplane seperates the two classes (Blue dots vs Orange dots). The highlighted dots are the support vectors determining the hyperplain.\n"]},{"cell_type":"markdown","id":"9cfca041-6fef-4fd2-9b40-bb2fbdca8418","metadata":{},"source":["If we want to include non-linear kernels, we should get a non-linear decision boundary in the 2-d space (maybe linear in higher feature space). So here we use a `rbf` kernel:\n"]},{"cell_type":"code","execution_count":null,"id":"92819c9c-6041-4fca-b266-efed539015db","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["svm_rbf_kernel = SVC(C=100, kernel='rbf')\n","svm_rbf_kernel.fit(X_under, y_under)"]},{"cell_type":"code","execution_count":null,"id":"ec3c21c7-5b7a-4bdb-8c5d-8259919e32c6","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plot_decision_boundry(X_under, y_under, svm_rbf_kernel)"]},{"cell_type":"markdown","id":"2907bb97-d99a-46ff-bf15-f857cdf9b52b","metadata":{},"source":["We now see a non-linear hyperplane and margins separating the two classes.\n"]},{"cell_type":"markdown","id":"aff264f6-ed19-4e41-a2c6-c6601093bfad","metadata":{},"source":["### Coding Exercise: Try different `C` values and `kernels` to see the how they affect the hyperplanes and margins.\n"]},{"cell_type":"code","execution_count":null,"id":"ae4bcc7a-89e7-4c77-affc-efa190f4762a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Type your code here\n"]},{"cell_type":"markdown","id":"a2f13ade-0bef-40ed-8c9d-0145bffb70b0","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","svm_rbf_kernel = SVC(C=100, kernel='poly')\n","svm_rbf_kernel.fit(X_under, y_under)\n","plot_decision_boundry(X_under, y_under, svm_rbf_kernel)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"d5522a10-8fa1-4400-b573-6bb3429ac5d1","metadata":{},"source":["## Next Steps\n"]},{"cell_type":"markdown","id":"c370d65e-541a-42ec-a5f8-578288848cfa","metadata":{},"source":["Great! Now you have learned and practiced SVM model and applied it to solve a real-world food classification problem for diabetic patients. You also learned how to visualize the hyperplanes and margins generated by the SVM models.\n"]},{"cell_type":"markdown","id":"75a2f9a9-2354-4af0-8906-6288ab357fbc","metadata":{},"source":["Next, you will be learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
