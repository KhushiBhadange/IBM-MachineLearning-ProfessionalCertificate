{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 3, Part c: Support Vector Machines DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data.csv`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-04-10T00:04:57.164238Z","start_time":"2017-04-09T20:04:57.158472-04:00"},"vscode":{"languageId":"python"}},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1: Setup\n","\n","* Import the data.\n","* Create the target variable `y` as a 1/0 column where 1 means red.\n","* Create a `pairplot` for the dataset.\n","* Create a bar plot showing the correlations between each column and `y`\n","* Pick the most 2 correlated fields (using the absolute value of correlations) and create `X`\n","* Use MinMaxScaler to scale `X`. Note that this will output a np.array. Make it a DataFrame again and rename the columns appropriately.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-04-10T00:04:57.731417Z","start_time":"2017-04-09T20:04:57.168224-04:00"},"vscode":{"languageId":"python"}},"outputs":[],"source":["data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Wine_Quality_Data.csv\", sep=',')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-04-10T00:04:57.769148Z","start_time":"2017-04-09T20:04:57.734768-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["y = (data['color'] == 'red').astype(int)\n","fields = list(data.columns[:-1])  # everything except \"color\"\n","correlations = data[fields].corrwith(y)\n","correlations.sort_values(inplace=True)\n","correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["sns.set_context('talk')\n","#sns.set_palette(palette)\n","sns.set_style('white')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["sns.pairplot(data, hue='color')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["ax = correlations.plot(kind='bar')\n","ax.set(ylim=[-1, 1], ylabel='pearson correlation');"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","fields = correlations.map(abs).sort_values().iloc[-2:].index\n","print(fields)\n","X = data[fields]\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","X = pd.DataFrame(X, columns=['%s_scaled' % fld for fld in fields])\n","print(X.columns)"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: Linear Decision Boundary\n","\n","Our goal is to look at the decision boundary of a LinearSVC classifier on this dataset. Check out [this example](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork820-2023-01-01#sphx-glr-auto-examples-svm-plot-iris-svc-py) in sklearn's documentation. \n","\n","* Fit a Linear Support Vector Machine Classifier to `X`, `y`.\n","* Pick 300 samples from `X`. Get the corresponding `y` value. Store them in variables `X_color` and `y_color`. This is because original dataset is too large and it produces a crowded plot.\n","* Modify `y_color` so that it has the value \"red\" instead of 1 and 'yellow' instead of 0.\n","* Scatter plot X_color's columns. Use the keyword argument \"color=y_color\" to color code samples.\n","* Use the code snippet below to plot the decision surface in a color coded way.\n","\n","```python\n","x_axis, y_axis = np.arange(0, 1, .005), np.arange(0, 1, .005)\n","xx, yy = np.meshgrid(x_axis, y_axis)\n","xx_ravel = xx.ravel()\n","yy_ravel = yy.ravel()\n","X_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\n","y_grid_predictions = *[YOUR MODEL]*.predict(X_grid)\n","y_grid_predictions = y_grid_predictions.reshape(xx.shape)\n","ax.contourf(xx, yy, y_grid_predictions, cmap=colors, alpha=.3)\n","```\n","\n","With LinearSVC, it is easy to experiment with different parameter choices and see the decision boundary.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","\n","LSVC = LinearSVC()\n","LSVC.fit(X, y)\n","\n","X_color = X.sample(300, random_state=45)\n","y_color = y.loc[X_color.index]\n","y_color = y_color.map(lambda r: 'red' if r == 1 else 'yellow')\n","ax = plt.axes()\n","ax.scatter(\n","    X_color.iloc[:, 0], X_color.iloc[:, 1],\n","    color=y_color, alpha=1)\n","# -----------\n","x_axis, y_axis = np.arange(0, 1.005, .005), np.arange(0, 1.005, .005)\n","xx, yy = np.meshgrid(x_axis, y_axis)\n","xx_ravel = xx.ravel()\n","yy_ravel = yy.ravel()\n","X_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\n","y_grid_predictions = LSVC.predict(X_grid)\n","y_grid_predictions = y_grid_predictions.reshape(xx.shape)\n","ax.contourf(xx, yy, y_grid_predictions, cmap=plt.cm.autumn_r, alpha=.3)\n","# -----------\n","ax.set(\n","    xlabel=fields[0],\n","    ylabel=fields[1],\n","    xlim=[0, 1],\n","    ylim=[0, 1],\n","    title='decision boundary for LinearSVC');"]},{"cell_type":"markdown","metadata":{},"source":["## Part 3: Gaussian Kernel\n","\n","Let's now fit a Gaussian kernel SVC and see how the decision boundary changes.\n","\n","* Consolidate the code snippets in Question 2 into one function which takes in an estimator, `X` and `y`, and produces the final plot with decision boundary. The steps are:\n","    <ol>\n","     <li> Fit model\n","     <li> Get sample 300 records from X and the corresponding y's\n","     <li> Create grid, predict, plot using ax.contourf\n","     <li> Add on the scatter plot\n","    </ol>\n","* After copying and pasting code, the finished function uses the input `estimator` and not the LinearSVC model.\n","* For the following values of `gamma`, create a Gaussian Kernel SVC and plot the decision boundary.  \n","`gammas = [.5, 1, 2, 10]`\n","* Holding `gamma` constant, we plot the decision boundary for various values of `C`: \n","`[.1, 1, 10]`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def plot_decision_boundary(estimator, X, y):\n","    estimator.fit(X, y)\n","    X_color = X.sample(300)\n","    y_color = y.loc[X_color.index]\n","    y_color = y_color.map(lambda r: 'red' if r == 1 else 'yellow')\n","    x_axis, y_axis = np.arange(0, 1, .005), np.arange(0, 1, .005)\n","    xx, yy = np.meshgrid(x_axis, y_axis)\n","    xx_ravel = xx.ravel()\n","    yy_ravel = yy.ravel()\n","    X_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\n","    y_grid_predictions = estimator.predict(X_grid)\n","    y_grid_predictions = y_grid_predictions.reshape(xx.shape)\n","\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    ax.contourf(xx, yy, y_grid_predictions, cmap=plt.cm.autumn_r, alpha=.3)\n","    ax.scatter(X_color.iloc[:, 0], X_color.iloc[:, 1], color=y_color, alpha=1)\n","    ax.set(\n","        xlabel=fields[0],\n","        ylabel=fields[1],\n","        title=str(estimator))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","gammas = [.5, 1, 2, 10]\n","for gamma in gammas:\n","    SVC_Gaussian = SVC(kernel='rbf', gamma=gamma)\n","    plot_decision_boundary(SVC_Gaussian, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["Cs = [.1, 1, 10]\n","for C in Cs:\n","    SVC_Gaussian = SVC(kernel='rbf', gamma=2, C=C)\n","    plot_decision_boundary(SVC_Gaussian, X, y)"]},{"cell_type":"markdown","metadata":{},"source":["## Part 4: Comparing Kernel Execution Times\n","\n","In this exercise, we will compare the fitting times between SVC vs Nystroem with rbf kernel.  \n","<br><br>\n","Jupyter Notebooks provide a useful magic function **`%timeit`** which executes a line and prints out the time it took to fit. If we type **`%%timeit`** in the beginning of the cell, it will output the execution time.\n","\n","We proceed with the following steps:\n","* Create `y` from data.color, and `X` from the rest of the columns.\n","* Use `%%timeit` to get the time for fitting an SVC with rbf kernel.\n","* Use `%%timeit` to get the time for the following: fit_transform the data with Nystroem and then fit a SGDClassifier.\n","\n","Nystroem+SGD will take much less to fit. This difference will be more pronounced if the dataset was bigger.\n","\n","* Make 5 copies of X and concatenate them\n","* Make 5 copies of y and concatenate them\n","* Compare the time it takes to fit the both methods above\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.kernel_approximation import Nystroem\n","from sklearn.svm import SVC\n","from sklearn.linear_model import SGDClassifier\n","\n","y = data.color == 'red'\n","X = data[data.columns[:-1]]\n","\n","kwargs = {'kernel': 'rbf'}\n","svc = SVC(**kwargs)\n","nystroem = Nystroem(**kwargs)\n","sgd = SGDClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["%%timeit\n","svc.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["%%timeit\n","X_transformed = nystroem.fit_transform(X)\n","sgd.fit(X_transformed, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["X2 = pd.concat([X]*5)\n","y2 = pd.concat([y]*5)\n","\n","print(X2.shape)\n","print(y2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["%timeit svc.fit(X2, y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"vscode":{"languageId":"python"}},"outputs":[],"source":["%%timeit\n","X2_transformed = nystroem.fit_transform(X2)\n","sgd.fit(X2_transformed, y2)"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
