{"cells":[{"cell_type":"markdown","id":"84902af4-a9b7-4d69-b4c6-881c24d1f2cb","metadata":{},"source":["# **Categorical Cross-Entropy Loss**\n"]},{"cell_type":"markdown","id":"ba0281fd-a93d-4cdb-a250-26ae13483452","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","id":"ce8c2619-e91a-4017-aad1-b229aa750608","metadata":{},"source":["In this lab, we are going to use the MNIST hand-written digits dataset as a motivating example to understand categorical cross-entropy loss.\n"]},{"cell_type":"markdown","id":"670bb967-f2db-40f7-9f7b-97a9c89b93af","metadata":{},"source":["<h1> House Number Detection </h1></s>\n","\n","You have been tasked with building a model that can recognize house numbers from arbitrary street-view images. You are given a set of images of single-digit house numbers engraved into slates or wall surfaces. The images vary in size and color. In this lab, we are going to use the MNIST hand-written digits dataset as a motivating example to understand the __softmax function__, __one-hot encoding__, and __categorical cross-entropy loss__. The MNIST hand-written dataset has 10 classes, each representing a digit from 0-9. We will attempt to build a multi-class classification model that will identify which digit is present in the image. \n"]},{"cell_type":"markdown","id":"a0212564-bc65-4424-b2bf-da2414944dca","metadata":{},"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/house_number.jpg\" style=\"width: 30%\">\n","\n","<!-- Original Source: https://slate.com/human-interest/2020/06/a-quest-to-catalogue-every-single-house-number-in-one-suburban-zip-code.html --> \n"]},{"cell_type":"markdown","id":"cd1ae5e5-5063-41c4-ae69-363b525877a8","metadata":{},"source":["## **Table of Contents**\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li><a href=\"https://#Datasets\">Datasets</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://https://#Categorical Cross-Entropy\">Categorical Cross-Entropy</a>\n","        <ol>\n","            <li><a href=\"https://#Softmax Activation Function\">Softmax Activation Function</a></li>\n","            <li><a href=\"https://#One-Hot Encoding\">One-Hot Encoding</a></li>\n","            <li><a href=\"#Categorical Cross-Entropy\">Categorical Cross-Entropy</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://#Binary vs. Multi-Class Classification\">Binary vs. Multi-Class Classification</a>\n","        <ol>\n","            <li><a href=\"https://#Binary Classification\">Binary Classification</a></li>\n","            <li><a href=\"https://#Multi-Class Classification\">Multi-Class Classification</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Example: MNIST Hand-Written Digits\">Example: MNIST Hand-Written Digits</a> </li>\n","    <li>\n","        <a href=\"https://#Example: Single-Digit House Number Recognition\"> Example: Single-Digit House Number Recognition</a> </li>\n","    <li><a href=\"https://#Exercises\">Exercises</a>\n","        <ol>\n","            <li><a href=\"https://#Exercise 1 - Loading and plotting the images\">Exercise 1 - Loading and plotting the images</a></li>\n","            <li><a href=\"https://#Exercise 2 - Preparing the data\">Exercise 2 - Preparing the data</a></li>\n","            <li><a href=\"https://#Exercise 3 - One-hot encoding\">Exercise 3 - One-hot encoding</a></li>\n","            <li><a href=\"https://#Exercise 4 - Build model architecture\">Exercise 4 - Build model architecture</a></li>\n","            <li><a href=\"https://#(Optional) Exercise 5 - Sparse Categorical Cross-Entropy\">Sparse Categorical Cross-Entropy</a></li>\n","        </ol>\n","    </li>\n","</ol>\n"]},{"cell_type":"markdown","id":"3ab84007-4c13-45ef-aa7a-49c4a9c6206e","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   **Understand** what categorical cross-entropy is, and how it works with the Softmax activation function.\n","*   **Build** simple CNN models for binary and multi-class classification.\n"]},{"cell_type":"markdown","id":"de8a04ee-d7fd-4876-aa4c-5fd0f60c1646","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"999d07c6-a64f-4e29-9bf4-f06510935973","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","*   [`keras`](https://keras.io/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for loading datasets.\n"]},{"cell_type":"markdown","id":"7dcff019-bd2f-4dc5-abf5-3bf62feb7611","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip install mlxtend` in the following code cell.\n"]},{"cell_type":"markdown","id":"1f3688ea-98b1-4e60-86c0-63cc99dc670c","metadata":{},"source":["The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You will need to run the following cell** to install them. **Please RESTART KERNEL after installation,**.\n"]},{"cell_type":"code","execution_count":null,"id":"0886f3c9-585d-42e6-b193-b70fc75fdd60","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["%%capture\n","\n","!pip install mlxtend\n","!pip install --upgrade tensorflow"]},{"cell_type":"markdown","id":"0b233019-1d4b-4d85-b35d-b42a96b3a0f5","metadata":{},"source":["### Importing Required Libraries\n"]},{"cell_type":"code","execution_count":null,"id":"1a44ff15-1ab5-4a6a-99ed-55eedd4e423f","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import cv2\n","from urllib.request import urlopen\n","from PIL import Image\n","import IPython\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd, numpy as np\n","from keras.datasets import mnist, fashion_mnist\n","import random\n","from sklearn.preprocessing import OneHotEncoder\n","from tensorflow.keras.losses import CategoricalCrossentropy,SparseCategoricalCrossentropy,BinaryCrossentropy\n","from sklearn.datasets import make_blobs\n","from mlxtend.plotting import plot_decision_regions\n","import tensorflow as tf\n","print(tf.__version__)\n","from sklearn import preprocessing\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"markdown","id":"eb506408-753b-4f7c-be16-41c6ab41c497","metadata":{},"source":["### Defining Helper Functions\n","\n","*Use this section to define any helper functions to help the notebook's code readability:*\n"]},{"cell_type":"code","execution_count":null,"id":"5dc10339-bb5e-4a6c-9556-fb798201c21c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def generate_multiclass_blobs(num_samples_total, training_split, cluster_centers, num_classes, loss_function_used):\n","    X, targets = make_blobs(n_samples = num_samples_total, centers = cluster_centers, n_features = num_classes, center_box=(0, 1), cluster_std = 1.5)\n","    categorical_targets = to_categorical(targets)\n","    X_training = X[training_split:, :]\n","    X_testing = X[:training_split, :]\n","    Targets_training = categorical_targets[training_split:]\n","    Targets_testing = categorical_targets[:training_split].astype(np.int32)\n","    return X_training, Targets_training, X_testing, Targets_testing\n","\n","\n","def generate_binary_blobs(num_samples_total, training_split, loss_function_used):\n","    X, targets = make_blobs(n_samples = num_samples_total, centers = [(0,0), (15,15)], n_features = 2, center_box=(0, 1), cluster_std = 2.5)\n","    targets[np.where(targets == 0)] = -1\n","    X_training = X[training_split:, :]\n","    X_testing = X[:training_split, :]\n","    Targets_training = targets[training_split:]\n","    Targets_testing = targets[:training_split]\n","    return X_training, Targets_training, X_testing, Targets_testing"]},{"cell_type":"markdown","id":"9e95d0b0-36bb-49ad-bd84-38305bac56c2","metadata":{},"source":["## Categorical Cross-Entropy\n"]},{"cell_type":"markdown","id":"e7f92508-544d-4101-b142-d62dcba45e0c","metadata":{},"source":["When working on a machine learning problem, we use loss functions to optimize our models during training where a common objective is to minimize the loss function.\n","\n","Cross-entropy is a widely used loss or cost function, that is used to optimize classification models. Before delving into cross-entropy, let us first cover the prerequisites by learning about a common activation function called Softmax.\n"]},{"cell_type":"markdown","id":"3f9938cc-3181-47fb-82e0-d3a5a372b752","metadata":{},"source":["### Softmax Activation Function\n"]},{"cell_type":"markdown","id":"74c15689-fe35-456d-9aad-5aab4927fd31","metadata":{},"source":["Activation functions are transformations applied to the output from CNNs before loss computations.\n"]},{"cell_type":"markdown","id":"a249707a-e39c-444e-acb1-a6206bf0a7f8","metadata":{},"source":["The Softmax activation function is typically placed as the last layer in a neural network and used to normalize the output of a network to a probability distribution over predicted output classes.\n","\n","It does so by scaling numbers/logits into probabilities for each possible outcome or class present in our dataset. The resulting probabilities in the vector sum up to one.\n","\n","Mathematically, Softmax is defined as follows:\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/softmax.png\" style=\"width: 30%\">\n"]},{"cell_type":"markdown","id":"2c56526a-96af-4f75-841b-bcbd4a308137","metadata":{},"source":["Here, $\\overrightarrow{{z}}$ is an input vector to the Softmax function $\\sigma$.\n","\n","$\\overrightarrow{{z}}\\_i$ represents the $i$th element of the input vector, and can take on values from -inf to inf.\n","\n","$\\overrightarrow{{z}}_i$ represents the $i$th element of the input vector, and can take on values from -inf to inf. \n","\n","$e^{z_i}$ is a standard exponential function applied on the $i$th element of $z$, and the denominator is a normalizing term (L1-norm) to ensure the result is a valid probability distribution, that is, sums up to 1, and values are within the 0 - 1 range.\n"]},{"cell_type":"markdown","id":"1a285a9c-4763-40e6-8d73-7deedfcb0f03","metadata":{},"source":["Exponential is a steeply increasing function; that is, it increases the difference between outputs. In the final output, the largest element (which dominates the norm) is normalized to a value close to 1, while all the other elements end up being close to 0. Not only does the resulting vector show the winning class, but it also retains the original order of values.\n","\n","\n","Another important point to note is that Softmax is not affected by negative values, as the exponent of any value is always a positive value.\n"]},{"cell_type":"markdown","id":"f3145575-15d8-4520-a134-e839cae05873","metadata":{},"source":["Let us walk through an example to understand Softmax.\n"]},{"cell_type":"markdown","id":"1ec5eb79-cbbe-4ea3-9461-67003a1bffdc","metadata":{},"source":["Imagine you are building a CNN model to classify an image as a dog, cat, fish, or horse. The fully-connected layer of your CNN gives a vector of logits. You pass the vector through the Softmax function above to obtain probabilities.\n"]},{"cell_type":"markdown","id":"99c97c05-ef4d-4484-8174-6fabbba59955","metadata":{},"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/cce_diag.png)\n"]},{"cell_type":"markdown","id":"a54021b3-71b3-46e5-b9df-c4020bb0038c","metadata":{},"source":["### One-Hot Encoding\n"]},{"cell_type":"markdown","id":"59740782-b678-4c01-9384-1230876c7555","metadata":{},"source":["The ground truth is typically presented in form of categorical data; that is, a given image is categorized into one of these classes: dog, cat, fish, or horse.\n"]},{"cell_type":"markdown","id":"9d07e35a-ddce-4d6e-aef3-4be6a6d0e506","metadata":{},"source":["For categorical variables where no ordinal relationships exist, we can perform one-hot encoding to represent each class. This is best explained through an example.\n","\n","In our case, we will have four classes, and these are the corresponding one-hot encoded labels:\n"]},{"cell_type":"markdown","id":"7d26645a-153e-43ce-a817-2799ac39acfa","metadata":{},"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/ohe_labels.png\" style=\"width: 50%\">\n"]},{"cell_type":"markdown","id":"3ae779d3-ec1d-44ec-abca-0e25cfdb0866","metadata":{},"source":["So in one-hot encoding, we represent labels with a binary variable where for every given class, we have the value 1 for the position corresponding to that particular class and 0 elsewhere (that is, 100% probability of belonging to that class).\n"]},{"cell_type":"markdown","id":"37ae9a85-c30f-4a27-906b-6cb5abb54f41","metadata":{},"source":["### Back to Categorical Cross-Entropy\n"]},{"cell_type":"markdown","id":"0d82d63d-691e-4ede-aed9-596bc6acecc7","metadata":{},"source":["Now that we have converted the logits to output probabilities, we need to measure how good they are; that is, measure the distance from the truth values.\n"]},{"cell_type":"markdown","id":"b8961ab5-95d7-4802-9723-604a1611e3ae","metadata":{},"source":["For instance, in the example above, the desired output is [1,0,0,0], but the model outputs [0.775,0.116,0.039,0.070].\n"]},{"cell_type":"markdown","id":"7bc78d4b-9a6c-4e1a-9ea7-c9bae2f95228","metadata":{},"source":["The categorical cross-entropy is mathematically defined as follows:\n"]},{"cell_type":"markdown","id":"340d1bc6-e73a-4404-bbdf-d3205e553d29","metadata":{},"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/cce.png\" style=\"width: 30%\">\n"]},{"cell_type":"markdown","id":"7855de58-f840-45f3-b8ff-07cbde13f9d1","metadata":{},"source":["We can use the formula above to compute the cross-entropy loss. When training our model, we iteratively update the weights to minimize the cross-entropy loss. \n","\n","The Softmax is continuously differentiable, and this property makes it easy to compute the derivative of the loss function and, accordingly, adjust the model weights in each iteration to minimize the loss function.\n","\n","In the next iteration, if our $$L\\_{CE}$$ is lower than the current one, we say that the model is *learning*.\n"]},{"cell_type":"markdown","id":"185262f8-cb0c-44e8-938c-c139b27e3c19","metadata":{},"source":["## Binary vs. Multi-Class Classification\n"]},{"cell_type":"markdown","id":"8ac969a9-7abf-4e58-a7e4-4107011520ed","metadata":{},"source":["Now, let us do a quick review of binary and multi-class classification. Binary classification tasks require that all examples be assigned to one of two classes, whereas in multi-class classification, examples can belong to more than two classes.\n","\n","For binary classification problems, we have a final layer with a single node and a sigmoid activation function. It can map the output vector from a CNN to values between 0 and 1 before loss computations. The sigmoid function is denoted using the following formula:\n","\n","$$\\sigma(x) = 1/(1+e^{-x})$$\n"]},{"cell_type":"markdown","id":"1744d7a9-a002-4931-9099-f470d26a1403","metadata":{},"source":["Let's see binary classification and multi-class classification in action.\n"]},{"cell_type":"markdown","id":"271d4ab2-4f19-41b7-b4dd-e42a9ff225d1","metadata":{},"source":["### Binary Classification\n"]},{"cell_type":"markdown","id":"bef83705-8c14-449e-aaae-220782358544","metadata":{},"source":["Using a helper function defined at the beginning of the notebook, we will use the `make_blobs()` function from `sklearn` to generate isotropic Gaussian blobs for classification.\n","\n","We will create a data set with 1000 samples, and use 750 of those samples for training a simple CNN model.\n"]},{"cell_type":"markdown","id":"96137c0e-58f4-4d3a-9351-da1e0777e4c0","metadata":{},"source":["Let us define a few more configuration options.\n"]},{"cell_type":"markdown","id":"ba31a566-dd89-4023-a1d8-701d9b9ef40e","metadata":{},"source":["*   **num_samples**: refers to the total number of samples in our dataset\n","*   **test_split**: refers to the number of samples to be used for testing\n","*   **cluster_centers**: we define 2 centers for our isotropic Gaussian blobs\n","*   **num_classes**: we have 2 classes\n","*   **loss_function_used**: since this is a binary classification problem, we use binary cross-entropy loss\n"]},{"cell_type":"code","execution_count":null,"id":"4b44df89-b83c-4688-8b71-13079df8e90a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["num_samples = 1000\n","test_split = 250\n","cluster_centers = [(15,0), (30,15)]\n","num_classes = len(cluster_centers)\n","loss_function_used = BinaryCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"id":"19cfb026-f80f-4a36-a2f6-beead1280b6d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_training, Targets_training, X_testing, Targets_testing=generate_binary_blobs(num_samples, test_split, loss_function_used)\n"]},{"cell_type":"markdown","id":"3733c644-bb0f-40e8-8409-e5412ab452df","metadata":{},"source":["We can take look at one of the samples.\n"]},{"cell_type":"code","execution_count":null,"id":"9fc89246-18b1-4882-9dea-9467ad179754","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plt.figure(figsize=(4, 4))\n","plt.scatter(X_training[:,0], X_training[:,1])\n","plt.title('Linearly separable data')\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","plt.show()"]},{"cell_type":"markdown","id":"bf34da83-db60-47f7-9ac9-2d8c61f0ca37","metadata":{},"source":["We will build a simple CNN model with two hidden layers, that uses sigmoid as the activation function, and binary cross-entropy as the loss function. Let's define the architecture of our model:\n"]},{"cell_type":"code","execution_count":null,"id":"cbc44b71-2ac9-44b2-853f-ce558e3a650c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_vector_shape = X_training.shape[1]\n","input_shape = (feature_vector_shape,)\n","\n","model = Sequential()\n","model.add(Dense(12, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation = 'sigmoid'))"]},{"cell_type":"code","execution_count":null,"id":"8efa3a52-3bd4-4305-816e-d1312777e107","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.01), metrics=['accuracy'])\n","history = model.fit(X_training, Targets_training, epochs=30, batch_size=5, verbose=1, validation_split=0.2)"]},{"cell_type":"markdown","id":"3d384cf0-6583-44a5-9dac-be4c82e38e8b","metadata":{},"source":["We will now use the trained model to make predictions on an unseen test dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"8dd63381-6398-477f-963a-646644627f38","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["test_results = model.evaluate(X_testing, Targets_testing, verbose=1)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')"]},{"cell_type":"code","execution_count":null,"id":"86d5c15d-cf99-447a-a5ac-37f1c55c7b6d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plot_decision_regions(X_testing, Targets_testing, clf=model, legend=2)\n","plt.figure(figsize=(4, 4))\n","plt.show()"]},{"cell_type":"markdown","id":"5f1faf4a-f106-4a71-93a0-97b68ffb8914","metadata":{},"source":["The model successfully classifies the two clusters into two classes.\n"]},{"cell_type":"markdown","id":"0621f6b9-667e-45cb-abe3-6e4ca74b0d9a","metadata":{},"source":["### Multi-Class Classification\n"]},{"cell_type":"markdown","id":"db39c23c-4c8b-475a-9ee9-8ac01e145b1c","metadata":{},"source":["Similar to binary classification, let's create another data set with 1000 samples, and use 750 of those samples for training a simple CNN model.\n"]},{"cell_type":"markdown","id":"b77a8f29-6559-4dcb-92b2-720d40f32d81","metadata":{},"source":["Let us define a few more configuration options.\n"]},{"cell_type":"markdown","id":"65104ceb-ba98-46d3-9557-8cc9646461df","metadata":{},"source":["*   **num_samples**: refers to the total number of samples in our dataset\n","*   **test_split**: refers to the number of samples to be used for testing\n","*   **cluster_centers**: we define 3 centers for our isotropic Gaussian blobs\n","*   **num_classes**: we have 3 classes\n","*   **loss_function_used**: since this is a multi-class classification problem, we use categorical cross-entropy loss\n"]},{"cell_type":"code","execution_count":null,"id":"c627cd7c-2cd2-47f4-9b52-d42361895db5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["num_samples = 1000\n","train_split = 250\n","cluster_centers = [(-10, 5), (0, 0), (10, 5)]\n","num_classes = len(cluster_centers)\n","loss_function_used = CategoricalCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"id":"c846e5da-2738-4e5d-82b0-9c37d60034b5","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_training, Targets_training, X_testing, Targets_testing= generate_multiclass_blobs(num_samples, train_split,\n","              cluster_centers, num_classes,\n","              loss_function_used)"]},{"cell_type":"markdown","id":"901260f6-19ae-412b-9ae3-13945db012d1","metadata":{},"source":["Let's take a look at one of the samples.\n"]},{"cell_type":"code","execution_count":null,"id":"b85c7ad0-c8fa-489e-b101-307a1a2288f8","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plt.scatter(X_training[:,0], X_training[:,1])\n","plt.title('Linearly separable data')\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","plt.show()"]},{"cell_type":"markdown","id":"27118b74-f43e-42e1-af3d-530800f2e19f","metadata":{},"source":["We will build a simple CNN model with two hidden layers, that uses softmax as the activation function, and categorical cross-entropy as the loss function. Let's define the architecture of our model:\n"]},{"cell_type":"code","execution_count":null,"id":"4676db5b-4a1e-4102-af2f-7af0a9db1029","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_vector_shape = X_training.shape[1]\n","input_shape = (feature_vector_shape,)\n","\n","model = Sequential()\n","model.add(Dense(12, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"id":"aa25e8f4-75f1-48eb-b807-12dde5c68e18","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n","history = model.fit(X_training, Targets_training, epochs=30, batch_size=5, verbose=1, validation_split=0.2)"]},{"cell_type":"markdown","id":"93f231ca-5c1a-4c02-a071-003edf5b9bfc","metadata":{},"source":["We will now use the trained model to make predictions on an unseen test dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"a8cb2415-6a3f-42a6-b09a-6cac2c8e0bd6","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["test_results = model.evaluate(X_testing, Targets_testing, verbose=1)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')"]},{"cell_type":"code","execution_count":null,"id":"983ca4aa-0d88-41bb-8664-477f4c40ffd4","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["class Onehot2Int(object):\n","\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def predict(self, X):\n","        y_pred = self.model.predict(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","# fit keras_model\n","keras_model_no_ohe = Onehot2Int(model)"]},{"cell_type":"code","execution_count":null,"id":"b60c0f66-228d-4e12-b2cd-c51ac5e8a60e","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Plot decision boundary\n","plot_decision_regions(X_testing, np.argmax(Targets_testing, axis=1), clf=keras_model_no_ohe, legend=3)\n","plt.show()"]},{"cell_type":"markdown","id":"5b74bb94-317f-4104-8915-6fbe51fb61f4","metadata":{},"source":["## Example: MNIST Hand-Written Digits\n"]},{"cell_type":"markdown","id":"33031939-924a-4e1c-8550-4fd1f89930e8","metadata":{},"source":["Let us first load our MNIST hand-written dataset using the Keras library.\n"]},{"cell_type":"code","execution_count":null,"id":"732f8fa4-9c81-46c3-8f2c-d1224aefe26c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data() "]},{"cell_type":"code","execution_count":null,"id":"c3d8a8e0-7b63-4aca-adf6-771cce5afbb7","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"X_train shape\", X_train.shape)\n","print(\"y_train shape\", y_train.shape)\n","print(\"X_test shape\", X_test.shape)\n","print(\"y_test shape\", y_test.shape)"]},{"cell_type":"markdown","id":"84970b43-0d10-4e05-ba2a-15125a4aac21","metadata":{},"source":["Now we can plot some sample digits from the training set.\n"]},{"cell_type":"code","execution_count":null,"id":"10f0326f-32fd-4a24-aff1-7522b7ebe6d6","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (6,6) \n","\n","for i in range(4):\n","    plt.subplot(2,2,i+1)\n","    num = random.randint(0, len(X_train))\n","    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n","    plt.title(\"class {}\".format(y_train[num]))\n","    \n","plt.tight_layout()"]},{"cell_type":"markdown","id":"08d6d14b-f0d3-456f-949e-987f0e1df6fe","metadata":{},"source":["Each image is on a grayscale (that is, only 1 channel) and is of size 28x28. We need to flatten each image by reshaping our matrix from 28x28 to a 784-length vector.\n"]},{"cell_type":"code","execution_count":null,"id":"5c32ab51-560e-40c8-9d7b-ed4d95320dd7","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]* X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n","\n","print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","id":"88cccae0-6de0-44eb-ad19-fdee098abec8","metadata":{},"source":["Next, we need to change integers to 32-bit floating point numbers. We would also need to normalize our input to be in the range [0, 1], rather than [0-255].\n"]},{"cell_type":"code","execution_count":null,"id":"0d1f5b29-036c-4cfd-bf7b-bd6c443d75bc","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","X_train /= 255\n","X_test /= 255"]},{"cell_type":"markdown","id":"2775b358-7420-4637-bb58-739e3da24c70","metadata":{},"source":["Our target variable, `y`, is an array of the following form:\n"]},{"cell_type":"code","execution_count":null,"id":"0f95ddf9-45e3-40eb-8e8a-5f601d54f17d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_train"]},{"cell_type":"markdown","id":"edeb0337-6cfc-41fb-8a3b-2dd0b65344d0","metadata":{},"source":["The categorical cross-entropy function expects that we provide one-hot encoded representations of the labels and predictions. Let us one-hot encode our target, `y`, using the `OneHotEncoder()` function from sklearn.\n"]},{"cell_type":"code","execution_count":null,"id":"49d2a797-082f-4423-ac75-407b6e66b005","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["enc = OneHotEncoder(sparse=False)\n","enc.fit(y_train.reshape(-1, 1))\n","print(enc.categories_)"]},{"cell_type":"code","execution_count":null,"id":"1e0fced2-7cd9-4623-be45-10593f2beecc","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_train_enc = enc.transform(y_train.reshape(-1,1))\n","y_test_enc = enc.transform(y_test.reshape(-1,1))"]},{"cell_type":"markdown","id":"a911e55d-18c9-4195-96b3-a3b4d9f7321b","metadata":{},"source":["In this multi-class classification task, we want to interpret our logistic regression model's results as a probability distribution; that is the model should output a vector that represents the probability of the given image belonging to each of the 10 classes. \n","\n","All elements in the vector should fall into the \\[0,1] range and sum up to 1. In order to set up our classification problem this way, we can feed the model output to a softmax function to produce probabilities.\n"]},{"cell_type":"markdown","id":"70de8ecb-73ef-4d4d-a76b-57682543c04e","metadata":{},"source":["For training our multi-class logistic classifier, we need a loss function. Categorical cross-entropy is a loss function widely used with softmax. We need to minimize this loss function in order to determine an optimal set of parameters.\n"]},{"cell_type":"markdown","id":"b230af42-3b72-48e5-b3f6-457b77b410e7","metadata":{},"source":["We will build a simple CNN model with two hidden layers, that uses softmax as the activation function, and categorical cross-entropy as the loss function. Let's define the architecture of our model:\n"]},{"cell_type":"code","execution_count":null,"id":"ef9d5e0c-2fe8-41a8-85d3-139717f1607d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_vector_shape = X_train.shape[1]\n","input_shape = (feature_vector_shape,)\n","num_classes = 10\n","loss_function_used=CategoricalCrossentropy(from_logits=True)\n","model = Sequential()\n","model.add(Dense(12, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"id":"a0a6c4ab-35ec-4d75-b02f-b672787e027a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n","history = model.fit(X_train, y_train_enc, epochs=25, batch_size=5, verbose=1, validation_split=0.2)\n"]},{"cell_type":"code","execution_count":null,"id":"22ac33f2-e7fe-4739-9cd0-92019fbc8bc3","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["test_results = model.evaluate(X_test, y_test_enc, verbose=1)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')"]},{"cell_type":"markdown","id":"e52bca00-792c-4fca-80f2-40ccd4f3c997","metadata":{},"source":["## Example: Single-Digit House Number Recognition\n"]},{"cell_type":"markdown","id":"aea7c934-d602-4c6a-8361-1ad787e54eb0","metadata":{},"source":["Recognizing multi-digit text from photographs is a hard problem. In this example, we deal with a simpler sub-problem that involves using a convolutional neural network with the categorical cross-entropy loss function to build a classifier that identifies single-digit numbers from custom images.\n"]},{"cell_type":"markdown","id":"12fe28f5-fb3a-47dd-870d-da605739c3af","metadata":{},"source":["First, let's load in a custom image using a URL. \\[Image from [comforthouse](https://www.comforthouse.com/floating-1-number-sign.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).]\n"]},{"cell_type":"code","execution_count":null,"id":"8eb91791-48cf-439f-a1bd-8fdda55b03c2","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/house_number_5.jpeg'\n","image = Image.open(urlopen(URL)).convert('RGB')\n","image"]},{"cell_type":"markdown","id":"fe008ca4-e01f-4b85-ae50-12a9b782864a","metadata":{},"source":["Let us define a CNN architecture that is similar to one used for MNIST hand-written digit classification. We use a softmax as the activation function and categorical cross-entropy as the loss function as this is a multi-class classification problem.\n"]},{"cell_type":"code","execution_count":null,"id":"65263a59-4267-4c1d-9b1b-b219f963b101","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["feature_vector_shape,input_shape = 784,784\n","num_classes = 10\n","loss_function_used = CategoricalCrossentropy(from_logits=True)\n","model = Sequential()\n","model.add(Dense(12, input_shape=(input_shape,), activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n","history = model.fit(X_train, y_train_enc, epochs=25, batch_size=5, verbose=1, validation_split=0.2)"]},{"cell_type":"markdown","id":"507a8401-3eff-4721-bb58-f5e948a3ac3b","metadata":{},"source":["We must now pre-process our raw input images to be grayscale and 28x28 in size.\n"]},{"cell_type":"code","execution_count":null,"id":"94dfe72e-faaf-4a7c-b085-882691cdb2f2","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["img_rows, img_cols = 28, 28\n","img = Image.fromarray(np.uint8(image)).convert('L')\n","img_gray = img.resize((img_rows, img_cols), Image.ANTIALIAS)\n","img_gray"]},{"cell_type":"markdown","id":"296a18f5-6487-4ab5-9d2f-1be83dac9064","metadata":{},"source":["We will now convert the image to an array, and reshape it such that it has a dimension of (1, 784).\n"]},{"cell_type":"code","execution_count":null,"id":"f8c68c59-c492-4a47-bfd2-d2f1e77cdb5b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["arr = np.array(img_gray)\n","arr = arr.reshape((img_cols*img_rows))\n","arr = np.expand_dims(arr, axis=0)"]},{"cell_type":"markdown","id":"000a5b14-117c-4628-9cf4-e3c12688b830","metadata":{},"source":["Finally, we will feed the array as an input to our model, and use the argmax function to determine the most likely digit label.\n"]},{"cell_type":"code","execution_count":null,"id":"75f7ec9e-05a0-4d88-a314-aeafb2a962c3","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["prediction = model.predict(arr)\n","np.argmax(prediction)"]},{"cell_type":"markdown","id":"a857f4a1-9acf-49f6-b038-56d1dcccb45b","metadata":{},"source":["We got it right! You are free to experiment with different custom images by simply changing the URL.\n"]},{"cell_type":"markdown","id":"335adc83-2522-468a-8428-aeaa3e6f22f9","metadata":{},"source":["# Exercises\n"]},{"cell_type":"markdown","id":"e448fe1b-d72d-4b22-a337-f70a41cbad7f","metadata":{},"source":["For these exercises, we will be using the Fashion-MNIST dataset. It consists of 60,000 28x28 grayscale training images and 10,000 test images for 10 different categories, listed below:\n"]},{"cell_type":"markdown","id":"41ab987b-6cb9-45db-8dac-47ef78c145f3","metadata":{},"source":["0.  T-shirt\n","1.  Trouser\n","2.  Pullover\n","3.  Dress\n","4.  Coat\n","5.  Sandal\n","6.  Shirt\n","7.  Sneaker\n","8.  Bag\n","9.  Ankle boot\n"]},{"cell_type":"markdown","id":"6c99465d-b9c9-4ea6-9b1c-cc843b56d3f4","metadata":{},"source":["### Exercise 1 - Loading and plotting the images\n"]},{"cell_type":"markdown","id":"e7de3fd5-b109-4082-b7b4-12dee0f4b7ac","metadata":{},"source":["Similar to the MNIST hand-written digits dataset, Fashion-MNIST is available for loading through Keras. Load the dataset using Keras in a similar manner. Plot some sample images from the training set.\n"]},{"cell_type":"code","execution_count":null,"id":"475902c4-4dbd-4391-9da3-39f2da8331f2","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Enter your solution here"]},{"cell_type":"markdown","id":"a3bc143b-ffd6-4f3b-900c-1bb9fdaf7894","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","\n","\n","plt.rcParams['figure.figsize'] = (6,6) \n","\n","for i in range(4):\n","    plt.subplot(2,2,i+1)\n","    num = random.randint(0, len(X_train))\n","    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n","    plt.title(\"class {}\".format(y_train[num]))\n","    \n","plt.tight_layout()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e2c6dafe-b3eb-4553-8bb0-7a8cfefde65d","metadata":{},"source":["### Exercise 2 - Preparing the data\n"]},{"cell_type":"markdown","id":"056dfd4f-f6c5-49f7-9ed2-ec6d3a2ebf9a","metadata":{},"source":["Convert all grayscale images of size 28x28 to a 784-length vector. Change integers to 32-bit floating point numbers and normalize the input to be in the range \\[0, 1], rather than \\[0-255].\n"]},{"cell_type":"code","execution_count":null,"id":"7c5e1f22-bdbb-4269-bd9a-31f13dbf1cf4","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Enter your solution here\n"]},{"cell_type":"markdown","id":"2a1140e9-649a-4d6e-aa67-384fae8bc02f","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]* X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n","\n","print(X_train.shape)\n","print(X_test.shape)\n","\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","X_train /= 255\n","X_test /= 255\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"30feb008-8102-45af-b05e-a7f0bb8a4825","metadata":{},"source":["### Exercise 3 - One-hot encoding\n"]},{"cell_type":"markdown","id":"95e353e9-7f56-4769-9521-af45692f7b4d","metadata":{},"source":["Create one-hot encoded representations of the target variable using the `OneHotEncoder()` function from sklearn.\n"]},{"cell_type":"code","execution_count":null,"id":"736bac79-2a8a-4db4-ad9f-f71ccf828683","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Enter your solution here\n"]},{"cell_type":"markdown","id":"954fec39-9a71-4066-b87f-53f0ffa8e8b4","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","enc = OneHotEncoder(sparse=False)\n","enc.fit(y_train.reshape(-1, 1))\n","print(enc.categories_)\n","y_train_enc = enc.transform(y_train.reshape(-1,1))\n","y_test_enc = enc.transform(y_test.reshape(-1,1))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ba57e465-4d6f-4cb3-8c18-d9b595b37e9a","metadata":{},"source":["### Exercise 4 - Build model architecture\n"]},{"cell_type":"markdown","id":"5828fc9e-6902-491a-a863-028bff5b8b18","metadata":{},"source":["Build a CNN model with 2 hidden layers, that uses softmax as the activation function, and categorical cross-entropy as the loss function.\n"]},{"cell_type":"code","execution_count":null,"id":"243f68ac-e1b7-4be8-945d-c7dc228886c0","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Enter your solution here"]},{"cell_type":"markdown","id":"11a1bc03-3d5c-4786-b4dd-56282415c1a9","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","feature_vector_shape = X_train.shape[1]\n","input_shape = (feature_vector_shape,)\n","num_classes = 10\n","loss_function_used = CategoricalCrossentropy(from_logits=True)\n","\n","model = Sequential()\n","model.add(Dense(12, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n","history = model.fit(X_train, y_train_enc, epochs=25, batch_size=5, verbose=1, validation_split=0.2)\n","\n","test_results = model.evaluate(X_test, y_test_enc, verbose=1)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"288e8d8c-a1eb-43e4-922c-61b0a2076905","metadata":{},"source":["### (Optional) Exercise 5 - Sparse Categorical Cross Entropy\n"]},{"cell_type":"markdown","id":"ded5fd42-dfe3-451f-8d99-bda229d84e8a","metadata":{},"source":["We saw that when using categorical cross-entropy, the ground truth labels need to be one-hot encoded. Keras provides another loss function called sparse cross-entropy where we can leave the labels as integers.\n","\n","Build a CNN model with two hidden layers, that uses softmax as the activation function, and sparse categorical cross-entropy as the loss function. Make sure to use the non-one-hot encoded ground-truth labels.  \n"]},{"cell_type":"code","execution_count":null,"id":"94962e8a-fa0c-46e9-af0a-a0b75128a8f1","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Enter your solution here\n"]},{"cell_type":"markdown","id":"fd2eeb3b-ec5b-4775-ae1c-9a0070dbc78a","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","feature_vector_shape = X_train.shape[1]\n","input_shape = (feature_vector_shape,)\n","num_classes = 10\n","loss_function_used = SparseCategoricalCrossentropy()\n","\n","model = Sequential()\n","model.add(Dense(12, input_shape=input_shape, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","model.compile(loss=loss_function_used, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n","history = model.fit(X_train, y_train, epochs=25, batch_size=5, verbose=1, validation_split=0.2)\n","\n","test_results = model.evaluate(X_test, y_test_enc, verbose=1)\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n","\n","```\n","\n","</details>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
