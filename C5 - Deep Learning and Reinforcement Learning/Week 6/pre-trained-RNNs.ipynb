{"cells":[{"cell_type":"markdown","id":"ed02574b-53ff-4cbd-9264-8c6932a928bd","metadata":{},"source":["# **Recurrent Neural Networks**\n"]},{"cell_type":"markdown","id":"0545db63-0ae3-44c5-8bcc-cc843ce099fe","metadata":{},"source":["Estimated time needed: **45** minutes\n","\n","A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data as input. Its typically used for ordinal or temporal problems like language translation, speech recognition, and time series forecasting. \n","\n","In this lab, we will understand the fundamental building blocks of an RNN. We will train a simple binary text classifier on top of an existing pre-trained module that embeds sentences.\n"]},{"cell_type":"markdown","id":"32467c3c-7080-45ca-93b2-dd6fa4cca1b2","metadata":{},"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#RNN-Fundamentals\">RNN Fundamentals</a>\n","        <ol>\n","            <li><a href=\"#Vanilla-Recurrent-Neural-Network\"> Vanilla Recurrent Neural Network</a></li>\n","            <li><a href=\"#Unrolling-in-time-of-a-RNN\">Unrolling in time of a RNN</a></li>\n","            <li><a href=\"#Training-an-RNN\">Training an RNN</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"#Types-of-RNNs\">Types of RNNs</a></li>\n","    <li><a href=\"#Pre-trained-RNNs\">Pre-trained RNNs</a></li>\n","</ol>\n"]},{"cell_type":"markdown","id":"6ffa8ef1-5e18-40ee-99ce-ab622b8620fa","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n"," - Describe the fundamental building blocks of RNNs.\n"," - Implement pre-trained RNNs to solve time-series prediction, and forecasting, and text classification tasks\n"]},{"cell_type":"markdown","id":"dbfc0cd0-37a4-493b-9b82-9dcd3903de65","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"28631fff-2635-4838-aa2f-9e702c82bc51","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"94e873e7-45e8-4bce-acf5-7a978178f5cc","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"764935b6-665e-4175-a512-7ba3e0e0c4f3","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":null,"id":"f22609f3-525a-4a56-a450-54c852cce0aa","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas numpy seaborn matplotlib scikit-learn\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"markdown","id":"9d3fd032-8fe7-4a7a-a9f8-69444a0b41bd","metadata":{},"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","execution_count":null,"id":"7ed95664-21fc-4793-a49d-b3bb4d4bc237","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["%%capture\n","\n","!pip install tensorflow_hub\n","!pip install tensorflow --upgrade\n","!mamba install -qy tqdm"]},{"cell_type":"markdown","id":"b711e10d-ec44-48cb-b35f-d0a63bc4ade8","metadata":{},"source":["### Importing Required Libraries\n"]},{"cell_type":"code","execution_count":null,"id":"5eaee927-895f-4fb1-900d-30fdc8aa7391","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import tensorflow as tf\n","print(tf. __version__)\n","import skillsnetwork\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.losses import mean_squared_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense, Embedding,Masking,LSTM, GRU, Conv1D, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Embedding, SimpleRNN\n","from tensorflow.keras.datasets import reuters\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n","import tensorflow_hub as hub\n","\n","\n","# You can also use this section to suppress warnings generated by your code:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')\n","\n","sns.set_context('notebook')\n","sns.set_style('white')\n","np.random.seed(2024)"]},{"cell_type":"markdown","id":"64c057cc-32a4-4c85-8462-3e391dcd86ce","metadata":{},"source":["## Helper Functions\n"]},{"cell_type":"code","execution_count":null,"id":"60a05cb5-eafb-4096-8778-02123df6504b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# function to compute the accuracy, precision, recall and F1 score of a model's predictions.\n","def calculate_results(y_true, y_pred):\n","    model_accuracy = accuracy_score(y_true, y_pred)\n","    model_precision, model_recall, model_f1,_ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\")\n","    model_results = {\"accuracy\":model_accuracy,\n","                     \"precision\":model_precision,\n","                     \"recall\" :model_recall,\n","                     \"f1\":model_f1}\n","    return model_results"]},{"cell_type":"markdown","id":"01e4f95a-a59e-43d9-8363-56c3b9ea5e2f","metadata":{"tags":[]},"source":["## RNN Fundamentals\n","\n","RNNs fall in the category of neural networks that maintain some kind of **state**. They can process sequential data of arbitrary length. By doing so, they overcome certain limitations faced by classical neural networks. Classical NNs only accept fixed-length vectors as input and output fixed-length vectors. RNNs operate over sequences of vectors. Classical NNs aren't built to consider the sequential nature of some data. RNNs work with sequential data forms like language, video frames, time series, and so on.\n","\n","The RNN layer uses a for-loop to iterate over the time-steps of a sequence, and maintains an internal state that encodes information about all time-steps that have been observed so far. The Keras RNN API has built-in `keras.layers.RNN` and `keras.layers.LSTM` layers that make it easy to quickly build RNN models.\n"]},{"cell_type":"markdown","id":"f2d46976-6f1c-48ee-bbdd-ba3ee0ebc3c3","metadata":{},"source":["### Vanilla Recurrent Neural Network\n","\n","RNNs use these two simple formulas:\n","\n","$$ \\mathbf s_t = \\mbox{tanh }(U \\mathbf x_t + W \\mathbf s_{t-1}) $$\n","\n","$$ \\mathbf y_t = V \\mathbf s_t $$ \n","\n","The following plot shows the hyperbolic tan function, `tanh`:\n","\n","<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/TanhReal.gif?raw=1\" alt=\"\" style=\"width: 300px;\">\n","\n","#### Terminology:\n","* $s_t$ current network, or the hidden state\n","* $\\mathbf s_{t-1}$ previous hidden state\n","* $\\mathbf x_t$ current input\n","* $U, V, W$ matrices that are parameters of the RNN\n","* $\\mathbf y_t$ output at time $t$\n","\n","These equations say that the current network state or the hidden state, is a function of the previous hidden state and the current input. \n","\n","### Unrolling in time of a RNN\n","\n","Given an input sequence, we apply RNN formulas in a recurrent way until we process all input elements. The $U,V,W$ parameters are shared across all recurrent steps. This implies that at each time step, the output is a function of all inputs from previous time steps. The network has a form of memory, encoding information about the time-steps it has seen so far.\n","\n","Some important observations:\n","- The initial values for $U,V,W$ as well as for $\\mathbf s$ must be provided when training an RNN.\n","- Hidden state  acts as a memory of the network. It can capture information about the previous steps. It embeds the representation of the sequence.\n","- We can look at the network's output at every stage or just the final stage.\n","\n","### Training an RNN\n","\n","A RNN has a layer for each time step, and its weights are shared across time. It is trained using backpropagation through time, and is done using the following steps:\n","- The input or the training set is made of several input ($n$-dimensional) sequences $\\{\\mathbf{X}_i \\}$ and corresponding outcomes. Each element of a sequence $\\mathbf{x}_j \\in \\mathbf{X}_i$ is also a vector.\n","- We use a loss function to measure how well the network's output fits to the expected outcome, such as ground truth.\n","- We apply an optimization method like stochastic gradient descent or Adam to optimize the loss function\n","- After the forward pass, gradients of the cost function are propagated backwards through the unrolled network\n"]},{"cell_type":"markdown","id":"ceaa5f19-ceea-43bd-876c-32e9a8909be1","metadata":{},"source":["## Types of RNNs\n","\n","Predicting the output, $y_t$, at each time step is not always the case. Different RNN architectures can be used to solve different kinds of problems.\n"]},{"cell_type":"markdown","id":"9869ab6e-db77-403b-8755-011eeb0e116e","metadata":{},"source":["|Type|Input|Output|Example problem\n","|-|-|-|-\n","|*many-to-many*|An input sequence|An output sequence|Part of Speech (POS) tagging\n","|*many-to-one*|An input sequence|Value of output sequence for last timestep|Text classification: positive tweet or negative?\n","|*one-to-many*|Single value of input sequence|An output sequence| Given an input image, predict sequence data\n"]},{"cell_type":"markdown","id":"9e1fc57f-19f8-4076-aa2b-a13bb3f56675","metadata":{},"source":["## Pre-trained RNNs\n"]},{"cell_type":"markdown","id":"7499aaba-6a3c-4933-96fc-881ba3c3d401","metadata":{},"source":["In this section, we will be experimenting with existing RNNs. We will use the NLP disaster dataset. The dataset contains a `test.csv` and a `train.csv` each of which have the following information:\n","\n","* The text of a tweet\n","* A keyword from that tweet (although this may be blank!)\n","* The location the tweet was sent from (may also be blank)\n","\n","Our task is to predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n"]},{"cell_type":"markdown","id":"f2457c40-04ea-4d4f-acc8-527f947e428d","metadata":{},"source":["Let us start by downloading and unzipping the dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"e7b8c05f-0165-4ceb-bdd1-9349525dd737","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/nlp_disaster.zip\")\n"]},{"cell_type":"markdown","id":"53138509-46b3-417a-b861-c312e73bf0b6","metadata":{},"source":["Now we will read in the train dataset. Here we use `frac=1` so all rows in the training dataset are returned in a random order. We also set a random state to ensure reproducibility of results.\n"]},{"cell_type":"code","execution_count":null,"id":"f2642e5d-631b-494a-9cc3-ae8247104ea8","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["train_df = pd.read_csv(\"train.csv\")\n","# shuffle the dataset \n","train_df_shuffled = train_df.sample(frac=1, random_state=42)\n","train_df_shuffled.head()"]},{"cell_type":"markdown","id":"aa8cee2c-5ef4-482f-8c8e-dbaad8d026d9","metadata":{},"source":["We will use 90% of the entire labelled dataset for training, and 10% of it for testing purposes.\n"]},{"cell_type":"code","execution_count":null,"id":"d1e0a9d5-554c-476f-ab54-cfeff7b352ed","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# split the data into 90% training and 10% testing\n","X_train, X_test, y_train, y_test = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                    train_df_shuffled[\"target\"].to_numpy(),\n","                                                    test_size = 0.1,\n","                                                    random_state=42)\n","X_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":null,"id":"e72599f3-17fc-4e43-b2d2-be73c93595a4","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_train[0:5]"]},{"cell_type":"markdown","id":"7d9bed73-1fb5-4f47-b964-1b5a133eceea","metadata":{},"source":["`TextVectorization` is a preprocessing layer which maps text features to integer sequences. We also specify `lower_and_strip_punctuation` as the standardization method to apply to the input text. The text will be lowercased and all punctuation removed. Next we split on the whitespace, and pass `None` to `ngrams` so no ngrams are created.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"463f67bb-31fc-44d5-9f69-68f705e7b45c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["text_vectorizer = TextVectorization(max_tokens=None, \n","                                    #remove punctuation and make letters lowercase\n","                                    standardize=\"lower_and_strip_punctuation\", \n","                                    #whitespace delimiter\n","                                    split=\"whitespace\", \n","                                    #dont group anything, every token alone\n","                                    ngrams = None, \n","                                    output_mode =\"int\",\n","                                    #length of each sentence == length of largest sentence\n","                                    output_sequence_length=None\n","                                    )\n"]},{"cell_type":"code","execution_count":null,"id":"44018c57-ff84-490c-9d5a-5cf3a229bb88","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# define hyperparameters\n","\n","# number of words in the vocabulary \n","max_vocab_length = 10000\n","# tweet average length\n","max_length = 15 "]},{"cell_type":"markdown","id":"169e1042-2608-4901-bc22-0d0f001216cd","metadata":{},"source":["Below we define an `Embedding` layer with a vocabulary of 10,000, a vector space of 128 dimensions in which words will be embedded, and input documents that have 15 words each.\n"]},{"cell_type":"code","execution_count":null,"id":"c280fb00-c208-4091-9fec-e1d70ae2eaae","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["embedding = layers.Embedding(input_dim= max_vocab_length,\n","                             output_dim=128,\n","                             input_length=max_length)"]},{"cell_type":"markdown","id":"3171967c-5beb-4af4-97d6-dbb09db34952","metadata":{},"source":["The `hub.KerasLayer` wraps a SavedModel (or a legacy TF1 Hub format) as a Keras Layer. The `universal-sentence-encoder` is an encoder of greater-than-word length text trained on a variety of data. It can be used for text classification, semantic similarity, clustering, and other natural language tasks. \n","\n","> We can train a simple binary text classifier on top of any TF-Hub module that can embed sentences. The Universal Sentence Encoder was partially trained with custom text classification tasks in mind. These kinds of classifiers can be trained to perform a wide variety of classification tasks often with a very small amount of labeled examples.\n","\n","More on this is found in the Tensorflow Hub [documentation](https://tfhub.dev/google/universal-sentence-encoder/4?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01)\n"]},{"cell_type":"code","execution_count":null,"id":"35ada2b2-1351-4322-bb29-000d4cf892fa","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n","                               input_shape=[],\n","                               dtype = tf.string,\n","                               trainable=False,\n","                               name=\"pretrained\")"]},{"cell_type":"markdown","id":"45968087-0dc6-4b00-a346-f35a54cd063c","metadata":{},"source":["The `encoder_layer` will take as input variable length English text and the output is a 512 dimensional vector.\n"]},{"cell_type":"markdown","id":"db7b4333-fa87-4db3-aa5d-b1f723a6aa2f","metadata":{},"source":["We will add a Dense layer with unit 1 to create a simple binary text classifier on top of any TF-Hub module. Next, we will compile and fit it using 20 epochs.\n"]},{"cell_type":"code","execution_count":null,"id":"6a63a7ea-feb9-4b7c-acef-abe94d8d8f6c","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = tf.keras.Sequential([\n","                             encoder_layer,\n","                             layers.Dense(1,activation=\"sigmoid\")], name=\"model_pretrained\")\n","model.compile(loss=\"binary_crossentropy\",\n","                     optimizer=\"adam\",\n","                     metrics=[\"accuracy\"])\n","\n","model.fit(x=X_train,\n","              y=y_train,\n","              epochs=20,\n","              validation_data=(X_test,y_test))"]},{"cell_type":"code","execution_count":null,"id":"d83f98ff-05fc-46ae-818f-80c04088f332","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["calculate_results(y_true=y_test,\n","                  y_pred=tf.squeeze(tf.round(model.predict(X_test))))"]},{"cell_type":"markdown","id":"e5e8045c-ad6e-423b-b55f-542b8fa6a2d5","metadata":{},"source":["The model is able to predict the tweet class with a fairly high accuracy.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":5}
