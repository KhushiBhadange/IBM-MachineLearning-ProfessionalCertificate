{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 5, Part b: Intro to Neural Networks LAB\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise: neurons as logic gates\n","In this exercise we will experiment with neuron computations.  We will show how to represent basic logic functions like AND, OR, and XOR using single neurons (or more complicated structures).  Finally, at the end we will walk through how to represent neural networks as a chain of matrix computations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import warnings\n","warnings.simplefilter('ignore')\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Sigmoid function:\n","\n","$$\n","\\sigma = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Quickly define the sigmoid function\n","def sigmoid(x):\n","    \"\"\"Sigmoid function\"\"\"\n","    return 1.0 / (1.0 + np.exp(-x))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Plot the sigmoid function\n","vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n","activation = sigmoid(vals)\n","fig = plt.figure(figsize=(12,6))\n","fig.suptitle('Sigmoid function')\n","plt.plot(vals, activation)\n","plt.grid(True, which='both')\n","plt.axhline(y=0, color='k')\n","plt.axvline(x=0, color='k')\n","plt.yticks()\n","plt.ylim([-0.5, 1.5]);"]},{"cell_type":"markdown","metadata":{},"source":["### Thinking of neurons as boolean logic gates\n","\n","A logic gate takes in two boolean (true/false or 1/0) inputs, and returns either a 0 or 1 depending on its rule. The truth table for a logic gate shows the outputs for each combination of inputs, (0, 0), (0, 1), (1,0), and (1, 1). For example, let's look at the truth table for an \"OR\" gate:\n","\n","### OR Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">OR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>\n","\n","A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values. Imagine a neuron that takes in two inputs, $x_1$ and $x_2$, and a bias term:\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/logic01.png)\n","\n","By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights (represented by ? marks above), such that it returns an output close to 0 or 1 depending on the inputs.\n","\n","What numbers for the weights would we need to fill in for this gate to output OR logic? Observe from the plot above that $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n","\n","$$\n","z = w_1 x_1 + w_2 x_2 + b\n","$$\n","\n","Let's think this through:\n","\n","* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for (0, 0) to be close to zero, $b$ should be negative (at least -10)\n","* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10.\n","* Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be? \n","    * At least +20\n","* So let's try out $w_1=20$, $w_2=20$, and $b=-10$!\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/logic02.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def logic_gate(w1, w2, b):\n","    # Helper to create logic gate functions\n","    # Plug in values for weight_a, weight_b, and bias\n","    return lambda x1, x2: sigmoid(w1 * x1 + w2 * x2 + b)\n","\n","def test(gate):\n","    # Helper function to test out our weight functions.\n","    for a, b in (0, 0), (0, 1), (1, 0), (1, 1):\n","        print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["or_gate = logic_gate(20, 20, -10)\n","test(or_gate)"]},{"cell_type":"markdown","metadata":{},"source":["<table>\n","\n","<tr>\n","<th colspan=\"3\">OR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>\n","\n","This matches! Great! Now you try finding the appropriate weight values for each truth table. Try not to guess and check- think through it logically and try to derive values that work.\n","\n","### AND Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">AND gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 1\n","Determine what values for the neurons would make this function as an AND gate.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# TO DO: Fill in the w1, w2, and b parameters such that \n","# the truth table matches\n","### BEGIN SOLUTION\n","w1 = 11\n","w2 = 10\n","b = -20\n","and_gate = logic_gate(w1, w2, b)\n","### END SOLUTION\n","test(and_gate)"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 2\n","Do the same for the NOR gate and the NAND gate.\n"]},{"cell_type":"markdown","metadata":{},"source":["### NOR (Not Or) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">NOR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# TO DO: Fill in the w1, w2, and b parameters such that the \n","# truth table matches\n","### BEGIN SOLUTION\n","w1 = -20\n","w2 = -20\n","b = 10\n","nor_gate = logic_gate(w1, w2, b)\n","### END SOLUTION\n","\n","test(nor_gate)"]},{"cell_type":"markdown","metadata":{},"source":["### NAND (Not And) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">NAND gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["### BEGIN SOLUTION\n","# TO DO: Fill in the w1, w2, and b parameters such that the \n","# truth table matches\n","w1 = -11\n","w2 = -10\n","b = 20\n","nand_gate = logic_gate(w1, w2, b)\n","### END SOLUTION\n","\n","test(nand_gate)"]},{"cell_type":"markdown","metadata":{},"source":["## The limits of single neurons\n","\n","If you've taken computer science courses, you may know that the XOR gates are the basis of computation. They can be used as so-called \"half-adders\", the foundation of being able to add numbers together. Here's the truth table for XOR:\n","\n","### XOR (Exclusive Or) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">XOR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>\n","\n","Can we create a set of weights such that a single neuron can output this property?\n","\n","It turns out we cannot, since single neurons can't correlate inputs. Can we still use neurons to somehow form an XOR gate?\n","\n","What if we tried something more complex:\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/logic03.png)\n","\n","Here, we've got the inputs going to two separate gates: the top neuron is an OR gate, and the bottom is a NAND gate. The output of these gates then get passed to another neuron, which is an AND gate. If you work out the outputs at each combination of input values, you'll see that this is an XOR gate.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Make sure we have or_gate, nand_gate, and and_gate working from above!\n","def xor_gate(a, b):\n","    c = or_gate(a, b)\n","    d = nand_gate(a, b)\n","    return and_gate(c, d)\n","test(xor_gate)"]},{"cell_type":"markdown","metadata":{},"source":["## Feedforward Networks as Matrix Computations\n","\n","We discussed previously how the feed-forward computation of a neural network can be thought of as matrix calculations and activation functions.  We will do some actual computations with matrices to see this in action.\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/FF_NN.png)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 3\n","Provided below are the following:\n","\n","- Three weight matrices `W_1`, `W_2` and `W_3` representing the weights in each layer.  The convention for these matrices is that each $W_{i,j}$ gives the weight from neuron $i$ in the previous (left) layer to neuron $j$ in the next (right) layer.  \n","- A vector `x_in` representing a single input and a matrix `x_mat_in` representing 7 different inputs.\n","- Two functions: `soft_max_vec` and `soft_max_mat` which apply the soft_max function to a single vector, and row-wise to a matrix.\n","\n","The goals for this exercise are:\n","1. For input `x_in` calculate the inputs and outputs to each layer (assuming sigmoid activations for the middle two layers and soft_max output for the final layer.\n","2. Write a function that does the entire neural network calculation for a single input\n","3. Write a function that does the entire neural network calculation for a matrix of inputs, where each row is a single input.\n","4. Test your functions on `x_in` and `x_mat_in`.\n","\n","This illustrates what happens in a NN during one single forward pass. Roughly speaking, after this forward pass, it remains to compare the output of the network to the known truth values, compute the gradient of the loss function and adjust the weight matrices `W_1`, `W_2` and `W_3` accordingly, and iterate. Hopefully this process will result in better weight matrices and our loss will be smaller afterwards.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["W_1 = np.array([[2,-1,1,4],[-1,2,-3,1],[3,-2,-1,5]])\n","W_2 = np.array([[3,1,-2,1],[-2,4,1,-4],[-1,-3,2,-5],[3,1,1,1]])\n","W_3 = np.array([[-1,3,-2],[1,-1,-3],[3,-2,2],[1,2,1]])\n","x_in = np.array([.5,.8,.2])\n","x_mat_in = np.array([[.5,.8,.2],[.1,.9,.6],[.2,.2,.3],\n","                     [.6,.1,.9],[.5,.5,.4],[.9,.1,.9],[.1,.8,.7]])\n","\n","def soft_max_vec(vec):\n","    return np.exp(vec)/(np.sum(np.exp(vec)))\n","\n","def soft_max_mat(mat):\n","    return np.exp(mat)/(np.sum(np.exp(mat),axis=1).reshape(-1,1))\n","\n","print('the matrix W_1\\n')\n","print(W_1)\n","print('-'*30)\n","print('vector input x_in\\n')\n","print(x_in)\n","print ('-'*30)\n","print('matrix input x_mat_in -- starts with the vector `x_in`\\n')\n","print(x_mat_in)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["### BEGIN SOLUTION\n","z_2 = np.dot(x_in,W_1)\n","z_2"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a_2 = sigmoid(z_2)\n","a_2"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["z_3 = np.dot(a_2,W_2)\n","z_3"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a_3 = sigmoid(z_3)\n","a_3"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["z_4 = np.dot(a_3,W_3)\n","z_4"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["y_out = soft_max_vec(z_4)\n","y_out\n","### END SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## A one-line function to do the entire neural net computation\n","\n","def nn_comp_vec(x):\n","    return soft_max_vec(sigmoid(sigmoid(np.dot(x,W_1)).dot(W_2)).dot(W_3))\n","\n","def nn_comp_mat(x):\n","    return soft_max_mat(sigmoid(sigmoid(np.dot(x,W_1)).dot(W_2)).dot(W_3))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["nn_comp_vec(x_in)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["nn_comp_mat(x_mat_in)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Example: Digit Recognition with Multi-layer Perceptron\n","\n","In this example, you will implement a simple neural network using scikit-learn's **MLPClassifier** function. The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images from [kaggle](https://www.kaggle.com/code/cezaryszulc/sklearn-simple-neural-network/notebook?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01).\n","\n","A multilayer perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN). An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01) for training. Its multiple layers and non-linear activation allow it to distinguish data that is not linearly separable.\n","\n","Let's download the digits dataset and display a few images!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import skillsnetwork\n","await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/digits.csv\",\n","                           overwrite=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import pandas as pd\n","digits = pd.read_csv(\"digits.csv\")\n","\n","labels = digits['label']\n","digits = np.array(digits.drop('label', axis=1)).astype('float')\n","digits.shape, labels.shape"]},{"cell_type":"markdown","metadata":{},"source":["There are 42000 digit images and each has 784 pixels, which means we can reshape them into $28\\times28$ images for displaying.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import random\n","plt.figure(figsize=(12,4))\n","for i in range(5):\n","    plt.subplot(1, 5, i+1)\n","    plt.imshow(random.choice(digits).reshape(28,28))\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's split the 42000 images into train and test set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["split = 0.7, 0.3 # train, test\n","# normalize data\n","digits /= 255.0\n","\n","split_ind = int(len(digits)*split[0])\n","X_train, X_test, y_train, y_test = digits[:split_ind], digits[split_ind:], labels[:split_ind], labels[split_ind:]\n","X_train.shape, X_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["With scikit-learn's **MLPClassifier**, we can utilize the GridSearch cross validation method to optimize the following parameters:\n","\n","- **hidden_layer_sizes: _tuple, length = n_layers - 2, default=(100,)_**. The ith element represents the number of neurons in the ith hidden layer.\n","\n","- **alpha: _float, default=0.0001_**. Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n","\n","- **max_iter: _int, default=200_**. Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n","\n","- **learning_rate_init: _float, default=0.001_**. The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n","\n","We will use the default activation \"relu\" and default solver \"adam\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","parameters = {'hidden_layer_sizes':[50, 75, 100],\n","              'alpha': [0.0001, 0.001, 0.01, 0.1], \n","              'max_iter': [200, 500, 800], \n","              'learning_rate_init':[0.0001, 0.001, 0.01, 0.1]}\n","\n","model = MLPClassifier()\n","clf = GridSearchCV(estimator=model, param_grid=parameters, cv=5, n_jobs=-1)\n","clf.fit(X_train[:3000], y_train[:3000]) # may need to reduce the train set size to shorten the training time\n","\n","print(\"The best parameter values found are:\\n\")\n","print(clf.best_params_)\n","\n","# store the best model found in \"bestmodel\"\n","bestmodel = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["Now we can use the **bestmodel**, which uses the most optimal set of parameter values found by GridSearchCV, to make predictions for the test set **X_test** and evaluate its performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = bestmodel.predict(X_test)\n","print(f\"The accuracy score of the best model is {accuracy_score(y_test, y_pred)}\\n\")\n","\n","plt.figure(figsize=(12,8))\n","for i in range(10):\n","    plt.subplot(2, 5, i+1)\n","    sample = random.choice(X_test)\n","    plt.imshow(sample.reshape(28,28))\n","    pred = bestmodel.predict(sample.reshape(1,-1))\n","    plt.title(f\"Predicted as {pred}\")\n","    plt.axis(\"off\")\n","\n","plt.tight_layout()"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
