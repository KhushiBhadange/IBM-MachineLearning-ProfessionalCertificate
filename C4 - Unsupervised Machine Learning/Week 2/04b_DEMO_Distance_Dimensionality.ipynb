{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 4, Part b: Distance and the Curse of Dimensionality DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["# Curse of Dimensionality \n","\n","### DEMO objectives: \n","* Gain understanding of why observations are far apart in high-dimensional space\n","* See an example of how adding dimensions degrades model performance for classification\n","* Learn how to fight the curse of dimensionality in your modeling projects\n","\n","-----\n","\n","### In high-dimensional space, points tend to be far apart.\n","This impacts data analysis. Intuitively, clustering is difficult when points are far away from each other: If my next nearest neighbor is very far away, does it still make sense to call it my neighbor? This notebook will show why high-dimensional space leads to sparse data. \n"]},{"cell_type":"markdown","metadata":{},"source":["### A circle inside a square \n","\n","Let's start by drawing a unit circle inside of a square. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def make_circle(point=0):\n","    fig = plt.gcf()\n","    ax = fig.add_subplot(111, aspect='equal')\n","    fig.gca().add_artist(plt.Circle((0,0),1,alpha=.5))\n","    ax.scatter(0,0,s=10,color=\"black\")\n","    ax.plot(np.linspace(0,1,100),np.zeros(100),color=\"black\")\n","    ax.text(.4,.1,\"r\",size=48)\n","    ax.set_xlim(left=-1,right=1)\n","    ax.set_ylim(bottom=-1,top=1)\n","    plt.xlabel(\"Covariate A\")\n","    plt.ylabel(\"Covariate B\")\n","    plt.title(\"Unit Circle\")\n","    \n","    if point:\n","        ax.text(.55,.9,\"Far away\",color=\"purple\")\n","        ax.scatter(.85,.85,s=10,color=\"purple\")\n","    else: \n","        plt.show()\n","    \n","make_circle()"]},{"cell_type":"markdown","metadata":{},"source":["Each axis is a different covariate. Imagine we've standard scaled our data, so they're centered on zero. This means that the average for each covariate is now zero, or the center of our circle. Points that our outside the unit circle would be harder to classify because the values of their covariates are far from the mean. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["make_circle(1)"]},{"cell_type":"markdown","metadata":{},"source":["### How much of the square is outside the circle? And the cube outside the sphere, etc.\n","\n","We've concluded that our purple point above would be hard to classify because it's far away from the rest of our observations. But what percentage of our points in the square are outside of the circle, and thus \"far away\"? Let's do some math!\n","\n","Since the square has length $2r$ and area $(2r)^2$, the percentage of the square outside the circle is:  1 - $\\frac {\\pi r^2} {(2r)^2} = 1 - \\frac \\pi 4$ = ~ 21% \n","\n","But what about a cube? To illustrate, we will use [matplotlib's 3-D axis](https://matplotlib.org/3.2.1/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) which allows the '3d' projection, as well as the [itertools](https://docs.python.org/3/library/itertools.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) library, which offers a number of efficient tools for looping and combining lists. We will use the [product](https://docs.python.org/2/library/itertools.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#itertools.product) (cartesian product) and [combinations](https://docs.python.org/2/library/itertools.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#itertools.combinations) (unique subsequences) functions. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from itertools import product, combinations\n","\n","# Create figure \n","fig = plt.figure(figsize=(8,8))\n","ax = fig.gca(projection='3d')\n","#ax.set_aspect(\"equal\")\n","\n","# Draw cube\n","r = [-1, 1]\n","for s, e in combinations(np.array(list(product(r,r,r))), 2):\n","    if np.sum(np.abs(s-e)) == r[1]-r[0]:\n","        ax.plot3D(*zip(s,e))\n","\n","# Draw sphere on same axis \n","u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n","x=np.cos(u)*np.sin(v)\n","y=np.sin(u)*np.sin(v)\n","z=np.cos(v)\n","ax.plot_wireframe(x, y, z, color=\"black\");"]},{"cell_type":"markdown","metadata":{},"source":["The volume of the sphere is given by the forumula: $\\frac{4}{3} \\pi r^3$, and since the cube has a radius of $2r$, it has volume $(2r)^3$. \n","\n","This allows us to calculate the percent of the cube's volume that's outside of the sphere: 1- $\\frac {\\frac 4 3 \\pi r^3} {(2r)^3} = 1 - \\frac \\pi 6$ = ~ 48%\n","\n","### Can we generalize to more than three dimensions? \n","\n","Let's draw a bunch of random points from a 0-1 distribution, then measure how far away from the origin they are. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Draw a sample of data in two dimensions\n","sample_data = np.random.sample((5,2))\n","print(\"Sample data:\\n\", sample_data, '\\n')\n","\n","def norm(x): \n","    ''' Measure the distance of each point from the origin.\n","    \n","    Input: Sample points, one point per row\n","    Output: The distance from the origin to each point\n","    '''\n","    return np.sqrt( (x**2).sum(1) ) # np.sum() sums an array over a given axis \n","\n","def in_the_ball(x): \n","    ''' Determine if the sample is in the circle. \n","    \n","    Input: Sample points, one point per row\n","    Output: A boolean array indicating whether the point is in the ball\n","    '''\n","    return norm(x) < 1 # If the distance measure above is <1, we're inside the ball\n","\n","\n","for x, y in zip(norm(sample_data),in_the_ball(sample_data)):\n","    print(\"Norm = \", x.round(2), \"; is in circle? \", y)"]},{"cell_type":"markdown","metadata":{},"source":["Can we generalize beyond two dimensions? \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def what_percent_of_the_ncube_is_in_the_nball(d_dim,\n","                                              sample_size=10**4):\n","    shape = sample_size,d_dim\n","    data = np.array([in_the_ball(np.random.sample(shape)).mean()\n","                     for iteration in range(100)])\n","    return data.mean()\n","\n","dims = range(2,15)\n","data = np.array(list(map(what_percent_of_the_ncube_is_in_the_nball,dims)))\n","\n","\n","for dim, percent in zip(dims,data):\n","    print(\"Dimension = \", dim, \"; percent in ball = \", percent)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Let's plot the above percentages \n","\n","plt.plot(dims, data, color='blue')\n","plt.xlabel(\"# dimensions\")\n","plt.ylabel(\"% of area in sphere\")\n","plt.title(\"What percentage of the cube is in the sphere?\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Check for understanding\n","\n","What happens to the percentage of the area in the sphere as we increase the number of dimensions? What does this mean about how the dispersion of our data? \n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["### Measuring the distance from the center of the cube to the nearest point\n","\n","Let's continue with our drawing of random points in the cube. This time, we'll draw a bunch of points, then measure the distance of all the points to the center of the cube to see how close the closest point is to the center. \n","\n","If we find out that the closest point is far from the center in high dimensions, that gives us more evidence that increasing dimensions will make it harder to classify our data properly. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def get_min_distance(dimension, sample_size=10**3):\n","    ''' Sample some random points and find the closet \n","    of those random points to the center of the data '''\n","    points = np.random.sample((sample_size,dimension))-.5   # centering our data\n","    return np.min(norm(points))\n","\n","def estimate_closest(dimension):\n","    ''' For a given dimension, take a random sample in that dimension and then find \n","        that sample's closest point to the center of the data. \n","        Repeat 100 times for the given dimension and return the min/max/mean \n","        of the distance for the nearest point. '''\n","    data = np.array([get_min_distance(dimension) for _ in range(100)])\n","    return data.mean(), data.min(), data.max()\n","\n","# Calculate for dimensions 2-100\n","dims = range(2,100)\n","min_distance_data = np.array(list(map(estimate_closest,dims)))\n","\n","# Test it for dimension 6\n","print(\"For dimension 6: \", estimate_closest(6))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Plot the min/max/mean of the closest point for each dimension using sampling \n","\n","plt.plot(dims,min_distance_data[:,0], color='blue')\n","plt.fill_between(dims, min_distance_data[:,1], min_distance_data[:,2],alpha=.5)\n","plt.xlabel(\"# dimensions\")\n","plt.ylabel(\"Estimated min distance from origin\")\n","plt.title(\"How far away from the origin is the closest point?\"); "]},{"cell_type":"markdown","metadata":{},"source":["## Check for understanding\n","\n","What would we have to do to get similar density of points as the low dimensions if we wanted to use higher dimensions of data? \n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["### How to fight the curse of dimensionality \n","\n","The curse of dimensionality is a common hurdle in real-world predictive modeling. We've already seen this in our bag-of-words approach to NLP. How can we create good models in cases where our data is of high dimensionality? \n","\n","* Feature selection: Use domain knowledge to reduce the number of features included in the model\n","* Feature extraction: Use dimensionality reduction techniques (e.g. PCA) to transform the raw data into a lower number of features that preserve (most of) the variability in the data\n","\n","We'll talk more about PCA next.\n"]},{"cell_type":"markdown","metadata":{},"source":["## How does high dimensionality affect model performance?\n","\n","We'll test model performance (classification accuracy) by creating some random data for classification, one set with a low number of features and another with a very high number of features. We'll then compare their performance using the same classification algorithm. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_classification\n","from sklearn.tree import DecisionTreeClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create a dataset with two features \n","\n","X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n","                           random_state=1, n_clusters_per_class=2)\n","rng = np.random.RandomState(2)\n","X += 2 * rng.uniform(size=X.shape)\n","\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["DT =  DecisionTreeClassifier()\n","DT.fit(X_train, y_train)\n","score = DT.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"Score from two-feature classifier: \", score)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Now do the same thing but with 200 features \n","\n","X, y = make_classification(n_features=200, n_redundant=0, n_informative=200,\n","                           random_state=1, n_clusters_per_class=2)\n","rng = np.random.RandomState(2)\n","X += 2 * rng.uniform(size=X.shape)\n","\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n","\n","DT =  DecisionTreeClassifier()\n","DT.fit(X_train, y_train)\n","score = DT.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"Score from 200-feature classifier: \", score)"]},{"cell_type":"markdown","metadata":{},"source":["Here we see that adding additional features, even if informative, can lead to worse model performance (due to increased overfitting of training data.\n","\n","\n","### Comparing accuracy of classification against number of features\n","\n","Let's see what happens if we keep adding features to our classification problem. We'll hold the number of classes at three in order to see how the curse of dimensionality hurts our model performance. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["scores = []\n","\n","increment, max_features = 50, 4000\n","\n","for num in np.linspace(increment, max_features, increment, dtype='int'):\n","\n","    X, y = make_classification(n_features=num, n_redundant=0, \n","                               random_state=1, n_clusters_per_class=1, n_classes = 3)\n","    rng = np.random.RandomState(2)\n","    X += 2 * rng.uniform(size=X.shape)\n","\n","    X = StandardScaler().fit_transform(X)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n","\n","    \n","    DT =  DecisionTreeClassifier()\n","    DT.fit(X_train, y_train)\n","    scores.append( DT.score(X_test, y_test) )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plt.plot(np.linspace(increment, max_features, increment, dtype='int'),scores)\n","plt.title(\"Accuracy of Classification with Increasing Features\")\n","plt.xlabel(\"Number of features\")\n","plt.ylabel(\"Classification accuracy\");"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from the chart, adding features can lead to reductions in accuracy -- in this example accuracy is highly volatile in the number of features, and increasing features can reduce accuracy. Additionally, in our example, we specified that none of the features are redundnant (`n_redundant=0`). In practice, the situation is often worse as several of the features may be either redundant or not relevant. For example, if we are predicting customer churn using a variety of customer characteristics, we may have collected extensive data for each customer across many dimensions. This is an example of high-dimensional, which can make it difficult to apply unsupervised learning methods directly and potentially to lead to issues with the curse of dimensionality.\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
