{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Gaussian Mixture Models**\n"]},{"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","metadata":{},"source":["## Use cases of GMM:\n","\n","- **Recommender systems** that make recommendations to users based on preferences (such as Netflix viewing patterns) of similar users (such as neighbors).\n","- **Anomaly detection** that identifies rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior.\n","- **Customer segmentation** that aims at separating customers into multiple clusters, and devise targeted marketing strategy based on each cluster's characteristics.\n"]},{"cell_type":"markdown","metadata":{},"source":["## When is GMM better than K-Means?\n","\n","Imagine you are a Data Scientist who builds a recommender for selling cars using K-Means clustering and you have two clusters. Everybody in cluster A is recommended to buy car A, which costs **100k** with a **25k** profit margin, and everyone in cluster B is recommended to buy car B, which costs **50k** with a **10k** profit margin.\n","\n","Let's say you want to get as many people in cluster A as possible, why not use an algorithm that informs you of exactly how likely somebody would be interested in purchasing car A, instead of one that only tells you a hard yes or no (This is what K-Means does!). \n","\n","With GMM, not only will you be getting the predicted cluster labels, the algorithm will also give you the probability of a data point belonging to a cluster. How amazing is that! \n","\n","Whoever, is selling those cars should definitely work on a better plan for a customer with a 90% chance of purchasing, than for someone with a 75% chance of purchasing, even though they might show up in the same cluster.\n","\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/car.png\" style=\"width: 60%\">\n"]},{"cell_type":"markdown","metadata":{},"source":["In this lab, you will be applying clustering analysis on multivariate datasets using **Gaussian Mixture Models** (GMM).\n"]},{"cell_type":"markdown","metadata":{},"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#What-are-Gaussian-Mixture-Models (GMM)?\">What are Gaussian Mixture Models (GMM)?</a>\n","        <ol>\n","            <li><a href=\"#Background\">Background</a></li>\n","            <li><a href=\"#Playing around with means, standard deviations, and weights\">Playing around with means, standard deviations, and weights</a></li>\n","            <li><a href=\"#Introducing sklearn.mixture.GaussianMixture\">Introducing sklearn.mixture.GaussianMixture</a></li>\n","            <li><a href=\"#GMM.predict_proba\">GMM.predict_proba</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Example 1: Applying GMM on a 2d dataset\">Example 1: Applying GMM on a 2d dataset</a>\n","        <ol>\n","            <li><a href=\"#Generate an artificial 2d Gaussian mixture data\">Generate an artificial 2d Gaussian mixture data</a></li>\n","            <li><a href=\"#Fit a GMM\">Fit a GMM</a></li>\n","            <li><a href=\"#Plot the clusters\">Plot the clusters</a></li>\n","            <li><a href=\"#Try different values of Covariance_type\">Try different values of Covariance_type</a></li>          \n","        </ol>   \n","    </li>\n","    <li>\n","        <a href=\"#Example 2: Applying GMM on real world data - Image Segmentation\">Example 2: Applying GMM on real world data - Image Segmentation</a>\n","    </li>\n","</ol>\n","\n","<a href=\"#Exercises\">Exercises</a>\n","<ol>\n","    <li><a href=\"#Exercise 1 - Scale the data (using StandardScaler)\">Exercise 1 - Scale the data (using StandardScaler)</a></li>\n","    <li><a href=\"#Exercise 2 - Use PCA with n_components=2 for dimension reduction\">Exercise 2 - Use PCA with n_components=2 for dimension reduction</a></li>\n","    <li><a href=\"#Exercise 3 - Fit a GMM to the reduced data \">Exercise 3 - Fit a GMM to the reduced data</a></li>\n","    <li><a href=\"#Exercise 4 - Output the predicted labels for visualizing clusters\">Exercise 4 - Output the predicted labels for visualizing clusters</a></li>\n","    <li><a href=\"#Exercise 5 - Clustering and visualizing using 3 principal components (OPTIONAL)\">Exercise 5 - Clustering and visualizing using 3 principal components (OPTIONAL)</a></li>\n","</ol>\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","- __Understand__ what Gaussian mixture is and how its distribution parameters affect the prior probabilities.\n","- __Understand__ what Gaussian mixture model is and how it works as a clustering technique.\n","- __Apply__ GMM effectively.\n"]},{"cell_type":"markdown","metadata":{},"source":["----\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1 wget\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install pandas==1.3.4 ...\""]},{"cell_type":"markdown","metadata":{},"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"markdown","metadata":{},"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place (here):_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"vscode":{"languageId":"python"}},"outputs":[],"source":["# You can also use this section to suppress warnings generated by your code:\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as ss\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.mixture import GaussianMixture\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import multivariate_normal\n","from itertools import chain\n","from matplotlib.patches import Ellipse\n","\n","\n","sns.set_context('notebook')\n","sns.set_style('white')\n","\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Helper Functions\n","\n","_Use this section to define any helper functions to help the notebook's code readability:_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# This function will allow us to easily plot data taking in x values, y values, and a title\n","def plot_univariate_mixture(means, stds, weights, N = 10000, seed=10):\n","    \n","    \"\"\"\n","    returns the simulated 1d dataset X, a figure, and the figure's ax\n","    \n","    \"\"\"\n","    np.random.seed(seed)\n","    if not len(means)==len(stds)==len(weights):\n","        raise Exception(\"Length of mean, std, and weights don't match.\") \n","    K = len(means)\n","    \n","    mixture_idx = np.random.choice(K, size=N, replace=True, p=weights)\n","    # generate N possible values of the mixture\n","    X = np.fromiter((ss.norm.rvs(loc=means[i], scale=stds[i]) for i in mixture_idx), dtype=np.float64)\n","      \n","    # generate values on the x axis of the plot\n","    xs = np.linspace(X.min(), X.max(), 300)\n","    ps = np.zeros_like(xs)\n","    \n","    for mu, s, w in zip(means, stds, weights):\n","        ps += ss.norm.pdf(xs, loc=mu, scale=s) * w\n","    \n","    fig, ax = plt.subplots()\n","    ax.plot(xs, ps, label='pdf of the Gaussian mixture')\n","    ax.set_xlabel(\"X\", fontsize=15)\n","    ax.set_ylabel(\"P\", fontsize=15)\n","    ax.set_title(\"Univariate Gaussian mixture\", fontsize=15)\n","    #plt.show()\n","    \n","    return X.reshape(-1,1), fig, ax\n","    \n","    \n","def plot_bivariate_mixture(means, covs, weights, N = 10000, seed=10):\n","    \n","    \"\"\"\n","    returns the simulated 2d dataset X and a scatter plot is shown\n","    \n","    \"\"\"\n","    np.random.seed(seed)\n","    if not len(mean)==len(covs)==len(weights):\n","        raise Exception(\"Length of mean, std, and weights don't match.\") \n","    K = len(means)\n","    M = len(means[0])\n","    \n","    mixture_idx = np.random.choice(K, size=N, replace=True, p=weights)\n","    \n","    # generate N possible values of the mixture\n","    X = np.fromiter(chain.from_iterable(multivariate_normal.rvs(mean=means[i], cov=covs[i]) for i in mixture_idx), \n","                dtype=float)\n","    X.shape = N, M\n","    \n","    xs1 = X[:,0] \n","    xs2 = X[:,1]\n","    \n","    plt.scatter(xs1, xs2, label=\"data\")\n","    \n","    L = len(means)\n","    for l, pair in enumerate(means):\n","        plt.scatter(pair[0], pair[1], color='red')\n","        if l == L-1:\n","            break\n","    plt.scatter(pair[0], pair[1], color='red', label=\"mean\")\n","    \n","    plt.xlabel(\"$x_1$\")\n","    plt.ylabel(\"$x_2$\")\n","    plt.title(\"Scatter plot of the bivariate Gaussian mixture\")\n","    plt.legend()\n","    plt.show()\n","    \n","    return X\n","\n","\n","def draw_ellipse(position, covariance, ax=None, **kwargs):\n","    \n","    \"\"\"\n","    Draw an ellipse with a given position and covariance\n","    \n","    \"\"\"\n","    ax = ax or plt.gca()\n","    \n","    # Convert covariance to principal axes\n","    if covariance.shape == (2, 2):\n","        U, s, Vt = np.linalg.svd(covariance)\n","        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n","        width, height = 2 * np.sqrt(s)\n","    else:\n","        angle = 0\n","        width, height = 2 * np.sqrt(covariance)\n","    \n","    # Draw the Ellipse\n","    for nsig in range(1, 4):\n","        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))\n","        \n","        \n","def plot_gmm(gmm, X, label=True, ax=None):\n","    ax = ax or plt.gca()\n","    labels = gmm.fit(X).predict(X)\n","    if label:\n","        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","    else:\n","        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n","    ax.axis('equal')\n","    \n","    w_factor = 0.2 / gmm.weights_.max()\n","    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n","        draw_ellipse(pos, covar, alpha=w * w_factor)\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## What are Gaussian Mixture Models (GMM)?\n","\n","Put simply, Gaussian Mixture Models (GMM) is a clustering algorithm that:\n","\n","- Fits a weighted combination of Gaussian distributions to your data\n","- The data scientist (you) needs to determine the number of gaussian distributions (`k`)\n"]},{"cell_type":"markdown","metadata":{},"source":["<p style='color: blue'>Hard vs Soft Clustering:</p>\n","\n","- __Hard clustering__ algorithms assign each data point to exactly one cluster.\n","- __Soft clustering__ algorithms return probabilities of each data point belonging to all `k` clusters\n","\n","_GMM is a soft clustering algorithm._\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["### Background\n"]},{"cell_type":"markdown","metadata":{},"source":["A Gaussian mixture is a weighted combination of (`k`) Gaussians, where each is identified by the following parameters:\n","\n"," 1. a mean vector $\\boldsymbol{\\mu}_i$\n"," 2. a covariance matrix $\\boldsymbol{\\Sigma}_i$\n"," 3. a component weight $\\pi_i$ that indicates the contribution of the $i$th Gaussian\n","\n","When put altogether, the pdf of the mixture model is formulated as:\n","\n","$$\n","p(\\boldsymbol{x}) = \\sum_{i=1}^K\\pi_i \\mathcal{N}(x|\\boldsymbol{\\mu_i,\\Sigma_i}), \\\\\\\\ \\sum_{i=1}^K\\pi_i = 1\n","$$\n","\n","Before we start applying the model in a multivariate setting, let's delve into the three parameters and see how changing the parameter values affect the appearance of the Gaussian mixture in a lower dimension.\n","\n","_We will use the helper function **plot_univariate_mixture** to plot the mixture efficiently._\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["### Playing around with means, standard deviations, and weights\n","\n","Let's start with a mixture of 3 univariate Gaussians with \n","- means equal to **2, 5, 8** respectively\n","- std equal to **0.2, 0.5, 0.8** respectively\n","- component weight equal to **0.3, 0.3, 0.4** respectively\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X1, fig1, ax1 = plot_univariate_mixture(means=[2,5,8], stds=[0.2, 0.5, 0.8], weights=[0.3, 0.3, 0.4]) "]},{"cell_type":"markdown","metadata":{},"source":["The peaks in the distribution (pdf) plot above are around the x = 2, 5, 8, which are the means of our Gaussians. \n","\n","If we increase the value of the standard deviations, we will see the bell shapes become wider and there will be overlaps between two Gaussians.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X2, fig2, ax2 = plot_univariate_mixture(means=[2,5,8], stds=[0.6, 0.9, 1.2], weights=[0.3, 0.3, 0.4]) "]},{"cell_type":"markdown","metadata":{},"source":["Keeping the means and standard deviations unchanged, let's change the weights of the Gaussians and see how it affects the mixture's distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X3, fig3, ax3 = plot_univariate_mixture(means=[2,5,8], stds=[0.6, 0.9, 1.2], weights=[0.05, 0.35, 0.6]) "]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["When the weight of one Gaussian is reduced or increased, the likelihood of seeing a point from that Gaussian also decreases or increases, as illustrated in the distribution plot above.\n","\n","Hence, in conclusion, the **means** determine the centers of the mixed Gaussians; the **covariance** matrices determine the width and shape of the mixed Gaussians; the **weights** determine the contributions of the Gaussians to the Gaussian mixture.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introducing sklearn.mixture.GaussianMixture\n"]},{"cell_type":"markdown","metadata":{},"source":["With generated Gaussian mixture data, we know in advance the parameter values of the individual Gaussians. When we encounter a real-world dataset, how do we know the number of Gaussians to be included in the mixture model and their corresponding parameter values?\n","\n","We can utilize the **GaussianMixture** class from **Scikit-learn**. This class allows us to estimate the parameters of a Gaussian mixture distribution.\n","\n","A **GaussianMixture.fit** method is provided that learns a Gaussian Mixture Model from training data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the **GaussianMixture.predict** method.\n","\n","To solve for the parameters of GMM, the **EM algorithm** is implemented. **E** stands for **Expectation** and **M** stands for **Maximization**. The algorithm works by alternating between an **E** step, which calculates the expectation of the log-likelihood of observing the dataset using the current parameter estimates, and a **M** step, which seeks new parameters estimates that maximize the expectation found in the previous E step. This process is repeated  until convergence.\n","\n","For more detailed information on the **EM algorithm**, please see the page:<a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01\"> EM-link</a> \n"]},{"cell_type":"markdown","metadata":{},"source":["### GMM.predict_proba\n"]},{"cell_type":"markdown","metadata":{},"source":["One parameter you must specify when implementing a GMM is `n_components`, which tells the algorithm how many clusters to look for in the dataset. Another parameter you should consider is `covariance_type`, which we will talk about later with a multivariate dataset.\n","\n","Starting with **X1**, the univariate dataset we generated before using **plot_univariate_mixture**, we can fit a GMM to it with `n_components = 3` and plot the predicted prior probabilities of each point belonging to the 3 clusters. **GMM.predict_proba_** evaluates the components' density for each sample or for sample $x_n$ the probability  $p(i|x_{n})$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"vscode":{"languageId":"python"}},"outputs":[],"source":["# sort X1 in ascending order for plotting purpose\n","X1_sorted = np.sort(X1.reshape(-1)).reshape(-1,1)\n","\n","# fit the GMM\n","GMM = GaussianMixture(n_components=3, random_state=10)\n","GMM.fit(X1_sorted)\n","\n","# store the predicted probabilities in prob_X1\n","prob_X1 = GMM.predict_proba(X1_sorted)\n","\n","# start plotting! \n","ax1.plot(X1_sorted, prob_X1[:,0], label='Predicted Prob of x belonging to cluster 1')\n","ax1.plot(X1_sorted, prob_X1[:,1], label='Predicted Prob of x belonging to cluster 2')\n","ax1.plot(X1_sorted, prob_X1[:,2], label='Predicted Prob of x belonging to cluster 3')\n","ax1.scatter(2, 0.6, color='black')\n","ax1.scatter(2, 1.0, color='black')\n","ax1.plot([2, 2], [0.6, 1.0],'--', color='black')\n","ax1.legend()\n","fig1"]},{"cell_type":"markdown","metadata":{},"source":["To interpret the predicted probabilities, let's take a look at the point colored in black, as an example. On the Gaussian mixture pdf, the point is at the the peak of the first bell-shaped curve. Its corresponding probability of belonging to cluster 1 is equal to 1, which demonstrates that the probability of the center of a Gaussian distribution belonging to its own cluster is 100%. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Example 1: Applying GMM on a 2d dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate an artificial 2d Gaussian mixture data\n"]},{"cell_type":"markdown","metadata":{},"source":["We will use the helper function **plot_bivariate_mixture** to first generate a 2d Gaussian mixture dataset, and then visualize the data points.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# setting parameter values for the Gaussian mixture data\n","# we include three Gaussians in this example\n","\n","mean = [(1,5), (2,1), (6,2)]\n","cov1 = np.array([[0.5, 1.0],[1.0, 0.8]])\n","cov2 = np.array([[0.8, 0.4],[0.4, 1.2]])\n","cov3 = np.array([[1.2, 1.3],[1.3, 0.9]])\n","cov = [cov1, cov2, cov3]\n","weights = [0.3, 0.3, 0.4]\n","\n","X4 = plot_bivariate_mixture(means=mean, covs=cov, weights=weights, N=1000)                    "]},{"cell_type":"markdown","metadata":{},"source":["To work with GMM, we can use the **GaussianMixture** function from **sklearn.mixture**. The function will fit a GMM to our data and return the clustering result which includes the parameter values we discussed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"The dataset we generated has a shape of\", X4.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Fit a GMM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# since we generated a mixture dataset X4 with 3 Gaussians, it makes sense to set n_components = 3.\n","gm = GaussianMixture(n_components=3, random_state=0).fit(X4)\n","print(\"Means of the 3 Gaussians fitted by GMM are\\n\")\n","print(gm.means_)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"Covariances of the 3 Gaussians fitted by GMM are\")\n","gm.covariances_"]},{"cell_type":"markdown","metadata":{},"source":["The 3 centers returned by our fitted GMM are all very close to the means we set for generating the mixture data, which means the model fitting was quite successful. \n","\n","We can also draw the 3 ellipses, using the fitted parameter values, to check if they indeed represent the 3 clusters from our generated data.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plot the clusters\n"]},{"cell_type":"markdown","metadata":{},"source":["We will use the helper functions **draw_ellipse** and **plot_gmm** to plot the original scatter plot as well as the clusters produced by a fitted GMM.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plot_gmm(GaussianMixture(n_components=3, random_state=0), # the model, \n","          X4) # simulated Gaussian mixture data"]},{"cell_type":"markdown","metadata":{},"source":["### Try different values of Covariance_type\n"]},{"cell_type":"markdown","metadata":{},"source":["For the **sklearn.mixture.GaussianMixture** function, not only can we specify the number of clusters we want the GMM to fit, we can also specify the type of covariance parameters to use (recall we discussed that **covariance** matrices determine the shapes of our Gaussians. Depending on the dataset (different datasets have different Gaussian mixture structures), there are 4 values you can try for **Covariance_type**:\n","\n","- `full` each component has its own general covariance matrix.\n","- `tied` all components share the same general covariance matrix.\n","- `diag` each component has its own diagonal covariance matrix.\n","- `spherical` each component has its own single variance.\n","\n","_The default **Covariance_type** in **sklearn.mixture.GaussianMixture** is `full`._\n","\n","**Sometimes you can't use covariance_type = full, because you can't invert it and this will give you an error.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# try Covariance_type = 'tied'\n","plot_gmm(GaussianMixture(n_components=3, covariance_type='tied',random_state=0), # the model, \n","         X4)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# try Covariance_type = 'diag'\n","plot_gmm(GaussianMixture(n_components=3, covariance_type='diag',random_state=0), # the model, \n","         X4)"]},{"cell_type":"markdown","metadata":{},"source":["It is clear visually that **covariance_type =** `full` is a good choice for the generated data. This makes sense because we did intentionally make each Gaussian have its own covariance matrix.\n","\n","Now we know how to work with GMM in a lower dimension (2d), we can import a real-world dataset and use GMM to perform a clustering analysis on it.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Example 2: Applying GMM on real world data - Image Segmentation\n"]},{"cell_type":"markdown","metadata":{},"source":["Image segmentation is the process of segmenting an image into multiple important regions.\n","\n","We can use a GMM to segment an image into **K** regions `(n_components = K)` according to significant colors.\n","\n","Each pixel would be a data point with three features (r, g, b), or one feature, if greyscale. \n","\n","For instance, if we are working with a 256 $\\times$ 256 image, you would have 65536 pixels in total and your data $X$ would have a shape of 65536 $\\times$ 3.\n","\n","Let's look at an example using a picture of a house cat:\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat.jpeg\" style=\"width: 50%\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's download the image: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat.jpeg"]},{"cell_type":"markdown","metadata":{},"source":["First, let's segment our image using 2 gaussian distributions:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["img = plt.imread('gauss-cat.jpeg')\n","\n","# If img is greyscale, then change to .reshape(-1, 1):\n","X = img.reshape(-1, 3)\n","# The number of components; you can change this to a positive integer of your choice!:\n","n = 2\n","gmm = GaussianMixture(n_components=n, covariance_type='tied')\n","gmm.fit(X)\n","labels = gmm.predict(X) # num of pixels x 1"]},{"cell_type":"markdown","metadata":{},"source":["Now, we replace each pixel with the \"average color\" or the mean RGB values of the gaussian distribution it belongs to:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["seg = np.zeros(X.shape) # num of pixels x 3\n","\n","for label in range(n):\n","    seg[labels == label] = gmm.means_[label]\n","seg = seg.reshape(img.shape).astype(np.uint8)\n","\n","plt.figure(figsize=(6,6))\n","plt.imshow(seg)"]},{"cell_type":"markdown","metadata":{},"source":["Similarly, if we increase the number of components to 8:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["n = 8\n","gmm = GaussianMixture(n_components=n, covariance_type='tied')\n","gmm.fit(X)\n","labels = gmm.predict(X) # num of pixels x 1\n","seg = np.zeros(X.shape) # num of pixels x 3\n","\n","for label in range(n):\n","    seg[labels == label] = gmm.means_[label]\n","seg = seg.reshape(img.shape).astype(np.uint8)\n","#cv2.imwrite(f'gauss-cat-{n}.jpeg', seg)\n","\n","plt.figure(figsize=(6,6))\n","plt.imshow(seg)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Our segmented image looks remarkably similar to the original, even though it uses only 8 colors!\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["# Exercises\n"]},{"cell_type":"markdown","metadata":{},"source":["For the exercises, we will use the Customer Personality Analysis dataset ([marketing_campaign.csv](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01)) from Kaggle, provided by Dr. Omar Romero-Hernandez. Several preprocessing steps were taken, including but not limited to, impute missing values, feature engineering, and feature encoding.\n","\n","The preprocessed customers dataset contains 19 attributes on the customers' purchasing behaviors, education, income, marital status, etc. You will be able to practice principal component analysis, and apply GMM on the dataset for the purpose of customer segmentation.\n"]},{"cell_type":"markdown","metadata":{},"source":["First we load data into a `pandas.DataFrame`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/customers.csv\")\n","data.head()\n","\n","# you can also download the csv file to your local workspace using:\n","# ! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/customers.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 1 - Scale the data (using StandardScaler)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","SS = # TODO \n","X = # TODO "]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","from sklearn.preprocessing import StandardScaler\n","SS = StandardScaler()\n","X = SS.fit(data).transform(data)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["### Exercise 2 - Use PCA with n_components = 2 for dimension reduction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca2 = # TODO \n","reduced_2_PCA = # TODO "]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","from sklearn.decomposition import PCA\n","pca2 = PCA(n_components=2)\n","reduced_2_PCA = pca2.fit(X).transform(X)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 3 - Fit a GMM to the reduced data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["model = # TODO \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","model = GaussianMixture(n_components=4, random_state=0)\n","model.fit(reduced_2_PCA)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 4 - Output the predicted labels for visualizing clusters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["PCA_2_pred = # TODO "]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","PCA_2_pred = model.predict(reduced_2_PCA)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["Now you can plot the clusters in 2d using 2 principal components, colored by predicted labels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"vscode":{"languageId":"python"}},"outputs":[],"source":["x = # TODO \n","y = # TODO \n","plt.scatter(x, y, c=PCA_2_pred)\n","plt.title(\"2d visualization of the clusters\")\n","plt.xlabel(\"PCA 1\")\n","plt.ylabel(\"PCA 2\")"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","x = reduced_2_PCA[:,0]\n","y = reduced_2_PCA[:,1]\n","plt.scatter(x, y, c=PCA_2_pred)\n","plt.title(\"2d visualization of the clusters\")\n","plt.xlabel(\"PCA 1\")\n","plt.ylabel(\"PCA 2\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 5 - Clustering and visualizing using 3 principal components (OPTIONAL) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"vscode":{"languageId":"python"}},"outputs":[],"source":["# use PCA with n=3 and implement GMM to make predictions\n","\n","pca3 = # TODO \n","reduced_3_PCA = # TODO \n","mod = # TODO \n","PCA_3_pred = # TODO \n","\n","# plotting\n","\n","reduced_3_PCA = pd.DataFrame(reduced_3_PCA, columns=(['PCA 1', 'PCA 2', 'PCA 3']))\n","fig = plt.figure(figsize=(10,8))\n","ax = fig.add_subplot(111, projection=\"3d\")\n","ax.scatter(reduced_3_PCA['PCA 1'],reduced_3_PCA['PCA 2'],reduced_3_PCA['PCA 3'], c=PCA_3_pred)\n","ax.set_title(\"3D projection of the clusters\")"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","# PCA with n=3 and clustering\n","pca3 = PCA(n_components=3)\n","reduced_3_PCA = pca3.fit(X).transform(X)\n","mod = GaussianMixture(n_components=4, random_state=0)\n","PCA_3_pred = mod.fit(reduced_3_PCA).predict(reduced_3_PCA)\n","\n","# plotting\n","reduced_3_PCA = pd.DataFrame(reduced_3_PCA, columns=(['PCA 1', 'PCA 2', 'PCA 3']))\n","fig = plt.figure(figsize=(10,8))\n","ax = fig.add_subplot(111, projection=\"3d\")\n","ax.scatter(reduced_3_PCA['PCA 1'],reduced_3_PCA['PCA 2'],reduced_3_PCA['PCA 3'], c=PCA_3_pred)\n","ax.set_title(\"3D projection of the clusters\")\n","```\n","\n","</details>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
