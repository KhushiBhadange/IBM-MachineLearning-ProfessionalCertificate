{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Principle Component Analysis**\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/PCA.jpeg)\n","\n","Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Use cases of PCA\n","\n","*   Facial Recognition\n","*   Image Compression\n","*   Finding patterns in data of high dimension in the field of quantitative finance.\n","\n","For instance, suppose you are a fund manager who has 200 stocks in a portfolio. To analyze the potential movements and relationships of the stocks, you would need to  at least work with a 200$\\times$200 correlation or covariance matrix, which is very complex.\n","\n","However, instead of looking at 200 stock variances, would it be more efficient to just look at 10 most dominant/principal directions of variances that best represent the original variances of the stocks?\n","\n","PCA is a methodology to reduce the dimensionality of a complex problem.\n","\n","<img src='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/fundmanager.jpg' style=\"width: 70%\">\n"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, you will explore how to simplify and reduce the dimensionality of various data using **principle component analysis** (PCA)\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## **Table of Contents**\n","\n","<!-- <a href=\"#Principle-Component-Analysis\">Principle Component Analysis</a> -->\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li><a href=\"https://#Datasets\">Datasets</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Background\">Background</a>\n","        <ol>\n","            <li><a href=\"https://##What-does-PCA-do?\">What does PCA do?</a></li>\n","            <li><a href=\"https://#How-does-PCA-work?-(optional)\">How does PCA work? (optional)</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://##Visual-Example\">Visual Example</a>\n","        <ol>\n","            <li><a href=\"https://#Scaling-data\">Scaling Data</a></li>\n","            <li><a href=\"https://#Applying-PCA\">Applying PCA</a></li>\n","            <li><a href=\"https://#Putting-it-all-Together\">Putting it all Together</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Using-PCA-to-Improve-Facial-Recognition\">Using PCA to Improve Facial Recognition</a>\n","    </li>\n","</ol>\n","\n","<a href=\"https://#Exercises\">Exercises</a>\n","\n","<ol>\n","    <li><a href=\"https://#Exercise-1---Scaling-the-Data\">Exercise 1 - Scaling the Data</a></li>\n","    <li><a href=\"https://#Exercise-2---Fitting-PCA-Object\">Exercise 2 - Fitting PCA Object</a></li>\n","    <li><a href=\"https://#Exercise-3---Finding-Desired-Number-of-Components\">Exercise 3 - Finding Desired Number of Components</a></li>\n","    <li><a href=\"https://#Exercise-4---Dimensionality-Reduction\">Exercise 4 - Dimentionality Reduction</a></li>\n","</ol>\n"]},{"cell_type":"markdown","metadata":{},"source":["***\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","metadata":{},"source":["*   **Understand** what PCA is and how (generally) it works.\n","*   **Understand** when PCA is useful.\n","*   **Apply** PCA effectively.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Datasets\n","\n","Datasets for this lab are gathered from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) under the MIT License.\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or locally) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install pandas==1.3.4 ...\""]},{"cell_type":"markdown","metadata":{},"source":["### Importing Required Libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Surpress warnings from using older version of sklearn:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from itertools import accumulate\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.datasets import fetch_lfw_people\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import SVC\n","from scipy.stats import loguniform\n","\n","warnings.filterwarnings('ignore')\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Helper Functions\n","\n","Below, we define helper functions to simplify your code later on:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_explained_variance(pca):\n","    # This function graphs the accumulated explained variance ratio for a fitted PCA object.\n","    acc = [*accumulate(pca.explained_variance_ratio_)]\n","    fig, ax = plt.subplots(1, figsize=(50, 20))\n","    ax.stackplot(range(pca.n_components_), acc)\n","    ax.scatter(range(pca.n_components_), acc, color='black')\n","    ax.set_ylim(0, 1)\n","    ax.set_xlim(0, pca.n_components_-1)\n","    ax.tick_params(axis='both', labelsize=36)\n","    ax.set_xlabel('N Components', fontsize=48)\n","    ax.set_ylabel('Accumulated explained variance', fontsize=48)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Background\n","\n","Before we begin using **PCA**, we should first understand:\n","\n","1.  What PCA does\n","2.  How PCA Works\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["### What does PCA do?\n","\n","*   Reduces the dimensionality of data. By reducing data dimensionality, PCA can also help with visualization.\n","*   May reduce noise in the process\n","\n","\\-Can be used to pre-process data improving the result of your algorithm\n"]},{"cell_type":"markdown","metadata":{},"source":["### How does PCA work? (optional)\n","\n","1.  Looks at an $n$ -dimensional dataset and breaks it down into \"general trends\" or **components**\n","\n","```\n","- When we say \"$n$-dimensional\", we mean the data has $n$ features.\n","```\n","\n","2.  The components are then **sorted by how much of the explained variance they account for** (*eigenvalues* provide this information)\n","\n","```\n","- This means if a component is highly-uncorrelated with all others, it's a \"strong\" component and provides useful information that is very hard to infer from all other components.\n","```\n","\n","3.  Then, given some parameter (usually chosen by the data engineer), the new dimension of the data is decided. Let this be $k$.\n","\n","```\n","- Note $k$ is always $k \\leq n$ because we're only trying to reduce the dimension of our data.\n","```\n","\n","4.  Finally, the original $n$ dimensional dataset is projected onto the $k$-dimensional plane chosen by our **top-$k$ components that take care of the most explained variance**.\n","\n","```\n","- These top- $k$ components are now used \n","```\n","\n","Because principle components span an (at most) $k$-dimensional surface, we have successfully reduced our data to at least $k \\leq n$ dimensions!\n"]},{"cell_type":"markdown","metadata":{},"source":["## Visual Example\n","\n","Let's look at an example that will visually demonstrate PCA in action.\n","\n","Load the dataset `HeightsWeights.csv` which contains a list of various people's heights (in inches) and weight (in pounds and kg):\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hwdf = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/HeightsWeights.csv', index_col=0)\n","hwdf.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Scaling data\n","\n","**You should (almost) always scale your data before applying PCA**\n","\n","**Why?**: There are many reasons, here are some:\n","\n","*   Scaling your features make the features have the same standard deviation => same weight.\n","\n","*   If the features have the same weight, PCA is able to best find the most significant components (principal components) without being biased towards features with high variance.\n","\n","*   Computers do not do well in adding large numbers and small numbers, so, if all data is in the same range  algorithms usually perform better.\n","\n","Let's use the `StandardScaler` from `sklearn.preprocessing`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()\n","hwdf[:] = scaler.fit_transform(hwdf)\n","hwdf.columns = [f'{c} (scaled)' for c in hwdf.columns]\n","hwdf.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's look at 3-D plot of our data (one dimension per feature):\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(121, projection='3d')\n","xs, ys, zs = [hwdf[attr] for attr in hwdf.columns]\n","ax1.scatter(xs, ys, zs)\n","\n","ax2 = fig.add_subplot(122, projection='3d')\n","xs, ys, zs = [hwdf[attr] for attr in hwdf.columns]\n","ax2.view_init(elev=10, azim=-10)\n","ax2.scatter(xs, ys, zs)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, our data here forms a plane.\n","\n","This is because the *weight in kilograms does not provide any more information than weight in pounds* (or vice-versa).\n","\n","This becomes clear with the following, alternate perspective, showing the 2d relationships between the pairs of data and calculating the correlation  :\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.pairplot(hwdf)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hwdf.corr().style.background_gradient(cmap='coolwarm')"]},{"cell_type":"markdown","metadata":{},"source":["We see the weights are perfectly correlated, which means **Weight(Pounds)** tells us everything we need to know about **Weight(Kilograms)** ,thus we have clearly **redundant** data! Although, this example is exaggerated, it'll help demonstrate where PCA shines.\n"]},{"cell_type":"markdown","metadata":{},"source":["<b>Note</b> Standardizing your data before applying PCA is called *whitening*.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Applying PCA\n","\n","It's time to apply PCA, let's first apply PCA keeping the same dimension as the original data, i.e.: `n_components=3`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA()\n","pca.fit(hwdf)"]},{"cell_type":"markdown","metadata":{},"source":["We can find the projection of the dataset onto the principal components call it `Xhat` , this is our \"new\" dataset, it is the same shape as the original dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Xhat = pca.transform(hwdf)\n","Xhat.shape"]},{"cell_type":"markdown","metadata":{},"source":["Let’s look at the new dataset as a dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hwdf_PCA = pd.DataFrame(columns=[f'Projection on Component {i+1}' for i in range(len(hwdf.columns))], data=Xhat)\n","hwdf_PCA.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Why** are the values in the third column all essentially zero?\n","\n","Let's look at the principle components:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["colors = ['red', 'red', 'green']\n","\n","fig = plt.figure(figsize=(12,8))\n","ax1 = fig.add_subplot(121, projection='3d')\n","xs, ys, zs = [hwdf[attr] for attr in hwdf.columns]\n","ax1.view_init(elev=10, azim=75)\n","ax1.scatter(xs, ys, zs)\n","\n","for component, color in zip(pca.components_, colors):\n","    ax1.quiver(*[0, 0, 0], *(8 * component), color=color)\n","\n","    \n","ax2 = fig.add_subplot(122, projection='3d')\n","xs, ys, zs = [hwdf[attr] for attr in hwdf.columns]\n","ax2.view_init(elev=0, azim=0)\n","ax2.scatter(xs, ys, zs)\n","\n","for component, color in zip(pca.components_, colors):\n","    ax2.quiver(*[0, 0, 0], *(8 * component), color=color)\n","\n","plt.show()\n","\n","for color, ev in zip(colors, pca.explained_variance_ratio_):\n","    print(f'{color} component accounts for {ev * 100:.2f}% of explained variance')"]},{"cell_type":"markdown","metadata":{},"source":["The 3 colored arrows represent the directions of maximum variance in the original data `hwdf`. The new dataset `Xhat` is the projection of `hwdf` onto each principal component.\n","\n","Most of the original data seems parallel to the red principle components meaning they are the two most dominant directions of variance of `hwdf`. The  green component is perpendicular to the data, as a result the projection is small.\n"]},{"cell_type":"markdown","metadata":{},"source":["We convert this new data to a Dataframe and see the points appear uncorrelated:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.pairplot(hwdf_PCA)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hwdf_PCA.corr().style.background_gradient(cmap='coolwarm')"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, the correlations of the 3 principal components are now zero, meaning we have successfully de-correlated `hwdf` and obtained features that are linearly independent of each other.\n","\n","Each component provides variance/information on a different direction. As we saw before that, the third component had a small projection, which means it doesn't provide much information about our original data `hwdf` in the new feature space.\n","\n","Thus, we can remove the third dimension, while still keeping the vast majority of our data's information:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hwdf_PCA.drop('Projection on Component 3', axis=1, inplace=True)\n","hwdf_PCA.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Putting it all Together\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that you have some intuition behind PCA, let's start from the beginning and understand the PCA-pipeline.\n","\n","In **sklearn.decomposition.PCA**, there is a parameter called `whiten` which helps standardize your input data if you set `whiten = True`. You could also use `StandardScaler()` as a separate step before using PCA.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()\n","X = pd.DataFrame(scaler.fit_transform(hwdf), index=hwdf.index, columns=hwdf.columns)\n","X.head()\n","\n","pca = PCA()\n","X_PCA = pd.DataFrame(pca.fit_transform(X), index=X.index, columns=[f'Component {i}' for i in range(pca.n_components_)])\n","# (Remember it's technically \"Projection onto Component {i}\")\n","X_PCA.head()"]},{"cell_type":"markdown","metadata":{},"source":["By default, `sklearn.decomposition.PCA` sorts the components by their explained variance.\n","\n","Let's analyze the explained variance ratios:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_explained_variance(pca)"]},{"cell_type":"markdown","metadata":{},"source":["Suppose a $99%$ threshold is sufficient for our task, let's see how many components (dimensions) we can drop:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["threshold = 0.99\n","num = next(i for i, x in enumerate(accumulate(pca.explained_variance_ratio_), 1) if x >= threshold)\n","print(f'We can keep the first {num} components and discard the other {pca.n_components_-num},')\n","print(f'keeping >={100 * threshold}% of the explained variance!')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_PCA.drop([f'Component {i}' for i in range(num, pca.n_components_)], axis=1, inplace=True)\n","X_PCA.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Using PCA to Improve Facial Recognition\n","\n","PCA is commonly used for Facial Recognition.\n","\n","In this example, we will apply a method called \"**Eigenfaces**\"\n","\n","The idea of *eigenfaces* is:\n","\n","1.  You have images of faces of dimension $a \\times b$ pixels.\n","2.  You \"roll\" these out into vectors of size $a \\cdot n$.\n","3.  You apply PCA to the vectors.\n","4.  You determine how many principal components you want to train under; let's call this $C$.\n","5.  You train on the original image-vectors of size $a \\cdot b$ projected onto your $C$ components, reshaped back to $a \\times b$ bitmaps.\n"]},{"cell_type":"markdown","metadata":{},"source":["Load the Labeled Faces in the Wild (LFW) people datasetclassification\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"]},{"cell_type":"markdown","metadata":{},"source":["Introspect the images arrays to find the shapes (for plotting)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# introspect the images arrays to find the shapes (for plotting)\n","N, h, w = lfw_people.images.shape\n","target_names = lfw_people.target_names"]},{"cell_type":"markdown","metadata":{},"source":["We load our features <code>X</code> and labels <code>y</code>. The images are flattened such that each one is a row in the NumPy array <code>X</code>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = lfw_people.target\n","X = lfw_people.data\n","n_features = X.shape[1]"]},{"cell_type":"markdown","metadata":{},"source":["We plot out each class and an image belonging to that class:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for person in np.unique(lfw_people.target):\n","    idx = np.argmax(lfw_people.target == person)\n","    plt.imshow(lfw_people.images[idx], cmap='gray')\n","    plt.title(lfw_people.target_names[person])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We split the data into training and testing\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, random_state=42\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We train a Support Vector Machines model for classification and use a random search method to find a set of optimal hyperparameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["param_grid = {\n","    \"C\": loguniform(1e3, 1e5),\n","    \"gamma\": loguniform(1e-4, 1e-1)\n","}\n","clf = RandomizedSearchCV(\n","    SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\n",")\n","clf = clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["We make a prediction using the test data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Let's see how well our `SVC` did on the test data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hmap = sns.heatmap(\n","    confusion_matrix(y_test, y_pred),\n","    annot=True,\n","    xticklabels=lfw_people.target_names,\n","    yticklabels=lfw_people.target_names,\n","    fmt='g'\n",")\n","hmap.set_xlabel('Predicted Value')\n","hmap.set_ylabel('Truth Value')"]},{"cell_type":"markdown","metadata":{},"source":["We see all the images are being classifed as George Bush. Clearly it's having trouble differentiating between the faces.\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, let’s try using PCA, we fit a PCA model :\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA(svd_solver='full',  whiten=True).fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["We find the projections on to each principle  component for a person int the dataset, we select the sample <code>person_index</code> :\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["person_index=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Xhat=pca.transform(X[person_index,:].reshape(1, -1))"]},{"cell_type":"markdown","metadata":{},"source":["We can find the projections back to each component, i.e the inverse transform;as we use all the non-zero components the images are identical.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(pca.inverse_transform(Xhat).reshape(h, w), cmap='gray')\n","plt.title(\"Image after PCA and inverse transform\"  ) \n","plt.show()\n","plt.imshow(lfw_people.images[person_index],cmap='gray')\n","plt.title(\"Image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can use the Explained variance-ratio to determine the number of components to keep, we can plot it as Cumulative distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_explained_variance(pca)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["It looks like 150 components explain over 95% of the variance, usually 80% will do, let’s try and visualize some components.\n","\n","<p>\n","<b>Note:</b> you can use Cross-validation to select the number of components  \n","</p>\n"]},{"cell_type":"markdown","metadata":{},"source":["Let’s select the  components that explain over 60% of the variance\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["threshold = 0.60"]},{"cell_type":"markdown","metadata":{},"source":["This corresponds to 7 principle components\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["components = np.cumsum(pca.explained_variance_ratio_) < threshold\n","components.sum()"]},{"cell_type":"markdown","metadata":{},"source":["We can reshape the principle components to a rectangle  and plot them, remember the images are linear  combinations of these components\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for component in pca.components_[components,:]:\n","    plt.imshow(component.reshape(h, w),cmap='gray')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's now use PCA with `n_components = 150`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA(n_components=150, svd_solver=\"randomized\", whiten=True).fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["We apply the PCA transform on the training and testing data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_pca = pca.transform(X_train)\n","X_test_pca = pca.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["**NOTE**: We can also transform the data back (\"inverse_transform\") to its original space, with the rest of the components to zero, then convert it to an image. For instance, let's look at one of the images using <code>person_index = 1</code>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["person_index = 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 4))\n","plt.subplot(1,2,1)\n","plt.imshow(lfw_people.images[person_index,:,:],cmap='gray')\n","plt.title(\"Original image\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(pca.inverse_transform(pca.transform(X[person_index ,:].reshape(1, -1))).reshape(h, w),cmap='gray')\n","plt.title(\"PCA transformed and inverse-transformed image \") \n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We train the model and find the best Hyperparameters using the transformed data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["param_grid = {\n","    \"C\": loguniform(1e3, 1e5),\n","    \"gamma\": loguniform(1e-4, 1e-1),\n","}\n","clf = RandomizedSearchCV(\n","    SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=10\n",")\n","\n","clf = clf.fit(X_train_pca, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["We see the model using PCA performs much better!\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = clf.predict(X_test_pca)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hmap = sns.heatmap(\n","    confusion_matrix(y_test, y_pred),\n","    annot=True,\n","    xticklabels=lfw_people.target_names,\n","    yticklabels=lfw_people.target_names,\n","    fmt='g'\n",")\n","hmap.set_xlabel('Predicted Value')\n","hmap.set_ylabel('Truth Value')"]},{"cell_type":"markdown","metadata":{},"source":["# Exercises\n","\n","Run the following code cell to complete the exercises\n","\n","We download and clean the dataset `energydata_complete.csv` containing real-world energy data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/energydata_complete.csv')\n","df.drop('date', axis=1, inplace=True)\n","df = df.dropna().astype(np.float64)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["The columns can be described as follows:\n","\n","*   `Appliances` - energy use of appliances in Wh\n","*   `lights` - energy use of light fixtures in the house in Wh\n","*   `T1` - Temperature in kitchen area, in Celsius\n","*   `RH_1` - Humidity in kitchen area, in $%$\n","*   `T2` - Temperature in living room area, in Celsius\n","*   `RH_2` - Humidity in living room area, in $%$\n","*   $\\dots$\n","*   `To` - Temperature outside (from Chievres weather station), in Celsius\n","*   `Pressure` - (from Chievres weather station), in mm Hg\n","*   `RH_out` - Humidity outside (from Chievres weather station), in %\n","*   `Wind speed` - (from Chievres weather station), in m/s\n","*   `Visibility` - (from Chievres weather station), in km\n","*   `Tdewpoint` - (from Chievres weather station), Â°C\n","*   `rv1`, `rv2` - Random variable 1 & 2, nondimensional\n","\n","Please see the [data source](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for more information.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 1 - Scaling the Data\n"]},{"cell_type":"markdown","metadata":{},"source":["Scale the data in `df` using sklearn.preprocessing.StandardScaler\n","\n","(You don't need to change the names of the columns like we did in the examples).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","scaler = StandardScaler()\n","df[:] = scaler.fit_transform(df)\n","df.head()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 2 - Fitting PCA Object\n"]},{"cell_type":"markdown","metadata":{},"source":["Create a `PCA` object called `pca` and fit it to the dataframe `df`\n","\n","(You don't need to change the names of the columns).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","pca = PCA()\n","pca.fit(df)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 3 - Finding Desired Number of Components\n"]},{"cell_type":"markdown","metadata":{},"source":["Find the minimum number of components that cover a total explained variance of $95%$ or more\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for a Sample Solution</summary>\n","\n","```python\n","np.argwhere(pca.explained_variance_ratio_.cumsum() >= 0.95)[0][0] + 1\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 4 - Dimensionality Reduction\n"]},{"cell_type":"markdown","metadata":{},"source":["Reduce your dataset to one with PCA applied to it, where the new number of dimensions is the answer you got in the Exercise 3.\n","\n","Assign the result to a variable called `reduced_data`. (No need to convert the result to a `pandas.DataFrame`).\n","\n","Hint: It might be easiest to use a new `PCA` object using the [`n_components` parameter](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for a Sample Solution</summary>\n","\n","```python\n","pca = PCA(n_components=12)\n","reduced_data = pca.fit(df).transform(df)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's view the result as a `pandas.DataFrame`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(reduced_data, columns=[f'Component {i}' for i in range(reduced_data.shape[1])]).head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
