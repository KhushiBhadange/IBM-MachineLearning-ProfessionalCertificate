{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Matrix Review\n","\n","Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","metadata":{},"source":["Matrix operations are a fundamental part of machine learning and many fields, such as digital signal processing, optimization, control systems, computer graphics, and so on. As a result, there has been a lot of work on optimizing matrix operations, not only in software, but in hardware. Let's say you work for a AI hardware  company, the company has developed a new processor that can only perform matrix operations. The company needs you to implement some popular  statistics such as mean, variance and Principle Component Analysis using Matrix operations\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/technology-electronic-device-computer-hardware-computer-component-electronics-motherboard-personal-computer-hardware-cpu-computer-accessory-1536663.jpeg\" width=\"500\" alt=\"https://pxhere.com/en/photo/1536663\">\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Table of Contents\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Basics of Matrices\">Basics of Matrices</a>\n","        <ol>\n","            <li><a href=\"https://https://https://#What's a Matrix?\">What's a Matrix? </a></li>\n","            <li><a href=\"https://https://https://#Rank of a Matrix\">Rank of a Matrix </a></li>\n","            <li><a href=\"https://#Frobenius Norm of a Matrix\">Frobenius Norm of a Matrix</a></li>\n","            <li><a href=\"https://#Matrix Additione\"> Matrix Addition</a></li>\n","        </ol>\n","    </li>\n","  <li>\n","        <a href=\"https://https://#Matrix and Vector Multiplication\">Matrix and Vector Multiplication</a>\n","        <ol>\n","            <li><a href=\"#What's a Matrix?\"> Dot-Product </a></li>\n","            <li><a href=\"#Rank of a Matrix\">The Outer product</a></li>\n","            <li><a href=\"#Matrix and Vector Multiplication\">Matrix and Vector Multiplication</a></li>\n","            <li><a href=\"https://#Matrix Multiplication\">Multiplying Matrices</a></li>\n","     </ol>\n","   </li>\n","   <li>\n","        <a href=\"https://#Eigen Decomposition\">Eigen Decomposition</a>\n","        <ol>\n","            <li><a href=\"#What's a Matrix?\"> Eigenvectors and Eigenvalues </a></li>\n","            <li><a href=\"#Rank of a Matrix\">The Factorization for PCA</a></li>\n","        </ol>     \n","   </li>\n","\n"," </ol>    \n"]},{"cell_type":"markdown","metadata":{},"source":["***\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   **Understand** Basic Matrix operations such as Matrix addition, Vector Multiplication  Eigen decomposition\n","\n","*   **Apply** Apply these Matrix operations using numpy\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Installing required libraries\n"]},{"cell_type":"markdown","metadata":{},"source":["The following required modules are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install pandas ==1.3.4 ...\"\n"]},{"cell_type":"markdown","metadata":{},"source":["You can install and use  <a href=\"https://www.sympy.org/en/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\">SymPy </a> to print matrices\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["!conda install -c anaconda sympy -y"]},{"cell_type":"markdown","metadata":{},"source":["### Importing required libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Surpress warnings from using older version of sklearn:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import numpy as np \n","import matplotlib.pylab as plt\n","\n","from sklearn.decomposition import PCA\n","from sympy import Matrix, init_printing,Symbol\n","from numpy.linalg import qr,eig,inv,matrix_rank,inv, norm\n","from scipy.linalg import null_space\n","init_printing()"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Helper Functions\n","\n","*Use this section to define any helper functions to help the notebook's code readability:*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def plot_2d(dict_):\n","    for key, value in dict_.items():\n","        if value.shape[0]>2:\n","            plt.scatter(value[:, 0], value[:, 1],label=key)\n","        else:\n","            print(value)\n","            plt.quiver([0],[0],value[:,0],value[:,1],label=key)\n","\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Basics of Matrices\n"]},{"cell_type":"markdown","metadata":{},"source":["### What's a Matrix\n"]},{"cell_type":"markdown","metadata":{},"source":["Numpy treats matrix objects as numpy arrays, so we will use Numpy to create matrices. Let's consider the square matrix A.\n"]},{"cell_type":"markdown","metadata":{},"source":["**NOTE**: We use the function `Matrix` to print out the Matrix , each column of matrix has two columns: Matrix $\\mathbf{A}=[\\mathbf{a}*{1},\\mathbf{a}*{2}]$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["A=np.array([[2,-3],[4,7]])\n","\n","Matrix(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a1=A[:,0]\n","a1"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a2=A[:,1]\n","a2"]},{"cell_type":"markdown","metadata":{},"source":["Denoted by $A^{T}$, the transpose of a matrix  switches the row and column indices of the matrix, in numpy we can determine the transpose of $A$ as follows:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["AT=A.T\n","Matrix(AT)"]},{"cell_type":"markdown","metadata":{},"source":["### Rank of a Matrix\n"]},{"cell_type":"markdown","metadata":{},"source":["The rank of a matrix is the number of dimension the rows of the matrix \"live in\".  You can verify the Matrix truly behaves like a  square matrix if it's <a href='https://en.wikipedia.org/wiki/Rank_(linear_algebra)?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01'>rank</a> is equal to the number of rows or columns, then it is referred to as **full rank**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["matrix_rank(A)"]},{"cell_type":"markdown","metadata":{},"source":["We can plot the columns of $A$ as vectors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (12, 7))\n","\n","ax.quiver([0, 0],[0, 0],A[0,0], A[1,0],scale=30,label=\"$a_{1}$\")\n","ax.quiver([0, 0],[0, 0],A[0,1], A[1,1],scale=30,label=\"$a_{2}$\")\n","plt.title(\"columns of $A$ \")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As the vectors are not parallel, we can define any point on the 2d space as a scaled  combination of those two vectors.\n","\n","If any of the columns in a matrix  are multiples of each other, the vectors point in the same direction and the matrix is not full rank.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["F=np.array([[2,4],[4,8]])\n","matrix_rank(F)"]},{"cell_type":"markdown","metadata":{},"source":["We see the columns are pointing  in the same direction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (12, 7))\n","ax.quiver([0, 0],[0, 0],F[0,1], F[1,1],scale=30,label=\"$f_{2}$\",color='red')\n","ax.quiver([0, 0],[0, 0],F[0,0], F[1,0],scale=30,label=\"$f_{1}$\")\n","plt.title(\"columns of $F$ \")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["A common occurance is a matrix with more columns than rows, but in data, more rows than columns is more common. Consider the matrix $F$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["F=np.array([[1,2],[1,-2],[-1,1]])\n","Matrix(F)"]},{"cell_type":"markdown","metadata":{},"source":["Although, the rows of matrix $F$ are 3 dimensions, you can only describe the point lying in the 2D plane, as shown here:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["ax = plt.figure().add_subplot(projection='3d')\n","p=null_space(F.T)\n","xx, yy = np.meshgrid(np.arange(-3,3,0.1), np.arange(-3,3,0.1))\n","z=(p[0]*xx+p[1]*yy)/p[2]\n","ax.plot_surface(xx, yy, z, alpha=0.1)\n","ax.quiver([0,0], [0,0], [0,0], F[0,:], F[1,:], F[2,:])\n","\n","ax.set_xlim([-3, 3])\n","ax.set_ylim([-3, 3])\n","ax.set_zlim([-3, 3])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Therefore, the rank is 2.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["matrix_rank(F)"]},{"cell_type":"markdown","metadata":{},"source":["### Frobenius Norm of a Matrix\n"]},{"cell_type":"markdown","metadata":{},"source":["The Frobenius norm, sometimes also called the **Euclidean norm**, is the matrix norm of an $m√ón$. Matrix A is defined as the square root of the sum of the absolute squares of its elements:\n","\n","$$|A| *F = \\sqrt{\\sum*{i=1}^m \\sum\\_{j=1}^n |a\\_{ij}|^2}$$\n","\n","Let's use the matrix A again as an example:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(A), norm(A)"]},{"cell_type":"markdown","metadata":{},"source":["You could also calculate the matrix norm manually, using the formula provided above, and confirm that the results are the same.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["m, n = A.shape[0], A.shape[1] # get number of rows and columns \n","ss = 0\n","\n","for i in range(m):\n","    for j in range(n):\n","        ss += A[i,j] ** 2\n","np.sqrt(ss)"]},{"cell_type":"markdown","metadata":{},"source":["### Matrix Addition\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider matrix $B$:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["B=np.array([[1,1],[1,-1]])\n","Matrix(B)"]},{"cell_type":"markdown","metadata":{},"source":["In general, if we have matrix $\\mathbf{A}$ and matrix $\\mathbf{B}$\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\begin{align}\n","\\mathbf{A}= \\begin{bmatrix}\n","a\\_{11} & a\\_{12} & \\cdots & a\\_{1n} \\\\\\\\\\\\\n","a\\_{21} & a\\_{22} & \\cdots & a\\_{2n} \\\\\\\\\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\\n","a\\_{m1} & a\\_{m2} & \\cdots & a\\_{mn} \\\\\\\\\\\\\n","\\end{bmatrix}\n","\\end{align}$\n","$\\begin{align}\n","\\mathbf{B}=\\begin{bmatrix}\n","b\\_{11} & b\\_{12} & \\cdots & b\\_{1n} \\\\\\\\\\\\\n","b\\_{21} & b\\_{22} & \\cdots & b\\_{2n} \\\\\\\\\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\\n","b\\_{m1} & b\\_{m2} & \\cdots & b\\_{mn} \\\\\\\\\\\\\n","\\end{bmatrix}\n","\\end{align}$\n"]},{"cell_type":"markdown","metadata":{},"source":["To be added, the two matrices must have an equal number of rows and columns, where we add the corresponding row and column element:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\begin{align}\n","\\mathbf{A}+\\mathbf{B}=\\begin{bmatrix}\n","a\\_{11} + b\\_{11} & a\\_{12} + b\\_{12} & \\cdots & a\\_{1n} + b\\_{1n} \\\\\\\\\\\\\n","a\\_{21} + b\\_{21} & a\\_{22} + b\\_{22} & \\cdots & a\\_{2n} + b\\_{2n} \\\\\\\\\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\\n","a\\_{m1} + b\\_{m1} & a\\_{m2} + b\\_{m2} & \\cdots & a\\_{mn} + b\\_{mn} \\\\\\\\\\\\\n","\\end{bmatrix}\n","\\end{align}$\n"]},{"cell_type":"markdown","metadata":{},"source":["In numpy, we can add two arrays as follows:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["C=A+B\n","Matrix(C)"]},{"cell_type":"markdown","metadata":{},"source":["This matrix is symmetric. This means $B=B^{T}$, we can verify this in Python:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["B_T=B.T\n","Matrix(B_T)"]},{"cell_type":"markdown","metadata":{},"source":["Arbitrary semantic matrix $S=C+C^T$, where $C$ is any matrix.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["C=np.random.randn(2,2)\n","S=C+C.T"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(S)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(S.T)"]},{"cell_type":"markdown","metadata":{},"source":["<b>Diagonal matrix</b> is a matrix in which the entries outside of the main diagonal are all zero.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(np.diag(np.array([1,2,3])))"]},{"cell_type":"markdown","metadata":{},"source":["If the main diagonal is one, it's called a identity matrix in numpy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(np.eye(3))"]},{"cell_type":"markdown","metadata":{},"source":["## Matrix and Vector Multiplication\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dot-Product\n"]},{"cell_type":"markdown","metadata":{},"source":["We can define a vector as a one dimensional array of $\\mathbf{a}$ and $\\mathbf{b}$, and we can find the dot product of the two as follows:\n"]},{"cell_type":"markdown","metadata":{},"source":["$ a^T  b = \\sum\\_{i=1}^n a_i b_i = a\\_1 b\\_1 + \\cdots + a_n b_n$\n"]},{"cell_type":"markdown","metadata":{},"source":["Where  $\\mathbf{a}$ and  $\\mathbf{b}$ have the same length, in numpy:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a=np.array([1,1])\n","b=np.array([1,2])"]},{"cell_type":"markdown","metadata":{},"source":["We can verify that the dimension is one.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a.ndim"]},{"cell_type":"markdown","metadata":{},"source":["Find the dot product\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a@b"]},{"cell_type":"markdown","metadata":{},"source":["If the array is 2- dimension, the order matters. $a^{T}$ must be a row vector of shape $1xm$ and $b$ a column vector of shape $mx1$.\n"]},{"cell_type":"markdown","metadata":{},"source":["$a^{T} = \\begin{bmatrix}\n","a\\_{1},a\\_{2},..,a\\_{n}\n","\\end{bmatrix}$\n","\n","$b = \\begin{bmatrix}\n","b\\_{1} \\\\\\\\\\\\\n","b\\_{2} \\\\\\\\\\\\\n","\\vdots \\\\\\\\\\\\\n","b\\_{n}\n","\\end{bmatrix}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["a=np.array([[1],[1]])\n","b=np.array([[1],[2]])\n","a.T@b"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 1\n","\n","Calculate the dot product of $a$ and $one$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["one=np.ones(2)\n","\n","# TO DO \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","a.T@one\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### The Outer product\n"]},{"cell_type":"markdown","metadata":{},"source":["The outer product is another useful matrix operation. Consider the matrix or vector $ \\mathbf{u}$ of size $nx1$ and matrix or vector $\\mathbf{v}$ $1xm$. Then their outer product is:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\n","\\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u}\\mathbf{v}^\\textsf{T} =\n","\\begin{bmatrix}u\\_1 \\\\\\\\\\ u\\_2 \\ \\vdots\\ u_n\\end{bmatrix}\n","\\begin{bmatrix}v\\_1 & .. & v_m\\end{bmatrix} =\n","\\begin{bmatrix}\n","u\\_1 v\\_1 & .. & u\\_1 v_m\\\\\\\\\\\\\n","u\\_2 v\\_1 & u\\_2 v\\_2 & u\\_2 v\\_3 \\\\\\\\\\\\\n","\\vdots   & \\ddots & \\vdots \\\\\\\\\\\\\n","u_n v\\_1 &  .. & u_n v_m\n","\\end{bmatrix}.\n","$\n"]},{"cell_type":"markdown","metadata":{},"source":["We can perform the outer product in numpy as follows\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["u= np.array([[1],[2],[3],[4]])\n","v= np.array([[0],[1],[2],[3],[6]])\n","# TO DO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","Matrix(u@v.T)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 2\n","\n","Create a matrix with 4 columns where each even column is  the vector  `u`  or else it is  zero, show the rank is one:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["u=np.array([[1],[2]])\n","# TO DO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","Matrix(u@np.array([[0,1,0,1]]))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Matrix and Vector Multiplication\n"]},{"cell_type":"markdown","metadata":{},"source":["We can also multiply a matrix by a vector and get a new vector. Consider the vector $\\mathbf{x}$:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\n","\\mathbf{x}=\n","\\begin{bmatrix}\n","x\\_1 \\\\\\\\\\\\\n","x\\_2 \\\\\\\\\\\\\n","\\vdots \\\\\\\\\\\\\n","x_n\n","\\end{bmatrix}\n","$\n"]},{"cell_type":"markdown","metadata":{},"source":["The following  $\\mathbf{Ax}=\\mathbf b$  matrix multiplication  is defined as the dot product  $\\mathbf{x}$ with each row of $\\mathbf{A}$ :\n"]},{"cell_type":"markdown","metadata":{},"source":["$\n","\\begin{matrix}a\\_{11}x\\_1+\\cdots + a\\_{1n}x_n=b\\_1\n","\\\\\\\\\\ a\\_{21}x\\_1+\\cdots + a\\_{2n}x_n =b\\_2\n","\\\\\\\\\\ \\vdots\n","\\\\\\\\\\ a\\_{m1}x\\_1+\\cdots + a\\_{mn}x_n =b_m\\end{matrix}\n","$\n"]},{"cell_type":"markdown","metadata":{},"source":["Where ,\n"]},{"cell_type":"markdown","metadata":{},"source":["$\n","\\mathbf{b}=\n","\\begin{bmatrix}\n","b\\_1 \\\\\\\\\\\\\n","b\\_2 \\\\\\\\\\\\\n","\\vdots \\\\\\\\\\\\\n","b_m\n","\\end{bmatrix}\n","$\n"]},{"cell_type":"markdown","metadata":{},"source":["In numpy:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["x=np.array([1,1])"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["A=np.array([[-1,1],[1,2]])"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["b=A@x\n","Matrix(b)"]},{"cell_type":"markdown","metadata":{},"source":["We can see that $b$ is a combination of the rows of $A$:\n","\n","$\\mathbf{b}=x\\_{1}\\mathbf{a}*{1}+\\mathbf{x}*{2}a\\_{2}$ or $\\mathbf{b}$ is a rotation of  $\\mathbf{x}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (12, 7))\n","ax.quiver([0, 0],[0, 0],A[0,0], A[1,0],scale=10,label=\"$a_{1}$\")\n","ax.quiver([0, 0],[0, 0],A[0,1], A[1,1],scale=10,label=\"$a_{2}$\")\n","ax.quiver([0,0],[0,0],b[0], b[1],scale=10,label=\"b\",color='r')\n","ax.quiver([0,0],[0,0],x[0], x[1],scale=10,label=\"x\",color='b')\n","ax.set_xlim([-10,10])\n","ax.set_ylim([-5,10])\n","fig.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Multiplying Matrices\n"]},{"cell_type":"markdown","metadata":{},"source":["If a matrix $\\mathbf{C}$ is the product of matrix $\\mathbf{A}$ and matrix $\\mathbf{B}$, then the $i-th$ row $j-th$ column is obtained by multiplying term-by-term the entries of the $i-th$ row of $\\mathbf{A}$ and the $j-th$ column of $\\mathbf{B}$, and summing these $n$ products. In other words, you can also think of the  $i-th$ row $j-th$ column as the dot product of the $i-th$ row of $\\mathbf{A}$  and the $j-th$ column of $\\mathbf{B}$:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\mathbf{C}=\\mathbf{AB}$\n","\n","$=\\begin{pmatrix}\n","a\\_{11}b\\_{11} +\\cdots + a\\_{1n}b\\_{n1} & a\\_{11}b\\_{12} +\\cdots + a\\_{1n}b\\_{n2} & \\cdots & a\\_{11}b\\_{1p} +\\cdots + a\\_{1n}b\\_{np} \\\\\\\\\\\\\n","a\\_{21}b\\_{11} +\\cdots + a\\_{2n}b\\_{n1} & a\\_{21}b\\_{12} +\\cdots + a\\_{2n}b\\_{n2} & \\cdots & a\\_{21}b\\_{1p} +\\cdots + a\\_{2n}b\\_{np} \\\\\\\\\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\\n","a\\_{m1}b\\_{11} +\\cdots + a\\_{mn}b\\_{n1} & a\\_{m1}b\\_{12} +\\cdots + a\\_{mn}b\\_{n2} & \\cdots & a\\_{m1}b\\_{1p} +\\cdots + a\\_{mn}b\\_{np} \\\\\\\\\\\\\n","\\end{pmatrix} $\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["C=A@B\n","Matrix(C)"]},{"cell_type":"markdown","metadata":{},"source":["You can repeat the process for more matrices, if the matrix is full rank, we can invert it: $\\mathbf{A}^{-1}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["A_inv=inv(A)\n","Matrix(A_inv)"]},{"cell_type":"markdown","metadata":{},"source":["This can be a complex computation. If we multiply a matrix with its inverse, we get the Identity matrix $\\mathbf{AA}^{-1}=I$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["I=A_inv@A\n","Matrix(I)"]},{"cell_type":"markdown","metadata":{},"source":["If we multiply any square matrix with an Identity matrix, we get the original Matrix, for example $IA=A$ in numpy:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["A@I"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 3\n","\n","Use the inverse of  matrix $\\mathbf{A}$ to solve for $\\mathbf{x}$ given $\\mathbf{b}$ and assign it to `x_` compare the result to `x`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# TO DO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","x_=A_inv@b\n","print(\"x_ :\",x_)\n","print(\"x:\",x)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["Another type of matrixes is the **orthogonal matrix**. An <b>orthogonal matrix</b> is extremely useful: if matrix $Q$ is orthogonal, such as $Q^T=Q^{-1}$, the rows and columns are orthogonal also. For example:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Q=np.array([[1,1],[1,-1]])*2**(-1/2)\n","Q"]},{"cell_type":"markdown","metadata":{},"source":["We can verify that $Q^T=Q^{-1}$:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["I=Q@Q.T\n","Matrix(I)"]},{"cell_type":"markdown","metadata":{},"source":["For an <b>orthogonal matrix</b>, the columns are orthogonal:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (12, 7))\n","ax.quiver([0, 0],[0, 0],B[0,0], B[1,0],scale=10,label=\"$q_{1}$\")\n","ax.quiver([0, 0],[0, 0],B[0,1], B[1,1],scale=10,label=\"$q_{2}$\")\n","plt.title(\"columns of $B$ \")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We usually place our data in a design matrix $X$, which is a matrix where each row represents an individual sample. Consider the following example `X_`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["samples=200\n","\n","u=np.array([[1.0,1.0],[0.10,-0.10]])/(2)**(0.5)\n","\n","X_=np.dot(4*np.random.randn(samples,2),u)+10\n","X_[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["We can plot the samples:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["dict_={\"design matrix samples\":X_}\n","plot_2d(dict_)"]},{"cell_type":"markdown","metadata":{},"source":["We can find the shape of the design matrix:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["N,D=X_.shape\n","print(\"number of smaples {}, dimensions is {}\".format(N,D))"]},{"cell_type":"markdown","metadata":{},"source":["We can also perform lots of operations with Matrix multiplication, for example we can calculate the mean with the following:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["mean=(np.ones((1,N))/N)@X_\n","mean"]},{"cell_type":"markdown","metadata":{},"source":["We can verify the result in numpy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["X_.mean(axis=0)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["### Exercise 4\n","\n","Perform matrix multiplication using **no_mean** and **X\\_**, save the result in **X**. Show the mean of **X** with respect to the rows is approximately zero:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["I=np.identity(N)\n","col1=np.ones((1,N))\n","row1=np.ones((N,1))/N\n","no_mean=(I-row1@col1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# TO DO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","X=no_mean@X_\n","print(\"mean of X\",X.mean(axis=0))\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["dict_={\"original data\":X_,\"zero mean data\":X,\"mean\":mean}\n","plot_2d(dict_)    "]},{"cell_type":"markdown","metadata":{},"source":["As we subtracted the mean, we can calculate the empirical covariance matrix using matrix multiplication.\n"]},{"cell_type":"markdown","metadata":{},"source":["$C=\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} $\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["C=X.T@X/N\n","Matrix(C)"]},{"cell_type":"markdown","metadata":{},"source":["We can see the matrix is full rank:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["matrix_rank(C)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Eigen Decomposition\n"]},{"cell_type":"markdown","metadata":{},"source":["### Eigenvectors and Eigenvalues\n"]},{"cell_type":"markdown","metadata":{},"source":["If the Matrix is full rank we can apply Eigen factorization or Eigen decomposition to it, so that it is represented in terms of its eigenvalues $\\mathbf{\\Lambda}$ and eigenvectors embedded in $\\mathbf{Q}$, symbolically:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\mathbf{A}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}$\n"]},{"cell_type":"markdown","metadata":{},"source":["We can obtain the Matrices as follows:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["eigen_values , eigen_vectors = eig(A)"]},{"cell_type":"markdown","metadata":{},"source":["`eigen_values` is a vector so we convert it to a diagonal matrix  using the `np.diag` function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Matrix(np.diag(eigen_values))"]},{"cell_type":"markdown","metadata":{},"source":["We can retrieve the original matrix as follows:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["A=eigen_vectors@np.diag(eigen_values)@inv(eigen_vectors)\n","Matrix(A)"]},{"cell_type":"markdown","metadata":{},"source":["## Factorization for PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["We can only use  eigen decomposition if the matrix is full rank, but even if the matrix is full rank, the eigenvalues and eigenvectors could be complex.\n","\n","A useful application of the symmetric matrix ($S=S^{T}$) is where we want the eigenvalues to be real and the eigenvectors to be orthogonal. For example, the covariance matrix $C$ in PCA.\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\mathbf{C}=\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{T}$\n"]},{"cell_type":"markdown","metadata":{},"source":["Using numpy,\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["eigen_values , eigen_vectors = eig(C)"]},{"cell_type":"markdown","metadata":{},"source":["We can perform PCA using factorization. To find the first principle component, we need to find the eigenvector with the largest eigenvalue.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["v=eigen_vectors[:, np.argmax(eigen_values)].reshape(-1,1)\n","v"]},{"cell_type":"markdown","metadata":{},"source":["We can calculate first principal component by finding the projection of the original data onto $\\mathbf{v}$.\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\mathbf{Z}=\\mathbf{X} \\mathbf{v}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Z=X@v"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 5\n","\n","Perform PCA with `n_components=1` on **X**. Store the transformed data in **X_transformed**. Also do an inverse_transform using the same fitted PCA object, store the result in **X\\_**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","pca = PCA(n_components=1)\n","X_transformed=pca.fit_transform(X)\n","X_=pca.inverse_transform(X_transformed)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["We can also  transform the data back to its original space, using matrix multiplication:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\hat{X}=\\mathbf{Z} \\mathbf{v}^T$\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise 6\n","\n","Find the transform data back to its original space from the above equation, call it **Xhat**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","Xhat=Z@v.T\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["You can also check that the two methods give the same \"inverse transform\" result (**X\\_** and **Xhat** will overlap in the plot) by running the following cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["dict_ = {\"Sklearn inverse_transform\": X_, \"Matrix inverse transform\": Xhat, \"First Principal Component\": v.T}\n","plot_2d(dict_)"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
