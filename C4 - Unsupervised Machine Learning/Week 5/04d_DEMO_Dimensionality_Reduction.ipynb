{"cells":[{"cell_type":"markdown","id":"88ee1e68-4393-4103-95a6-a4ac4011604b","metadata":{},"source":["# Machine Learning Foundation\n","\n","## Course 4, Part d: Dimensionality Reduction DEMO\n"]},{"cell_type":"markdown","id":"d84f09a8-1c10-452a-9031-82d5c530474f","metadata":{},"source":["## Introduction\n","\n","We will be using customer data from a [Portuguese wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) for clustering. This data file is called `Wholesale_Customers_Data`.\n","\n","It contains the following features:\n","\n","* Fresh: annual spending (m.u.) on fresh products\n","* Milk: annual spending (m.u.) on milk products\n","* Grocery: annual spending (m.u.) on grocery products\n","* Frozen: annual spending (m.u.) on frozen products\n","* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n","* Delicatessen: annual spending (m.u.) on delicatessen products\n","* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n","* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n","\n","In this data, the values for all spending are given in an arbitrary unit (m.u. = monetary unit).\n"]},{"cell_type":"code","execution_count":null,"id":"ef56165e-72ba-4e4a-872c-578f2795a9f8","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"code","execution_count":null,"id":"f1a7d22b-6dc8-418a-8560-d460664dc858","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import os, pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","id":"1bb87ec2-b198-45d8-9b27-9f45c74d7961","metadata":{},"source":["## TASK 1\n","\n"," \n","\n","__Task 1 includes Part 1 to Part 4 which covers data acquistion, preprocessing and PCA__\n"]},{"cell_type":"markdown","id":"a6d9bff9-b9a9-4bc8-9b7f-5c99fcf248c3","metadata":{},"source":["## Part 1\n","\n","In this section, we will:\n","\n","* Import the data and check the data types.\n","* Drop the channel and region columns as they won't be used since we focus on numeric columns for this example.\n","* Convert the remaining columns to floats if necessary.\n","* Copy this version of the data (using the `copy` method) to a variable to preserve it. We will be using it later.\n"]},{"cell_type":"code","execution_count":null,"id":"24859caf-30ea-464e-a40f-950afd2fa190","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/Wholesale_Customers_Data.csv', sep=',')"]},{"cell_type":"code","execution_count":null,"id":"7da47a17-1dc2-4791-a981-86f84da6240a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"id":"8b1111eb-8c45-4af0-9092-e5ecb644e672","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"id":"46e7f8d6-2ca9-4805-8522-2906371d9676","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data = data.drop(['Channel', 'Region'], axis=1)"]},{"cell_type":"code","execution_count":null,"id":"892c30a4-f18d-43da-9b2e-c04d0150c406","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data.dtypes"]},{"cell_type":"code","execution_count":null,"id":"e7853f8f-d7d7-4304-b315-b1efe1036d43","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Convert to floats\n","for col in data.columns:\n","    data[col] = data[col].astype(np.float)"]},{"cell_type":"markdown","id":"11d9a904-077f-4ea5-ac1d-86c5e1d0889f","metadata":{},"source":["Preserve the original data.\n"]},{"cell_type":"code","execution_count":null,"id":"b6352708-e035-488f-87ab-ead7e1469b88","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data_orig = data.copy()"]},{"cell_type":"markdown","id":"25a4da0b-0a76-4361-b08a-ad1d53e2ec67","metadata":{},"source":["## Part 2\n","\n","As with the previous lesson, we need to ensure the data is scaled and (relatively) normally distributed.\n","\n","* Examine the correlation and skew.\n","* Perform any transformations and scale data using your favorite scaling method.\n","* View the pairwise correlation plots of the new data.\n"]},{"cell_type":"code","execution_count":null,"id":"b7eccdf7-d097-4015-8faf-3de7d136ee5a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["corr_mat = data.corr()\n","\n","# Strip the diagonal for future examination\n","for x in range(corr_mat.shape[0]):\n","    corr_mat.iloc[x,x] = 0.0\n","    \n","corr_mat"]},{"cell_type":"markdown","id":"4266528d-bf96-4518-87f7-6e0e34829f43","metadata":{},"source":["As before, the two categories with their respective most strongly correlated variable.\n"]},{"cell_type":"code","execution_count":null,"id":"164be791-f9da-40f5-8f5f-a042e1b281b0","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["corr_mat.abs().idxmax()"]},{"cell_type":"markdown","id":"9a228d17-3cd1-44c3-b51f-40a9cd46b70a","metadata":{},"source":["Examine the skew values and log transform. Looks like all of them need it.\n"]},{"cell_type":"code","execution_count":null,"id":"6a281215-2a74-4da2-a2af-b20b4446bb89","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["log_columns = data.skew().sort_values(ascending=False)\n","log_columns = log_columns.loc[log_columns > 0.75]\n","\n","log_columns"]},{"cell_type":"code","execution_count":null,"id":"fd870481-a73b-4d19-9027-b220fbbf57ba","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# The log transformations\n","for col in log_columns.index:\n","    data[col] = np.log1p(data[col])"]},{"cell_type":"markdown","id":"27282b05-7163-44d6-ac01-41127514fbb8","metadata":{},"source":["Scale the data again. Let's use `MinMaxScaler` this time just to mix things up.\n"]},{"cell_type":"code","execution_count":null,"id":"d8a6a143-0181-4c4c-94a0-61c9f0835027","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","mms = MinMaxScaler()\n","\n","for col in data.columns:\n","    data[col] = mms.fit_transform(data[[col]]).squeeze()"]},{"cell_type":"markdown","id":"965f9b43-639c-4ced-96b1-e3b0bff85044","metadata":{},"source":["Visualize the relationship between the variables.\n"]},{"cell_type":"code","execution_count":null,"id":"385d4c27-5ada-4884-9842-65ef926aa487","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["sns.set_context('notebook')\n","sns.set_style('white')\n","sns.pairplot(data);"]},{"cell_type":"markdown","id":"c031d92a-af45-48c0-9efe-55f1b51fc4c2","metadata":{},"source":["## Part 3\n","\n","In this section, we will:\n","* Using Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html), recreate the data pre-processing scheme above (transformation and scaling) using a pipeline. If you used a non-Scikit learn function to transform the data (e.g. NumPy's log function), checkout  the custom transformer class called [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n","* Use the pipeline to transform the original data that was stored at the end of question 1.\n","* Compare the results to the original data to verify that everything worked.\n","\n","*Note:* Scikit-learn has a more flexible `Pipeline` function and a shortcut version called `make_pipeline`. Either can be used. Also, if different transformations need to be performed on the data, a [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces) can be used.\n"]},{"cell_type":"code","execution_count":null,"id":"05b170a9-badc-43e3-8669-2f041d94f891","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.preprocessing import FunctionTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# The custom NumPy log transformer\n","log_transformer = FunctionTransformer(np.log1p)\n","\n","# The pipeline\n","estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n","pipeline = Pipeline(estimators)\n","\n","# Convert the original data\n","data_pipe = pipeline.fit_transform(data_orig)"]},{"cell_type":"markdown","id":"3744dbbd-ea94-4bb5-bb46-92ce97e61f64","metadata":{},"source":["The results are identical. Note that machine learning models and grid searches can also be added to the pipeline (and in fact, usually are.)\n"]},{"cell_type":"code","execution_count":null,"id":"a1062048-7f36-4275-b63e-88987d3ee073","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["np.allclose(data_pipe, data)"]},{"cell_type":"markdown","id":"98839eb1-8599-4bcf-86b1-d640d9475b0a","metadata":{},"source":["## Part 4\n","\n","In this section, we will:\n","* Perform PCA with `n_components` ranging from 1 to 5. \n","* Store the amount of explained variance for each number of dimensions.\n","* Also store the feature importance for each number of dimensions. *Hint:* PCA doesn't explicitly provide this after a model is fit, but the `components_` properties can be used to determine something that approximates importance. How you decided to do so is entirely up to you.\n","* Plot the explained variance and feature importances.\n"]},{"cell_type":"code","execution_count":null,"id":"8a86489f-521b-45f8-aeab-dd8e2718eb00","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca_list = list()\n","feature_weight_list = list()\n","\n","# Fit a range of PCA models\n","\n","for n in range(1, 6):\n","    \n","    # Create and fit the model\n","    PCAmod = PCA(n_components=n)\n","    PCAmod.fit(data)\n","    \n","    # Store the model and variance\n","    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n","                               'var': PCAmod.explained_variance_ratio_.sum()}))\n","    \n","    # Calculate and store feature importances\n","    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n","    feature_weight_list.append(pd.DataFrame({'n':n, \n","                                             'features': data.columns,\n","                                             'values':abs_feature_values/abs_feature_values.sum()}))\n","    \n","pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n","pca_df"]},{"cell_type":"markdown","id":"178a777b-80d4-4baf-a404-5bf93d4658b1","metadata":{},"source":["Create a table of feature importances for each data column.\n"]},{"cell_type":"code","execution_count":null,"id":"7dfbd79d-ca79-438c-baa3-b0f0ca544bec","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["features_df = (pd.concat(feature_weight_list)\n","               .pivot(index='n', columns='features', values='values'))\n","\n","features_df"]},{"cell_type":"markdown","id":"67aeac80-4759-4126-a0ef-3a9ff4f2eff9","metadata":{},"source":["Create a plot of explained variances.\n"]},{"cell_type":"code","execution_count":null,"id":"87d70c12-fc0a-4b3c-a1cc-ae8deeaf7b1f","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["sns.set_context('talk')\n","ax = pca_df['var'].plot(kind='bar')\n","\n","ax.set(xlabel='Number of dimensions',\n","       ylabel='Percent explained variance',\n","       title='Explained Variance vs Dimensions');"]},{"cell_type":"markdown","id":"c88d868f-2aca-4656-87fb-ee82bfcd197f","metadata":{},"source":["And here's a plot of feature importances.\n"]},{"cell_type":"code","execution_count":null,"id":"5df60130-ba98-4727-b098-dbf68387d65b","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["ax = features_df.plot(kind='bar', figsize=(13,8))\n","ax.legend(loc='upper right')\n","ax.set(xlabel='Number of dimensions',\n","       ylabel='Relative importance',\n","       title='Feature importance vs Dimensions');"]},{"cell_type":"markdown","id":"ab45d762-efc9-4220-ae9f-402bded665fc","metadata":{},"source":["This concludes \"Demo lab: Dimensionality Reduction (Part 1)\". We will be going over the remaining parts in the next module.\n"]},{"cell_type":"markdown","id":"5c63541c-751b-4496-b43d-b5b45ebe9b23","metadata":{},"source":["## TASK 2\n","\n"," \n","\n","__Task 2 covers part 5 and 6. It includes KernelPCA and Model accuracy.__\n","\n"," \n","\n",">Note: Task 1 is a pre-requisite for this task. Please make sure you complete Task 1 before proceeding to Task 2.\n"]},{"cell_type":"markdown","id":"f3f77dee-a3e9-4729-b4b2-111d04734c7d","metadata":{},"source":["## Part 5\n","\n","In this section, we will:\n","* Fit a `KernelPCA` model with `kernel='rbf'`. You can choose how many components and what values to use for the other parameters (`rbf` refers to a Radial Basis Function kernel, and the `gamma` parameter governs scaling of this kernel and typically ranges between 0 and 1). Several other [kernels](https://scikit-learn.org/stable/modules/metrics.html) can be tried, and even passed ss cross validation parameters (see this [example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)).\n","* If you want to tinker some more, use `GridSearchCV` to tune the parameters of the `KernelPCA` model. \n","\n","The second step is tricky since grid searches are generally used for supervised machine learning methods and rely on scoring metrics, such as accuracy, to determine the best model. However, a custom scoring function can be written for `GridSearchCV`, where larger is better for the outcome of the scoring function. \n","\n","What would such a metric involve for PCA? What about percent of explained variance? Or perhaps the negative mean squared error on the data once it has been transformed and then inversely transformed?\n"]},{"cell_type":"code","execution_count":null,"id":"00981447-a571-4088-8414-9cf2a8d69bc0","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.decomposition import KernelPCA\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","\n","# Custom scorer--use negative rmse of inverse transform\n","def scorer(pcamodel, X, y=None):\n","\n","    try:\n","        X_val = X.values\n","    except:\n","        X_val = X\n","        \n","    # Calculate and inverse transform the data\n","    data_inv = pcamodel.fit(X_val).transform(X_val)\n","    data_inv = pcamodel.inverse_transform(data_inv)\n","    \n","    # The error calculation\n","    mse = mean_squared_error(data_inv.ravel(), X_val.ravel())\n","    \n","    # Larger values are better for scorers, so take negative value\n","    return -1.0 * mse\n","\n","# The grid search parameters\n","param_grid = {'gamma':[0.001, 0.01, 0.05, 0.1, 0.5, 1.0],\n","              'n_components': [2, 3, 4]}\n","\n","# The grid search\n","kernelPCA = GridSearchCV(KernelPCA(kernel='rbf', fit_inverse_transform=True),\n","                         param_grid=param_grid,\n","                         scoring=scorer,\n","                         n_jobs=-1)\n","\n","\n","kernelPCA = kernelPCA.fit(data)\n","\n","kernelPCA.best_estimator_"]},{"cell_type":"markdown","id":"b3e49b46-8349-4ebf-adb6-f3afea498163","metadata":{},"source":["## Part 6\n","\n","Let's explore how our model accuracy may change if we include a `PCA` in our model building pipeline. Let's plan to use sklearn's `Pipeline` class and create a pipeline that has the following steps:\n","<ol>\n","  <li>A scaler</li>\n","  <li>`PCA(n_components=n)`</li>\n","  <li>`LogisticRegression`</li>\n","</ol>\n","\n","* Load the Human Activity data from the datasets.\n","* Write a function that takes in a value of `n` and makes the above pipeline, then predicts the \"Activity\" column over a 5-fold StratifiedShuffleSplit, and returns the average test accuracy\n","* For various values of n, call the above function and store the average accuracies.\n","* Plot the average accuracy by number of dimensions.\n"]},{"cell_type":"code","execution_count":null,"id":"d841e99f-15b0-40c8-bbfe-3e8523484b1d","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/Human_Activity_Recognition_Using_Smartphones_Data.csv', sep=',')"]},{"cell_type":"code","execution_count":null,"id":"a44cf322-af4b-4272-bec2-f7b3bcdd6d5a","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["data.columns"]},{"cell_type":"code","execution_count":null,"id":"2eb860af-3534-4f5e-b254-f863926fa8e3","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","X = data.drop('Activity', axis=1)\n","y = data.Activity\n","sss = StratifiedShuffleSplit(n_splits=5, random_state=42)\n","\n","def get_avg_score(n):\n","    pipe = [\n","        ('scaler', StandardScaler()),\n","        ('pca', PCA(n_components=n)),\n","        ('estimator', LogisticRegression(solver='liblinear'))\n","    ]\n","    pipe = Pipeline(pipe)\n","    scores = []\n","    for train_index, test_index in sss.split(X, y):\n","        X_train, X_test = X.loc[train_index], X.loc[test_index]\n","        y_train, y_test = y.loc[train_index], y.loc[test_index]\n","        pipe.fit(X_train, y_train)\n","        scores.append(accuracy_score(y_test, pipe.predict(X_test)))\n","    return np.mean(scores)\n","\n","\n","ns = [10, 20, 50, 100, 150, 200, 300, 400]\n","score_list = [get_avg_score(n) for n in ns]"]},{"cell_type":"code","execution_count":null,"id":"3f11e6e9-e89f-445c-aa2b-ca48b056581f","metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["sns.set_context('talk')\n","\n","ax = plt.axes()\n","ax.plot(ns, score_list)\n","ax.set(xlabel='Number of Dimensions',\n","       ylabel='Average Accuracy',\n","       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n","ax.grid(True)"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
