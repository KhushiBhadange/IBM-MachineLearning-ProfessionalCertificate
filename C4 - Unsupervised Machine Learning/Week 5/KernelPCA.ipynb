{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Kernel Principal Component Analysis**\n"]},{"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","metadata":{},"source":["Rumor has it that the ultra-wealthy community consists of either investment bankers or entrepreneurs in the tech industry that dropped out of college. Is that stereotype really true? Ever wonder if the top billionaires in the world share anything in common? Although, we can't say with certainty what it takes to be one, we do have a way to determine if any patterns exist among the richest people in the world.\n","\n","In this notebook, you will explore Kernel Principal Component Analysis (Kernel PCA) - an extension of principal component analysis (PCA) - to extract key feature patterns in the dataset, which is usually of higher dimension. In addition to analyzing billionaires around the globe, we will also use this unsupervised learning technique to denoise images.\n","\n","![img](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/RichPeople.png)\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## **Table of Contents**\n","\n","<!-- <a href=\"#Principle-Component-Analysis\">Principle Component Analysis</a> -->\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a>\n","        <ol>\n","            <li><a href=\"https://#Datasets\">Datasets</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Background\">Background</a>\n","        <ol>\n","            <li><a href=\"https://#What-does-kernel-PCA-do-?\">What does PCA do?</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Visual-Example-Transform-a-Dataset-Before-Applying-PCA\">Visual Example - Transform a Dataset Before Applying PCA  </a>\n","        <ol>\n","            <li><a href=\"https://#Apply-PCA\"> Apply PCA </a></li>\n","            <li><a href=\"https://#Transform-a-Dataset-to-a-Higher-Dimension-and-Then-Apply-PCA\">Transform a Dataset to a Higher Dimension and Then Apply PCA</a></li>\n","        </ol>\n","    </li>\n","     <li>\n","        <a href=\"https://#Kernel-PCA\"> Kernel PCA </a>\n","        <ol>\n","            <li><a href=\"https://#Whats-a-Kernel\">What's a Kernel? </a></li>\n","            <li><a href=\"https://https://#Applying-Kernel-PCA\">Applying Kernel PCA</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Using-Kernel-PCA-to-Predict-if-Youre-the-Richest-Person-in-the-World\"> Using  Kernel PCA to Predict if You're the Richest Person in the World </a>\n","        <ol>\n","            <li><a href=\"https://#Data-Analysis\">Data Analysis</a></li>\n","            <li><a href=\"#Applying-Kernel-PCA\">Applying Kernel PCA</a></li>\n","            <li><a href=\"https://#Using-Kernel-PCA-to-Improve-Visualization\">Using Kernel PCA to Improve Visualization  </a></li>\n","            <li><a href=\"https://#Using-Kernel-PCA-to-Improve-Prediction\">Using Kernel PCA to Improve Prediction </a></li>\n","        </ol>\n","    </li>\n","</ol>\n","\n","<a href=\"https://#Exercises\">Exercises</a>\n","\n","<ol>\n","    <li><a href=\"https://#Exercise-1---Fitting-PCA-Kernel\">Exercise 1 - Fitting PCA and Kernel PCA Objects</a></li>\n","    <li><a href=\"https://#Exercise-2---Reconstruct-the-Digits\">Exercise 2 - Reconstruct the Digits</a></li>\n","    <li><a href=\"https://#Exercise-3---Visualize-Denoised-Digit-Images\">Exercise 3 - Visualize Denoised Digit Images</a></li>\n","</ol>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","metadata":{},"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","metadata":{},"source":["*   **Understand** why we  transform a dataset to a higher dimension and apply PCA.\n","*   **Understand** what is Kernel PCA.\n","*   **Apply** Kernel PCA effectively to real world datasets for purposes ranging from prediction to visualization.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets\n","\n","Datasets for this lab are gathered from the [kaggle](https://www.kaggle.com/datasets/jjdaguirre/forbes-billionaires-2022?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) under the Public Domain License.\n"]},{"cell_type":"markdown","metadata":{},"source":["***\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Anaconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","#!mamba   install pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==1.0.2\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==1.0.2\""]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we require a newer version of scikit-learn.If you're using the SN Labs environment, please run this before importing sklearn. Restart the kernel after successful upgrade to start using the newer version.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install --upgrade scikit-learn"]},{"cell_type":"markdown","metadata":{},"source":["### Importing Required Libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Surpress warnings from using older version of sklearn:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import numpy as np\n","import pandas as pd\n","from itertools import accumulate\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA,KernelPCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","warnings.filterwarnings('ignore')\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Helper Functions\n"]},{"cell_type":"markdown","metadata":{},"source":["This function will plot projections onto a vector.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_proj(A,v,y,name=None):\n","\n","    plt.scatter(A[:,0],A[:,1],label='data',c=y,cmap='viridis')\n","    \n","    #plt.plot(np.linspace(A[:,0].min(),A[:,0].max()),np.linspace(A[:,1].min(),A[:,1].max())*(v[1]/v[0]),color='black',linestyle='--',linewidth=1.5,label=name)   \n","    plt.plot(np.linspace(-1,1),np.linspace(-1,1)*(v[1]/v[0]),color='black',linestyle='--',linewidth=1.5,label=name)  \n","    # Run through all the data\n","\n","    for i in range(len(A[:,0])-1):\n","        #data point \n","        w=A[i,:]\n","\n","        # projection\n","        cv = (np.dot( A[i,:],v))/np.dot(v,np.transpose(v))*v\n","\n","        # line between data point and projection\n","        plt.plot([A[i,0],cv[0]],[A[i,1],cv[1]],'r--',linewidth=1.5)\n","    plt.plot([A[-1,0],cv[0]],[A[-1,1],cv[1]],'r--',linewidth=1.5,label='projections' )\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Background\n","\n","Kernel principal component analysis (kernel PCA) is an extension of principal component analysis (PCA) using kernel methods to map the data to a higher dimensional space, then performs PCA.\n"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### What does kernel PCA do?\n","\n","Kernel PCA maps the data into a higher dimensional space then performs PCA. The mapping to the higher dimensional space may be difficult to calculate, so the mapping is done via a kernel. As a result, you don't need to calculate the mapping, but you get all the benefits. The process is shown here:\n","\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/images/image.png\"  />\n","</center>\n","\n","Many standard tasks can be done with PCA, but in many applications, you may get a better result using kernel PCA. Applications include:\n","\n","*   Reduce the dimensionality of data\n","    *   By reducing data dimensionality, PCA can also help with visualization\n","*   May reduce noise in the process\n","*   Can be used to pre-process data improving the result of your algorithm\n"]},{"cell_type":"markdown","metadata":{},"source":["# Visual Example: Transform a Dataset Before Applying  PCA\n","\n","Let's look at an example that will visually demonstrate kernel PCA in action. Consider the `make_circles` toy dataset. The dataset contains a large ring of one class, enclosing a smaller ring of another class in 2 dimensions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.datasets import make_circles\n","from sklearn.model_selection import train_test_split\n","\n","# Create the toy dataset\n","X, y = make_circles(n_samples=1000, factor=0.01, noise=0.05, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the data\n","_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))\n","\n","train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,cmap='viridis')\n","train_ax.set_xlabel(\"$x_{0}$\")\n","train_ax.set_ylabel(\"$x_{1}$\")\n","train_ax.set_title(\"Training data\")\n","\n","test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test,cmap='viridis')\n","test_ax.set_xlabel(\"$x_{0}$\")\n","test_ax.set_ylabel(\"$x_{1}$\")\n","test_ax.set_title(\"Test data\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Apply PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["There are several issues with the dataset above, the first being samples from each class cannot be linearly separated: no plane can split the samples of the inner ring from the outer ring. Let's see what happens when we perform PCA and evaluate the projection or score.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# Fit on a PCA object\n","pca = PCA(n_components=2)\n","\n","score_pca = pca.fit(X_train).transform(X_test)\n","pca"]},{"cell_type":"markdown","metadata":{},"source":["We can plot the 2 principal components:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_pca[:, 0], score_pca[:, 1], c=y_test,label=\"data points\", cmap='viridis')\n","plt.quiver([0,0],[0,0], pca.components_[0,:], pca.components_[1,:], label=\"eigenvectors\")\n","plt.xlabel(\"$x_{0}$\")\n","plt.ylabel(\"$x_{1}$\")\n","plt.legend(loc='center right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The following red lines show the direction of projection onto the first principal component (the black dotted line) for each data point.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_proj(X_train,pca.components_[0,:],y_train,\"first principal component\")"]},{"cell_type":"markdown","metadata":{},"source":["We can also plot out the actual points of projection onto the first principal component, which are also known as their scores for the PC. We see that the dataset is not linearly separable, as data points from different classes cannot be separated with a plane:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_pca [:,0],np.zeros(score_pca[:,0].shape[0]),c=y_test,cmap='viridis')\n","plt.title(\"Projection of testing data\\n using PCA\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["If we plot the projection onto both principal components, we see the data looks the same, or isotropic, meaning that it has the same value when measured in a different coordinate system. This is because the data is symmetric and the variance is equal on each side. As a result, PCA will not help us classify the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_pca[:, 0], score_pca[:, 1], c=y_test,cmap='viridis')\n","plt.title(\"Projection of testing data\\n using PCA\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Building a Logistic Regression model on the training set, we can see that the accuracy score is around 50%, suggesting that the model is arbitrarily worse. Hence, we can see that it'd be unsuitable to project the data onto a low-dimensional linear subspace.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lr= LogisticRegression().fit(X_train, y_train)\n","print(str.format(\"Test set  mean accuracy score for for PCA: {}\", lr.score(X_test, y_test)))"]},{"cell_type":"markdown","metadata":{},"source":["### Transform a Dataset to a Higher Dimension and then Apply PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["Instead, let's first apply the following polynomial transformation to the data:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\mathbf{\\phi}(\\mathbf{x})=\\[x\\_{1},x\\_{2},x\\_{1}^2+x\\_{2}^2]$ where $\\mathbf{x}=\\[x\\_{1},x\\_{2}]$\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PHI_train=np.concatenate((X_train, (X_train**2).sum(axis=1).reshape(-1,1)),axis=1)\n","PHI_test=np.concatenate((X_test, (X_test**2).sum(axis=1).reshape(-1,1)),axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["We can plot the result  where  on the horizontal axis we have $x\\_1$ and $x\\_2$, and on the vertical axis we have $x\\_1^2+x\\_2^2$. We see that the larger ring has been mapped upwards in the vertical direction as the sum of its squared components is larger.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot in 3D\n","fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","ax.scatter(PHI_train[:,0], PHI_train[:,1],  PHI_train[:,2], c=y_train, cmap='viridis', linewidth=0.5);\n","ax.set_xlabel('$x_{1}$')\n","ax.set_ylabel('$x_{2}$')\n","ax.set_zlabel('$x_{1}^2+x_{2}^2$')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We then perform PCA on this transformed data, specifying we want to keep the top three principal components.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA(n_components=3)\n","score_polly = pca.fit(PHI_train).transform(PHI_test)"]},{"cell_type":"markdown","metadata":{},"source":["Again, like we have previously done, we can plot out the data points' projection onto the first PC/scores. Below, we see that the dataset is almost linearly separable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_polly[:,0],np.zeros(score_polly[:,1].shape[0]),c=y_test,cmap='viridis')\n","plt.title(\"Projection onto the \\nfirst principal component\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Projecting onto a plane formed by the top two principal components (the ones with the largest absolute eigenvalues), we see the dataset is linearly separable with the exception of a couple of points.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_polly[:,0], score_polly[:,1],c=y_test, cmap='viridis', linewidth=0.5);\n","plt.xlabel(\"1st principal component\")\n","plt.ylabel(\"2nd principal component\")\n","plt.title(\"Projection onto PCs\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["By building a Logistic Regression model on the training set, we see the accuracy is much better\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr= LogisticRegression().fit(PHI_train, y_train)\n","print(str.format(\"Test set  mean accuracy score for for Kernal PCA: {}\", lr.score(PHI_test, y_test)))"]},{"cell_type":"markdown","metadata":{},"source":["The actual polynomial transform for 2 dimensions is given by:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\phi(\\mathbf{x}) = \\[x\\_2^2, x\\_1^2,  \\sqrt{2} x\\_{2} x\\_{1}, \\sqrt{2c} x\\_1, \\sqrt{2c} x\\_2, c ]$\n"]},{"cell_type":"markdown","metadata":{},"source":["For $D$ dimensions, this  generalizes to:\n"]},{"cell_type":"markdown","metadata":{},"source":["$\\phi(\\mathbf{x}) = \\[x_D^2, \\ldots, x\\_1^2, \\sqrt{2} x_D x\\_{D-1}, \\ldots, \\sqrt{2} x_D x\\_1, \\sqrt{2} x\\_{D-1} x\\_{D-2}, \\ldots, \\sqrt{2} x\\_{D-1} x\\_{1}, \\ldots, \\sqrt{2} x\\_{2} x\\_{1}, \\sqrt{2c} x_D, \\ldots, \\sqrt{2c} x\\_1, c ]$\n"]},{"cell_type":"markdown","metadata":{},"source":["Calculating the transform is of order $\\mathcal{O}(D^2)$, therefore if $D=2$ the complexity is 4, for $D=5$ the complexity is 25, and so on. In addition to the cost of calculating $\\phi(\\mathbf{x})$, you also have the cost of calculating the empirical covariance matrix. Although, the demonstrated example is for a second-order polynomial, the sample applies for higher order polynomials.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Kernel PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["### What's a Kernel?\n"]},{"cell_type":"markdown","metadata":{},"source":["From the previous example, we can see that mapping the data to a higher dimension first is helpful. It turns out, we can use the kernel trick to perform PCA in this transformed space without calculating polynomial transformation explicitly. For two samples $\\mathbf{x}$ and  $ \\mathbf{x'} $, we can calculate the second order polynomial kernel by setting $p=2$ and applying the following:\n"]},{"cell_type":"markdown","metadata":{},"source":["$k(\\mathbf{x}, \\mathbf{x'}) = \\phi(\\mathbf{x})^\\mathsf{T}\\phi(\\mathbf{x'}) = (\\mathbf{x}^\\mathsf{T} \\mathbf{x'} + c)^{p} $, where $c$ is a constant usually one\n"]},{"cell_type":"markdown","metadata":{},"source":["This is of complexity $\\mathcal{O}(D)$ coming from $\\mathbf{x}^\\mathsf{T} \\mathbf{x}$; we can also increase the order of the polynomial by changing $p$ and that wouldn't change the computation complexity.\n"]},{"cell_type":"markdown","metadata":{},"source":["In this lab we used the <a href=https://en.wikipedia.org/wiki/Radial_basis_function_kernel?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01>Radial Basis Function (RBF) kernel </a> - the  dimension $\\phi(\\mathbf{x})$ is infinite, but the kernel is simple to calculate as shown here:\n"]},{"cell_type":"markdown","metadata":{},"source":["$k(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {|\\mathbf {x} -\\mathbf {x'} |^{2}}{2\\sigma ^{2}}}\\right)$\n"]},{"cell_type":"markdown","metadata":{},"source":["Kernel calculation involves evaluating the Gram matrix, shown from $N$ samples, as well as its eigenvector and eigenvalues. Hence, if there are a lot of samples in your training set, you may encounter issues.\n"]},{"cell_type":"markdown","metadata":{},"source":["$ K= \\begin{vmatrix}\n","k(\\mathbf{x}\\_1, \\mathbf{x}\\_1) & k(\\mathbf{x}\\_1, \\mathbf{x}\\_2)&\\dots &  k(\\mathbf{x}\\_1, \\mathbf{x}\\_N) \\\\\\\\\n","k(\\mathbf{x}\\_2, \\mathbf{x}\\_1)  & k(\\mathbf{x}\\_2, \\mathbf{x}\\_2)  &\\dots & k(\\mathbf{x}\\_2, \\mathbf{x}\\_N) \\\\\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n","k(\\mathbf{x}\\_N, \\mathbf{x}\\_1)  & k(\\mathbf{x}\\_N, \\mathbf{x}\\_2)  &\\dots & k(\\mathbf{x}\\_N, \\mathbf{x}\\_N) \\\\\\\\\n","\\end{vmatrix}$\n"]},{"cell_type":"markdown","metadata":{},"source":["### Applying Kernel PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["First, we create a Kernel Principal Component Analysis (KPCA) object using:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kernel_pca = KernelPCA( kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["with the following parameters:\n"]},{"cell_type":"markdown","metadata":{},"source":["`n_components`: Number of components. If None (the default), all non-zero components are kept.\n","\n","`kernel`: Kernel used for PCA Kernels is one of the following: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’}. Default is ’linear’.\n","\n","`gamma`: $\\sigma$ is the Kernel coefficient for RBF, poly, and sigmoid kernels, but ignored by other kernels. If gamma is None (the default), then it is set to 1/n_features.\n","\n","`alpha`: Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).\n"]},{"cell_type":"markdown","metadata":{},"source":["We can now `fit` the model:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kernel_pca.fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["We can then `transform` the data, which helps us find the projection of the fitted data on the kernel principal components:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["score_kernel_pca = kernel_pca.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["To better see how much variance is explained by each principal component, we take a look at the magnitude of their respective eigenvalues. Below, you can see that the first few principal components have the largest absolute eigenvalue, and the values continues to decrease after that.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(kernel_pca.eigenvalues_)\n","\n","plt.title(\"Principal component and their eigenvalues\")\n","plt.xlabel(\"nth principal component\")\n","plt.ylabel(\"eigenvalue magnitude\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Again, by plotting the data points' projection onto the first two principal components, we can see that the data is linearly separable, demonstrating that kernel PCA performs better than our polynomial approximation:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(score_kernel_pca[:,0],score_kernel_pca[:,1] ,c=y_test,cmap='viridis')\n","plt.title(\"Projection onto PCs (kernel)\")\n","plt.xlabel(\"1st principal component\")\n","plt.ylabel(\"2nd principal component\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In PCA, the reconstruction was exactly the same as the original component, if `n_components` is equal to the dimension of the original features. On the other hand, Kernel PCA does not actually span $\\phi(\\mathbf{x})$ but instead spans a subspace. Therefore, the inverse transformation will not always reconstruct the data, like in PCA. We can verify this by comparing the inverse transformation of Kernel PCA and PCA.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_hat_kpca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))\n","\n","pca = PCA(n_components=2)\n","pca.fit(X_train)\n","X_hat_pca = pca.inverse_transform(pca.transform(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(X_test[:,0],X_test[:,1] ,c=y_test,cmap='viridis')\n","plt.title(\"Original data\")\n","plt.show()\n","\n","plt.scatter(X_hat_kpca[:,0],X_hat_kpca[:,1] ,c=y_test,cmap='viridis')\n","plt.title(\"Inversely Transformed Data (Kernel PCA)\")\n","plt.show()\n","\n","plt.scatter(X_hat_pca[:,0],X_hat_pca[:,1] ,c=y_test,cmap='viridis')\n","plt.title(\"Inversely Transformed Data (PCA)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can also calculate the error in the mean squared error for the inverse transformation, which is larger for Kernel PCA than for PCA, whose MSE is approximately 0, since the transformed data for it is identical to the original.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Mean squared error for Kernel PCA is:\",((X_test-X_hat_kpca)**2).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Mean squared error PCA is:\" ,((X_test-X_hat_pca)**2).mean())"]},{"cell_type":"markdown","metadata":{},"source":["## Using Kernel PCA to Predict if You're the Richest Person in the World\n"]},{"cell_type":"markdown","metadata":{},"source":["Returning back to the question posed at the beginning of this lab, let's see if we can use Kernel PCA to help us better visualize if any trends exist among the richest people in the world. \"The World's Billionaires\" is an annual ranking documenting the net worth of the wealthiest billionaires in the world, compiled and published in March, annually, by the American business magazine - Forbes. For this lab, we obtained the dataset from <a href=\"https://www.kaggle.com/datasets/jjdaguirre/forbes-billionaires-2022?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\"> here</a>.\n"]},{"cell_type":"markdown","metadata":{},"source":["The features available from the dataset are:\n","\n","*   Rank\n","*   Name\n","*   Net Worth - their net worth in billions USD\n","*   Age\n","*   Country\n","*   Source - their source of income\n","*   Industry - sector/industry/market segment in which each billionaire has made their fortune\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Analysis\n"]},{"cell_type":"markdown","metadata":{},"source":["We load the dataset and take a look to see if it loaded properly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the dataset and read it into a Pandas dataframe\n","df=pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/billionaires.csv',index_col=\"Unnamed: 0\")\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Checking the dataframe dimension, we notice that there are a total of 2600 samples.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["For each column, we can take a look at its unique values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for col in df:\n","    print(str.format(\"{} has {} unique values.\", col, len(df[col].unique())))"]},{"cell_type":"markdown","metadata":{},"source":["Notably, there are only 228 unique rank values, which is due to many ties in the rankings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[-100:-1]"]},{"cell_type":"markdown","metadata":{},"source":["Also, we note that there are a lot of unique values for `source` and `names`, suggesting that they possibly won't help with rank prediction and can be excluded in our task. To examine the categorical variables `country` and`industry`, we plot their respective histograms:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for column in ['country','industry']:\n","    \n","    df[column].hist(bins=len(df[column].unique()))\n","    plt.xticks(rotation='vertical')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that in this dataset, certain countries and industries contain more billionaires than others. We also examine the pairwise plot and correlation coefficient between age and rank, which tells us that the two features are negatively correlated.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.pairplot(df[['age','rank']])\n","df[['age','rank']].corr()"]},{"cell_type":"markdown","metadata":{},"source":["For the variables that we don't use in prediction, we can save them separately:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["B_names,networths,sources,industrys=df['name'],df['networth'],df['source'],df['industry']\n","B_names,networths,sources,industrys"]},{"cell_type":"markdown","metadata":{},"source":["We assign `rank`  to  `y`, our target, for prediction.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y=df['rank']\n","y.head()"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's drop all the features we are not going to use for the task:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop(columns=['name','networth','source'],inplace=True)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Since our categorical variables `country` and `industry` are not ordinal, meaning that the categories don't have a specific order, we utilize one-hot encoding to convert their levels into dummy variables. Other remaining variables in the dataset don't require this, so we specify `remainder=\"passthrough\"` to exclude them from the encoding. The features will then be assigned to the variable `data`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["one_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), ['country','industry']) ],remainder=\"passthrough\")\n","data=one_hot.fit_transform(df)"]},{"cell_type":"markdown","metadata":{},"source":["As of now, the output is a NumPy array, so let's get the feature names from the `one_hot` object using the method `get_feature_names_out()`, which will give  us the feature name with the name of the transformer as a prefix. For one-hot encoding, the prefix will also include the name of the column that generated the feature. Hence, we can strip out the prefix of the string for column names and save the labelled array as a dataframe:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["names=one_hot.get_feature_names_out()\n","column_names=[name[name.find(\"_\")+1:] for name in  [name[name.find(\"__\")+2:] for name in names]]\n","new_data=pd.DataFrame(data.toarray(),columns=column_names)\n","new_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Applying Kernel PCA\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's define a Kernel PCA object and fit it on this new data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kernel_pca = KernelPCA(kernel=\"rbf\" ,fit_inverse_transform=True, alpha=0.1)\n","kernel_score=kernel_pca.fit_transform(new_data)"]},{"cell_type":"markdown","metadata":{},"source":["### Using Kernel PCA to Improve Visualization\n"]},{"cell_type":"markdown","metadata":{},"source":["By plotting out the projection on the first two principal components and overlaying the plot with rank as the color, we are able to see a two dimensional visualization of the dataset, which can help distinguish any clusters that may be present in rankings. Here, the variable `ranking` was also defined, which will label the plot with the industry of the individual with that ranking value.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ranking=13"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","sc=ax.scatter(kernel_score[:,0],kernel_score[:,1] ,c=y,cmap='viridis')\n","fig.colorbar(sc, orientation='vertical')\n","ax.annotate(industrys[ranking], (kernel_score[ranking,0],kernel_score[ranking,1]))\n","plt.xlabel(\"1st principal component\")\n","plt.ylabel(\"2nd principal component\")\n","plt.title(\"Projection on the top 2 \\nprincipal components (colored by ranking)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We see a Bifurcation of the data dependent on the rank. The diverging branches suggest that difference in ranking can be associated with certain patterns in the data. This becomes more apparent in the 3-dimensional projection space:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","sc=ax.scatter(kernel_score[:,0], kernel_score[:,1],  kernel_score[:,2], c=y, cmap='viridis', linewidth=0.5);\n","fig.colorbar(sc, orientation='horizontal')\n","ax.set_xlabel('1st PC')\n","ax.set_ylabel('2nd PC')\n","ax.set_zlabel('3rd PC')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Each line corresponds to a different range of ranking (represented by the varying colors), so let's see what happens when we apply PCA and plot the projection space in 1, 2, and 3 dimensions respectively.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA()\n","score_pca = pca.fit_transform(new_data)"]},{"cell_type":"markdown","metadata":{},"source":["Even in the 1D projection space, we can see a pattern for rankings, which decreases as we move toward the right.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n","sc=ax.scatter(score_pca[:,0],np.zeros(score_pca[:,1].shape ),c=y,cmap='viridis')\n","ax.set_title('1-dimensional projection space\\n (1st PC)')\n","fig.colorbar(sc, orientation='vertical')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n","sc=ax.scatter(score_pca[:,0],score_pca[:,1] ,c=y,cmap='viridis')\n","fig.colorbar(sc, orientation='vertical')\n","ax.set_title('2-dimensional projection space\\n (Top 2 PCs)')\n","plt.xlabel(\"1st PC\")\n","plt.ylabel(\"2nd PC\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","sc=ax.scatter(score_pca[:,0], score_pca[:,1],  score_pca[:,2], c=y, cmap='viridis', linewidth=0.5);\n","ax.set_title('3-dimensional projection space\\n (Top 3 PCs)')\n","ax.set_xlabel('1st PC')\n","ax.set_ylabel('2nd PC')\n","ax.set_zlabel('3rd PC')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In the 2-dimensional and 3-dimensional projection space, we see a similar trend that rankings change across the x-axis, meaning that most of its variation occurs in the projection on the first principal component.\n","\n","From this example, we can observe that surely, some combination of the features `country`, `industry`, and `age` leads to consistent change in rankings. It is important to note that due to the nature of PCA in general, we aren't able to decrypt what that combination is.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Using Kernel PCA to Improve Prediction\n"]},{"cell_type":"markdown","metadata":{},"source":["Even though we can't extract information that PCA deems to be a significant pattern, we can still feed the transformed features into a prediction model. When we compare Kernel PCA and PCA, we see that Kernel PCA generally performs better given a higher R^2 (coefficient of determination) score on the test set:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(kernel_score, y, test_size=0.4, random_state=0)\n","lr = Ridge(alpha=0).fit(X_train, y_train)\n","print(str.format(\"Test set R^2 score for Kernel PCA: {}\", lr.score(X_test, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(score_pca, y, test_size=0.40, random_state=0)\n","lr= Ridge(alpha=0).fit(X_train, y_train)\n","print(str.format(\"Test set R^2 score for PCA: {}\", lr.score(X_test, y_test)))"]},{"cell_type":"markdown","metadata":{},"source":["# Exercises\n","\n","You may now wonder, \"What's the use in prediction, if we don't know which variables matter the most or what the significant split value is?\" Depending on the task type, we don't always need to know that to produce interpretable results. Image datasets are one such case - in the following exercises, you will take advantage of dimension reduction to compare how well PCA and Kernel PCA remove noise in images while still retaining its key features.\n","\n","Run the following code cell to complete the exercises.\n","\n","First, we load the necessary train and test data, which are created using the USPS digits dataset that have been modified to include noise. In the image below, you can see that the digits are fairly hard to see clearly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_noisy = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/X_train_noisy.csv').to_numpy()\n","X_test_noisy = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/X_test_noisy.csv').to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Helper function for plotting the digit images\n","def plot_digits(X, title):\n","    \"\"\"Small helper function to plot 100 digits.\"\"\"\n","    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(8, 8))\n","    for img, ax in zip(X, axs.ravel()):\n","        ax.imshow(img.reshape((16, 16)), cmap=\"Greys\")\n","        ax.axis(\"off\")\n","    fig.suptitle(title, fontsize=24)\n","    \n","plot_digits(X_test_noisy, \"Noisy test images\")"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 1 - Fitting PCA & Kernel PCA Objects\n"]},{"cell_type":"markdown","metadata":{},"source":["First, create a `PCA` object called `pca` and fit it to the noisy test set `X_test_noisy`. Do the same for `KernelPCA` object `kernel_pca` using a RBF kernel.\n","\n","Be mindful that `n_components` has to be a number between 0 and `min(n_samples, n_features) = 216` for PCA.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Sample Solution</summary>\n","\n","```python\n","pca = PCA(n_components=35)\n","pca.fit(X_train_noisy)\n","\n","kernel_pca = KernelPCA(n_components=400, kernel=\"rbf\", gamma=0.01, fit_inverse_transform=True, alpha=0.1)\n","kernel_pca.fit(X_train_noisy)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 2 - Reconstruct the Digits\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we want to see if PCA or kernel PCA is better at removing noise in the original images. To do so, recall how we reconstructed the `make_circles` toy dataset earlier in this lab. (Hint: apply inverse transformation to the transformed data on a lower dimension.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","X_hat_pca = pca.inverse_transform(pca.transform(X_test_noisy))\n","\n","X_hat_kpca = kernel_pca.inverse_transform(kernel_pca.transform(X_test_noisy))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 3 - Visualize Denoised Digit Images\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's visualize the images reconstructed from PCA and kernel PCA! You may find the helper function `plot_digits` helpful. Which one reveals digit images with less noise?\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for a Sample Solution</summary>\n","\n","```python\n","plot_digits(X_hat_pca, \"Reconstructed Test Set (PCA)\")\n","\n","plot_digits(X_hat_kpca, \"Reconstructed Test Set (Kernel PCA)\")\n","```\n","\n","</details>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc-autonumbering":false,"toc-showmarkdowntxt":false,"toc-showtags":true},"nbformat":4,"nbformat_minor":4}
